{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we are downloading PDF file, converting it to TXT and doing some \"pre-cleaning\": removing not meaningful parts of document and leaving just the most valuable leftovers for our future generator.\n",
    "THe outcome of the below code is pre-processed but still raw data.\n",
    "\n",
    "\n",
    "\"extracted_text\" variable has \"StringIO\" type: The StringIO object is part of Python's io module and is a class that provides an in-memory file-like object that can be used for reading from or writing to strings as if they were files. It allows you to treat strings as file-like objects, which can be useful in various situations, such as when you want to read from or write to a string in a way that mimics file operations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import of libraries\n",
    "from io import StringIO # extracted_text is the main variable, contains the whole text of document in stringIO format in memory\n",
    "import requests\n",
    "import re  # provides reg. exp. support\n",
    "import math\n",
    "import api\n",
    "from selenium import webdriver\n",
    "\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfdocument import PDFDocument\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.pdfparser import PDFParser\n",
    "import fitz\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import spacy\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "import sentencepiece\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer, BertForQuestionAnswering, BertTokenizer\n",
    "from transformers import AutoTokenizer\n",
    "from keybert import KeyBERT\n",
    "import gradio as gr # UI part for the quize\n",
    "import urllib3\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import fuzzywuzzy\n",
    "\n",
    "# libraries for conversion from csv to json, results of \"Flag\" button click CSV -> JSON\n",
    "import csv\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U pip setuptools wheel\n",
    "#!pip install -U spacy\n",
    "#!python -m spacy download en_core_web_sm\n",
    "#!pip install PyMuPDF # this is fitz\n",
    "#!pip install gradio\n",
    "#!pip install keybert\n",
    "#!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1113747"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# downloading pdf to '/data/' folder\n",
    "url = 'https://astqb.org/assets/documents/ISTQB_CTFL_Syllabus-v4.0.pdf'\n",
    "r = requests.get(url, allow_redirects=True)\n",
    "open('data/ISTQB_CTFL_Syllabus-v4.0.pdf', 'wb').write(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "r\"Page \\d{4,74} of 74\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted text for 'The Certified Tester Foundation Level in Software Testing' saved to 'data/ISTQB_CTFL_Syllabus-v4.0.txt'\n"
     ]
    }
   ],
   "source": [
    "#converting pdf to text and saving into .txt file initial version\n",
    "output_string = StringIO()\n",
    "output_file_path = 'data/ISTQB_CTFL_Syllabus-v4.0.txt'\n",
    "with open('data/ISTQB_CTFL_Syllabus-v4.0.pdf', 'rb') as in_file, open(output_file_path, 'w', encoding='utf-8') as out_file:\n",
    "    parser = PDFParser(in_file)\n",
    "    doc = PDFDocument(parser)\n",
    "    rsrcmgr = PDFResourceManager()\n",
    "    device = TextConverter(rsrcmgr, output_string, laparams=LAParams())\n",
    "    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "    for page in PDFPage.create_pages(doc):\n",
    "        interpreter.process_page(page)\n",
    "    # Getting the extracted text from StringIO, it means the entire text extracted from the PDF is stored as a single string in memory.\n",
    "    extracted_text = output_string.getvalue()\n",
    "    # Writing the extracted text to the output file\n",
    "    out_file.write(extracted_text)\n",
    "\n",
    "# Closing the stream\n",
    "output_string.close()\n",
    "\n",
    "\n",
    "# Printing message to indicate that the text has been saved to the file\n",
    "print(f\"Extracted text for 'The Certified Tester Foundation Level in Software Testing' saved to '{output_file_path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of string in bytes : 198489\n",
      "File size, document contains 70+ pages:  193.84 KB\n"
     ]
    }
   ],
   "source": [
    "# let us check size of StringIO on the full size of converted file, just out of curiosity\n",
    "size_bytes = len(extracted_text.encode('utf-8'))\n",
    "print ('The length of string in bytes : ' + str (size_bytes))\n",
    "\n",
    "# function's code is taken from stackoverflow ---\n",
    "def convert_size(size_bytes):\n",
    "   if size_bytes == 0:\n",
    "       return \"0B\"\n",
    "   size_name = (\"B\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\", \"EB\", \"ZB\", \"YB\")\n",
    "   i = int(math.floor(math.log(size_bytes, 1024)))\n",
    "   p = math.pow(1024, i)\n",
    "   s = round(size_bytes / p, 2)\n",
    "   return \"%s %s\" % (s, size_name[i])\n",
    "# ---\n",
    "print(\"File size, document contains 70+ pages: \", convert_size(size_bytes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted text for 'The Certified Tester Foundation Level in Software Testing' pre processed version 0.1 saved to 'data/ISTQB_CTFL_Syllabus-v4.0_v01.txt'\n"
     ]
    }
   ],
   "source": [
    "# Looking up for the text to remove everything before it\n",
    "target_text = \"1.1. What is Testing?\"\n",
    "\n",
    "# Finding the position of the target text in the extracted text\n",
    "start_position = extracted_text.find(target_text)\n",
    "\n",
    "# Checking if the target text was found, just in case\n",
    "if start_position != -1:\n",
    "    # Removing everything before the target text\n",
    "    extracted_text = extracted_text[start_position:]\n",
    "\n",
    "\n",
    "# let us save the content to .txt file with prefix '_v0.1' for further debugging purpose and human evaluation process\n",
    "\n",
    "output_file_path = 'data/ISTQB_CTFL_Syllabus-v4.0_v01.txt'\n",
    "with open(output_file_path, 'w', encoding='utf-8') as out_file:\n",
    "    # Writing the extracted text to the output file\n",
    "    out_file.write(extracted_text)\n",
    "\n",
    "# Closing the stream\n",
    "output_string.close()\n",
    "\n",
    "# Printing message to indicate that the text has been saved to the file\n",
    "print(f\"Extracted text for 'The Certified Tester Foundation Level in Software Testing' pre processed version 0.1 saved to '{output_file_path}'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing empty lines\n",
    "# _ - is iterator, if s.strip(): This part of the list comprehension checks whether the line s contains any non-whitespace characters. \n",
    "# If it does, the line is included in the resulting list.\n",
    "\n",
    "# extracted_text = \"\".join([_ for _ in extracted_text.strip().splitlines(True) if _.strip()])\n",
    "# \n",
    "# output_file_path = 'data/ISTQB_CTFL_Syllabus-v4.0_v02.txt'\n",
    "# with open('data/ISTQB_CTFL_Syllabus-v4.0_v01.txt', 'rb') as in_file, open(output_file_path, 'w', encoding='utf-8') as out_file:\n",
    " #   Writing the extracted text to the output file\n",
    "    # out_file.write(extracted_text)\n",
    "# \n",
    "#Closing the stream\n",
    "# output_string.close()\n",
    "# \n",
    "#Printing message to indicate that the text has been saved to the file\n",
    "# print(f\"Extracted text for 'The Certified Tester Foundation Level in Software Testing' pre processed version 0.2 saved to '{output_file_path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted text for 'The Certified Tester Foundation Level in Software Testing' pre processed version 0.3 saved to 'data/ISTQB_CTFL_Syllabus-v4.0_v03.txt'\n"
     ]
    }
   ],
   "source": [
    "# Removing text from 'Page 56 of 74' till the end of the text\n",
    "\n",
    "# Looking up for the text to remove everything after it\n",
    "target_text = \"Page 56 of 74\"\n",
    "\n",
    "# Finding the position of the target text in the extracted text\n",
    "end_position = extracted_text.find(target_text)\n",
    "\n",
    "# Checking if the target text was found, just in case\n",
    "if end_position != -1:\n",
    "    # Removing everything before the target text\n",
    "    extracted_text = extracted_text[:end_position]\n",
    "\n",
    "\n",
    "# let us save the content to .txt file with prefix '_v0.1' for further debugging purpose and human evaluation process\n",
    "\n",
    "output_file_path = 'data/ISTQB_CTFL_Syllabus-v4.0_v03.txt'\n",
    "with open(output_file_path, 'w', encoding='utf-8') as out_file:\n",
    "    # Writing the extracted text to the output file\n",
    "    out_file.write(extracted_text)\n",
    "\n",
    "# Closing the stream\n",
    "output_string.close()\n",
    "\n",
    "# Printing message to indicate that the text has been saved to the file\n",
    "print(f\"Extracted text for 'The Certified Tester Foundation Level in Software Testing' pre processed version 0.3 saved to '{output_file_path}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your stop words list\n",
    "# stop_words = [\"v4.0\", \"Page\", \"74\", \"18\", \"15\", \"of\", \"2023-04-21\", \"©\", \"Certified Tester\", \"Foundation\", \"Level\", \"International Software Testing Qualifications Board\"]\n",
    "# \n",
    "#Split the extracted_text into words\n",
    "# words = extracted_text.split()\n",
    "# \n",
    "#Filter out words that are in the stop words list\n",
    "# filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "# \n",
    "#Join the filtered words back into a text\n",
    "# extracted_text = \" \".join(filtered_words)\n",
    "# \n",
    "#Print the cleaned text\n",
    "#print(extracted_text)\n",
    "# \n",
    "# \n",
    "# output_file_path = 'data/ISTQB_CTFL_Syllabus-v4.0_v05.txt'\n",
    "# with open(output_file_path, 'w', encoding='utf-8') as out_file:\n",
    "#    Writing the extracted text to the output file\n",
    "    # out_file.write(extracted_text)\n",
    "# \n",
    "#Closing the stream\n",
    "# output_string.close()\n",
    "# \n",
    "#Printing message to indicate that the text has been saved to the file\n",
    "# print(f\"Extracted text for 'The Certified Tester Foundation Level in Software Testing' pre processed version 0.5 saved to '{output_file_path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to lower case all words in stringIO\n",
    "#extracted_text = extracted_text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#punctuation\n",
    "# Load the language model\n",
    "#nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Process the text with SpaCy\n",
    "###doc = nlp(extracted_text)\n",
    "\n",
    "# Create a list of tokens that are not punctuation\n",
    "#filtered_tokens = [token.text for token in doc if not token.is_punct]\n",
    "\n",
    "# Join the filtered tokens back into a text\n",
    "#extracted_text = \" \".join(filtered_tokens)\n",
    "\n",
    "# Print the text without punctuation\n",
    "#print(extracted_text)\n",
    "\n",
    "#output_file_path = 'data/ISTQB_CTFL_Syllabus-v4.0_v04.txt'\n",
    "#with open(output_file_path, 'w', encoding='utf-8') as out_file:\n",
    "    # Writing the extracted text to the output file\n",
    "#    out_file.write(extracted_text)\n",
    "\n",
    "# Closing the stream\n",
    "#output_string.close()\n",
    "\n",
    "# Printing message to indicate that the text has been saved to the file\n",
    "#print(f\"Extracted text for 'The Certified Tester Foundation Level in Software Testing' pre processed version 0.1 saved to '{output_file_path}'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check results, looks like some parts are not removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted text for 'The Certified Tester Foundation Level in Software Testing' pre processed version 0.5 saved to 'data/ISTQB_CTFL_Syllabus-v4.0_v05.txt'\n"
     ]
    }
   ],
   "source": [
    "# built by chatgpt on provided context from my side, I used a part of text of file above, reviewed and customized by me as well\n",
    "#stop_words = [\"buxton\", \"a\", \"about\", \"above\", \"additional\", \"an\", \"and\", \"another\", \"are\", \"as\", \"be\", \"being\", \"by\", \"can\", \"common\", \"commonly\", \"do\", \"does\", \"each\", \"even\", \"for\", \"from\", \"has\", \"have\", \"in\", \"including\", \"is\", \"it\", \"its\", \"it's\", \"many\", \"may\", \"more\", \"most\", \"not\", \"of\", \"74\", \"often\", \"on\", \"or\", \"over\", \"such\", \"than\", \"that\", \"the\", \"there\", \"these\", \"this\", \"to\", \"under\", \"was\", \"we\", \"what\", \"when\", \"which\", \"who\", \"why\", \"will\", \"with\", \"within\", \"work\", \"you\", \"2023\", \"04\", \"21\", \"v4.0\", \"page\", \"2023-04-21\", \"©\", \"international\", \"qualifications\", \"board\", \"certified\", \"tester\",  \"foundation\", \"level\", \"FL-\", \"K2\", \"see\", \"section\" , \"didn't\", \"doesn't\", \"don't\", \"i.e.\", \"it's\", \"let's\", \"that's\", \"there's\", \"they're\", \"you're\", \"e.g.\"]\n",
    "stop_words = [ \"©\", \"15\", \"16\", \"17\", \"18\", \"19\", \"20\", r\"\\b20\\b\", \"21\", \"22\", \"23\", \"24\", \"25\", \"26\", \"27\", \"28\", \"29\", \"30\", \"31\", \"32\", \"33\", \"34\", \"35\", \"36\",\n",
    "              \"37\", \"38\",\"39\", \"40\",\"41\", \"42\",\"43\", \"44\",\"45\", \"46\", \"47\", \"48\", \"49\", \"50\", \"51\", \"52\", \"53\", \"54\", \n",
    "              \"International Software Testing Qualifications Board Certified Tester Foundation Level\", \"21.04.2023\", \"01.07.2021\",\n",
    "              \"11.11.2019\", \"27.04.2018\", \"1.04.2011\", \"30.03.2010\", \"01.05.2007\", \"01.07.2005\", \"25.02.1999\", \"the\", \"market\", \"(\", \")\", \"in\", \"or\"]\n",
    " \n",
    "# Regular expression pattern to match phrases like \"15 74\", \"16 74\", ..., \"54 74\"\n",
    "pattern = re.compile(r\"(?s)^v4.0.*Foundation Level$\", re.DOTALL)\n",
    "# Split the extracted_text into words\n",
    "words = re.split(r'\\s+', extracted_text)\n",
    "# \n",
    "# Filter out words that match the regular expression pattern or are in the stop words list\n",
    "filtered_words = [word for word in words if not re.match(pattern, word) and word.lower() not in stop_words] #match\n",
    "# Join the filtered words back into a text\n",
    "extracted_text = \" \".join(filtered_words)\n",
    "\n",
    "# Ph. removal\n",
    "phrase_to_remove = \"International Software Testing Qualifications Board Certified Tester Foundation Level\"\n",
    "phrase_to_remove_v = \"v4.0 Page of 74 2023-04-21\"\n",
    "\n",
    "# Replace the phrase with an empty string and comas removal (across the whole text)\n",
    "extracted_text = extracted_text.replace(phrase_to_remove, \"\")\n",
    "#extracted_text = extracted_text.replace(\",\", \"\")\n",
    "extracted_text = extracted_text.replace(phrase_to_remove_v, \"\")\n",
    "# Regular expression pattern to match and remove text inside brackets and brackets as well\n",
    "pattern_brackets = r'\\([^)]*\\)'\n",
    "\n",
    "# removal of text inside brackets and brackets\n",
    "extracted_text = re.sub(pattern_brackets, '', extracted_text)\n",
    "\n",
    "output_file_path = 'data/ISTQB_CTFL_Syllabus-v4.0_v05.txt' \n",
    "with open(output_file_path, 'w', encoding='utf-8') as out_file:\n",
    "    # Writing the extracted text to the output file\n",
    "    out_file.write(extracted_text)\n",
    "output_string.close()\n",
    " \n",
    "# Printing message to indicate that the text has been saved to the file\n",
    "print(f\"Extracted text for 'The Certified Tester Foundation Level in Software Testing' pre processed version 0.5 saved to '{output_file_path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pattern = r'[0-9]'\n",
    "\n",
    "# Match all digits in the string and replace them with an empty string\n",
    "#extracted_text = re.sub(pattern, '', extracted_text)\n",
    "\n",
    "#extracted_text = ''.join((x for x in extracted_text if not x.isdigit()))\n",
    "\n",
    "\n",
    "#output_file_path = 'data/ISTQB_CTFL_Syllabus-v4.0_v06.txt'\n",
    "#with open(output_file_path, 'w', encoding='utf-8') as out_file:\n",
    "    # Writing the extracted text to the output file\n",
    "#    out_file.write(extracted_text)\n",
    "# \n",
    "# Closing the stream\n",
    "#output_string.close()\n",
    "# \n",
    "# Printing message to indicate that the text has been saved to the file\n",
    "#print(f\"Extracted text for 'The Certified Tester Foundation Level in Software Testing' pre processed version 0.6 saved to '{output_file_path}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# # Load the language model\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# # Your text\n",
    "# text = extracted_text\n",
    "\n",
    "# # Process the text with SpaCy\n",
    "# doc = nlp(text)\n",
    "\n",
    "# # Create a StringIO object to store the NER results\n",
    "# output_string = io.StringIO()\n",
    "\n",
    "# # Extract named entities and write them to the StringIO object\n",
    "# for ent in doc.ents:\n",
    "#     output_string.write(f\"Entity: {ent.text}, Type: {ent.label_}\\n\")\n",
    "\n",
    "# # Get the NER results as a string\n",
    "# ner_results = output_string.getvalue()\n",
    "\n",
    "# output_file_path = 'data/NER.txt'\n",
    "# with open(output_file_path, 'w', encoding='utf-8') as out_file:\n",
    "#     # Writing the extracted text to the output file\n",
    "#       out_file.write(ner_results)\n",
    "\n",
    "# # Closing the stream\n",
    "# output_string.close()\n",
    "\n",
    "# # Printing message to indicate that the text has been saved to the file\n",
    "# print(f\"Extracted NER list for 'The Certified Tester Foundation Level in Software Testing; {output_file_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point NER dict is saved into /data folder, edited manually and now let us import this file into stop_list StringIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Check the content of stop_list_stringio\n",
    "#content = stop_list_stringio.getvalue()\n",
    "#print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Markov Chain\n",
    "# Sample text (replace with your extracted_text)\n",
    "# Tokenize the text into words\n",
    "#tokens = nltk.word_tokenize(extracted_text)\n",
    "\n",
    "# Create a dictionary to store transition probabilities\n",
    "#transition_probabilities = {}\n",
    "\n",
    "# Build the transition probability matrix\n",
    "#for i in range(len(tokens) - 1):\n",
    "#    current_token = tokens[i]\n",
    "#    next_token = tokens[i + 1]\n",
    "    \n",
    "#    if current_token in transition_probabilities:\n",
    "#        transition_probabilities[current_token].append(next_token)\n",
    "#    else:\n",
    "#        transition_probabilities[current_token] = [next_token]\n",
    "\n",
    "# Start with an initial word\n",
    "#current_word = random.choice(tokens)\n",
    "\n",
    "# Generate a sentence of a certain length\n",
    "#generated_text = [current_word]\n",
    "#sentence_length = 10\n",
    "\n",
    "#for _ in range(sentence_length - 1):\n",
    "#    if current_word in transition_probabilities:\n",
    "#        next_word = random.choice(transition_probabilities[current_word])\n",
    "#        generated_text.append(next_word)\n",
    "#        current_word = next_word\n",
    "#    else:\n",
    "#        break\n",
    "\n",
    "# Join the generated words into a sentence\n",
    "#generated_sentence = \" \".join(generated_text)\n",
    "#print(generated_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted text for 'The Certified Tester Foundation Level in Software Testing' pre processed version 0.7 saved to 'data/ISTQB_CTFL_Syllabus-v4.0_v07.txt'\n"
     ]
    }
   ],
   "source": [
    "# remove chapter 4 beginning\n",
    "\n",
    "# Define the regular expression pattern for the text to remove\n",
    "pattern = r'4\\. Test Analysis and Design – 390 minutes.*?(K3) Use acceptance test-driven development (ATDD) to derive test cases'\n",
    "\n",
    "# Use re.sub to replace the matched text with a marker (e.g., 'REMOVED')\n",
    "extracted_text = re.sub(pattern, \"\", extracted_text, flags=re.DOTALL)\n",
    "\n",
    "# Define the phrase you want to remove\n",
    "phrase_to_remove = \"Learning Objectives for Chapter 4:\"\n",
    "\n",
    "# Replace the phrase with an empty string\n",
    "extracted_text = extracted_text.replace(phrase_to_remove, \"\")\n",
    "# \n",
    "output_file_path = 'data/ISTQB_CTFL_Syllabus-v4.0_v07.txt'\n",
    "with open(output_file_path, 'w', encoding='utf-8') as out_file:\n",
    "    # Writing the extracted text to the output file\n",
    "    out_file.write(extracted_text)\n",
    "# \n",
    "# Closing the stream\n",
    "output_string.close()\n",
    "# \n",
    "# Printing message to indicate that the text has been saved to the file\n",
    "print(f\"Extracted text for 'The Certified Tester Foundation Level in Software Testing' pre processed version 0.7 saved to '{output_file_path}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted text for 'The Certified Tester Foundation Level in Software Testing' pre processed version 0.8 saved to 'data/ISTQB_CTFL_Syllabus-v4.0_v08.txt'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# remove chapter 4 beginning\n",
    "# Define the regular expression pattern to remove the desired text\n",
    "pattern = r'4\\.1 Test Techniques Overview.*?4\\.5\\.3 \\(K3\\) Use acceptance test-driven development \\(ATDD\\) to derive test cases'\n",
    "\n",
    "# Use re.sub to replace the matched text with an empty string\n",
    "extracted_text = re.sub(pattern, '', extracted_text, flags=re.DOTALL)\n",
    "\n",
    "\n",
    "\n",
    "output_file_path = 'data/ISTQB_CTFL_Syllabus-v4.0_v08.txt'\n",
    "with open(output_file_path, 'w', encoding='utf-8') as out_file:\n",
    "    # Writing the extracted text to the output file\n",
    "    out_file.write(extracted_text)\n",
    "# \n",
    "# Closing the stream\n",
    "output_string.close()\n",
    "# \n",
    "# Printing message to indicate that the text has been saved to the file\n",
    "print(f\"Extracted text for 'The Certified Tester Foundation Level in Software Testing' pre processed version 0.8 saved to '{output_file_path}'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted text for 'The Certified Tester Foundation Level in Software Testing' pre processed version 0.9 saved to 'data/ISTQB_CTFL_Syllabus-v4.0_v09.txt'\n"
     ]
    }
   ],
   "source": [
    "# remove chapter 3 beginning\n",
    "# Define the regular expression pattern for the text to remove\n",
    "pattern = r'3\\. Static Testing – 80 minutes.*?FL-3\\.2\\.5 \\(K1\\) Recall the factors that contribute to a successful review'\n",
    "\n",
    "# Use re.sub to replace the matched text with a marker (e.g., 'REMOVED')\n",
    "extracted_text = re.sub(pattern, \"\", extracted_text, flags=re.DOTALL)\n",
    "\n",
    "# Define the phrase you want to remove\n",
    "phrase_to_remove = \"Learning Objectives for Chapter 4:\"\n",
    "\n",
    "# Replace the phrase with an empty string\n",
    "extracted_text = extracted_text.replace(phrase_to_remove, \"\")\n",
    "# \n",
    "output_file_path = 'data/ISTQB_CTFL_Syllabus-v4.0_v09.txt'\n",
    "with open(output_file_path, 'w', encoding='utf-8') as out_file:\n",
    "    # Writing the extracted text to the output file\n",
    "    out_file.write(extracted_text)\n",
    "# \n",
    "# Closing the stream\n",
    "output_string.close()\n",
    "# \n",
    "# Printing message to indicate that the text has been saved to the file\n",
    "print(f\"Extracted text for 'The Certified Tester Foundation Level in Software Testing' pre processed version 0.9 saved to '{output_file_path}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted text for 'The Certified Tester Foundation Level in Software Testing' pre processed version 0.10 saved to 'data/ISTQB_CTFL_Syllabus-v4.0_v10.txt'\n"
     ]
    }
   ],
   "source": [
    "# remove chapter 2 beginning\n",
    "\n",
    "# Define the regular expression pattern for the text to remove\n",
    "pattern = r'2\\. Testing Throughout the Software Development Lifecycle.*?FL-2\\.3\\.1 \\(K2\\) Summarize maintenance testing and its triggers'\n",
    "# Use re.sub to replace the matched text with a marker (e.g., 'REMOVED')\n",
    "extracted_text = re.sub(pattern, \"\", extracted_text, flags=re.DOTALL)\n",
    "\n",
    "# Define the phrase you want to remove\n",
    "phrase_to_remove = \"Learning Objectives for Chapter 4:\"\n",
    "\n",
    "# Replace the phrase with an empty string\n",
    "extracted_text = extracted_text.replace(phrase_to_remove, \"\")\n",
    "# \n",
    "output_file_path = 'data/ISTQB_CTFL_Syllabus-v4.0_v10.txt'\n",
    "with open(output_file_path, 'w', encoding='utf-8') as out_file:\n",
    "    # Writing the extracted text to the output file\n",
    "    out_file.write(extracted_text)\n",
    "# \n",
    "# Closing the stream\n",
    "output_string.close()\n",
    "# \n",
    "# Printing message to indicate that the text has been saved to the file\n",
    "print(f\"Extracted text for 'The Certified Tester Foundation Level in Software Testing' pre processed version 0.10 saved to '{output_file_path}'\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted text for 'The Certified Tester Foundation Level in Software Testing' pre processed version 0.11 saved to 'data/ISTQB_CTFL_Syllabus-v4.0_v11.txt'\n"
     ]
    }
   ],
   "source": [
    "# remove chapter 2 beginning\n",
    "\n",
    "# Define the regular expression pattern for the text to remove\n",
    "\n",
    "pattern = r'5\\. Managing the Test Activities – 335 minutes.*?FL-5\\.5\\.1 \\(K3\\) Prepare a defect report'\n",
    "# Use re.sub to replace the matched text with a marker (e.g., 'REMOVED')\n",
    "extracted_text = re.sub(pattern, \"\", extracted_text, flags=re.DOTALL)\n",
    "\n",
    "# Define the phrase you want to remove\n",
    "phrase_to_remove = \"Learning Objectives for Chapter 4:\"\n",
    "\n",
    "# Replace the phrase with an empty string\n",
    "extracted_text = extracted_text.replace(phrase_to_remove, \"\")\n",
    "# \n",
    "output_file_path = 'data/ISTQB_CTFL_Syllabus-v4.0_v11.txt'\n",
    "with open(output_file_path, 'w', encoding='utf-8') as out_file:\n",
    "    # Writing the extracted text to the output file\n",
    "    out_file.write(extracted_text)\n",
    "# \n",
    "# Closing the stream\n",
    "output_string.close()\n",
    "# \n",
    "# Printing message to indicate that the text has been saved to the file\n",
    "print(f\"Extracted text for 'The Certified Tester Foundation Level in Software Testing' pre processed version 0.11 saved to '{output_file_path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#pattern = r'\\d+\\.\\d+\\.\\d+\\.'\n",
    "#matches = re.findall(pattern, extracted_text)\n",
    "\n",
    "#for match in matches:\n",
    "#    match_without_dot = match[:-1]  # Remove the last dot\n",
    "#    print(match_without_dot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted text for 'The Certified Tester Foundation Level in Software Testing' pre processed version 0.12 saved to 'data/ISTQB_CTFL_Syllabus-v4.0_v12.txt'\n"
     ]
    }
   ],
   "source": [
    "# Remove bullet points using regular expressions\n",
    "extracted_text = re.sub(r'•', '', extracted_text)\n",
    "\n",
    "output_file_path = 'data/ISTQB_CTFL_Syllabus-v4.0_v12.txt'\n",
    "with open(output_file_path, 'w', encoding='utf-8') as out_file:\n",
    "    # Writing the extracted text to the output file\n",
    "    out_file.write(extracted_text)\n",
    "# \n",
    "# Closing the stream\n",
    "output_string.close()\n",
    "# \n",
    "# Printing message to indicate that the text has been saved to the file\n",
    "print(f\"Extracted text for 'The Certified Tester Foundation Level in Software Testing' pre processed version 0.12 saved to '{output_file_path}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a regular expression pattern to match section titles\n",
    "#section_pattern = r'\\d+\\.\\d+\\.\\d+\\.'\n",
    "\n",
    "# using combined reg. exp to extract 1.1.1. and 1.2.\n",
    "#section_pattern_3d = r'\\d+\\.\\d+\\.\\d+\\.'  # Pattern for \"1.1.1.\"\n",
    "#section_pattern_2d = r'\\d+\\.\\d+\\.'    # Pattern for \"1.2.\"\n",
    "#combined_pattern = f\"({section_pattern_3d}|{section_pattern_2d})\"#\n",
    "\n",
    "## Using re.finditer to find all section titles and their starting positions\n",
    "#section_matches = re.finditer(combined_pattern, extracted_text)#\n",
    "\n",
    "## Create lists to store sections\n",
    "#sections = []#\n",
    "\n",
    "## Iterate through section matches\n",
    "#for match in section_matches:\n",
    "#    start_pos = match.start()\n",
    "#    end_pos = (\n",
    "#        match.end()\n",
    "#        if match.end() < len(extracted_text)\n",
    "#        else len(extracted_text)\n",
    "#    )\n",
    "#    section_title = match.group().strip()\n",
    "#    \n",
    "#    # Remove the last dot from the section title\n",
    "#    section_title = section_title[:-1]  # Remove the last dot\n",
    "#    \n",
    "#    try:\n",
    "#        # Find the corresponding section content based on section title position\n",
    "#        next_match = next(section_matches)\n",
    "#        content_start = end_pos\n",
    "#        content_end = (\n",
    "#            next_match.start()\n",
    "#            if content_start < len(extracted_text)\n",
    "#            else len(extracted_text)\n",
    "#        )\n",
    "#        section_content = extracted_text[content_start:content_end].strip()\n",
    "#    except StopIteration:\n",
    "#        # Handle the case when there are no more matches\n",
    "#        section_content = extracted_text[end_pos:].strip()\n",
    "#    \n",
    "#    sections.append((section_title, section_content))#\n",
    "\n",
    "## Print the extracted sections\n",
    "#if sections:\n",
    "#    for section in sections:\n",
    "#        print(\"Section Title:\", section[0])\n",
    "#        print(\"Section Content:\", section[1])\n",
    "#        print(\"-\" * 40)\n",
    "#else:\n",
    "#    print(\"No sections found in the text, you did something wrong check once more\")#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(sections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding name?\n",
    "\n",
    "#text = \"1.1.2. Testing and Debugging\\nTesting and debugging are separate activities. Testing can trigger failures that are caused by defects in the software (dynamic testing) or can directly find defects in the test object (static testing). When dynamic testing (see chapter 4) triggers a failure, debugging is concerned with finding causes of this failure (defects), analyzing these causes, and eliminating the\"\n",
    "\n",
    "# Define a regular expression pattern to match section titles and names\n",
    "# section_pattern = r'(\\d+\\.\\d+\\.\\d+\\.\\s[^\\n]+)'\n",
    "# \n",
    "#Using re.finditer to find all section titles and their starting positions\n",
    "# section_matches = re.finditer(section_pattern, extracted_text)\n",
    "# \n",
    "#Create lists to store sections\n",
    "# sections = []\n",
    "# \n",
    "#Iterate through section matches\n",
    "# for match in section_matches:\n",
    "    # section_info = match.group(1).strip()\n",
    "# \n",
    " #   Find the corresponding section content based on section title position\n",
    "    # start_pos = match.end()\n",
    "    # end_pos = (\n",
    "        # next(section_matches, None)\n",
    "        # if start_pos < len(text)\n",
    "        # else None\n",
    "    # )\n",
    "    # \n",
    "    # if end_pos:\n",
    "        # section_content = text[start_pos:end_pos.start()].strip()\n",
    "    # else:\n",
    "        # section_content = text[start_pos:].strip()\n",
    "# \n",
    "    # sections.append((section_info, section_content))\n",
    "# \n",
    "#Print the extracted sections\n",
    "# if sections:\n",
    "    # for section in sections:\n",
    "        # print(\"Section Info:\", section[0])\n",
    "        # print(\"Section Content:\", section[1])\n",
    "        # print(\"-\" * 40)\n",
    "# else:\n",
    "    # print(\"No sections found in the text.\")\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DRAFT\n",
    "\n",
    "# Sample text (replace this with your actual text)\n",
    "\n",
    "# Define a regular expression pattern to match section titles\n",
    "# section_pattern = r'\\b\\d+\\.\\d+(?:\\.\\d+)?(?=\\s)'\n",
    "# \n",
    "#Using re.finditer to find all section titles and their starting positions\n",
    "# section_matches = re.finditer(section_pattern, extracted_text)\n",
    "# \n",
    "#Create lists to store sections\n",
    "# sections = []\n",
    "# \n",
    "#Iterate through section matches\n",
    "# for match in section_matches:\n",
    "    # start_pos = match.start()\n",
    "    # end_pos = (\n",
    "        # match.end()\n",
    "        # if match.end() < len(extracted_text)\n",
    "        # else len(extracted_text)\n",
    "    # )\n",
    "    # section_title = match.group().strip()\n",
    "    # \n",
    "#   Remove the last dot from the section title\n",
    "    # section_title = section_title[:-1]  # Remove the last dot\n",
    "    # \n",
    "#    Find the corresponding section content based on section title position\n",
    "    # content_start = end_pos\n",
    "    # content_end = (\n",
    "        # next(section_matches).start()\n",
    "        # if content_start < len(extracted_text)\n",
    "        # else len(extracted_text)\n",
    "    # )\n",
    "    # section_content = extracted_text[content_start:content_end].strip()\n",
    "    # \n",
    "    # sections.append((section_title, section_content))\n",
    "# \n",
    "#Print the extracted sections\n",
    "# if sections:\n",
    "    # for section in sections:\n",
    "        # print(\"Section Title:\", section[0])\n",
    "        # print(\"Section Content:\", section[1])\n",
    "        # print(\"-\" * 40)\n",
    "# else:\n",
    "    # print(\"No sections found in the text.\")\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!Base Modeling!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "# Load the T5 model and tokenizer\n",
    "model_name = \"t5-small\"\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Read the text from the StringIO file (replace with your own text)\n",
    "#extracted_text = \"Your text goes here.\"\n",
    "\n",
    "# Tokenize the text\n",
    "inputs = tokenizer(\"summarize: \" + extracted_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "\n",
    "# Generate keywords one at a time\n",
    "num_keywords = 150  # Adjust as needed\n",
    "keywords = []\n",
    "\n",
    "for _ in range(num_keywords):\n",
    "    output = model.generate(inputs[\"input_ids\"], max_length=len(keywords) + 1, num_return_sequences=1, no_repeat_ngram_size=2)\n",
    "    keyword = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    keywords.append(keyword)\n",
    "\n",
    "# Print the generated keywords\n",
    "print(\"Keywords:\", keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#paraphrasing text --> takes too long\n",
    "\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "# Load the T5 model and tokenizer\n",
    "model_name = \"t5-large\"\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    " \n",
    "# Tokenize the text\n",
    "inputs = tokenizer(\"paraphrase: \" + extracted_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "\n",
    "# Generate paraphrased sentences\n",
    "num_paraphrases = 155  # Adjust as needed\n",
    "paraphrases = []\n",
    "\n",
    "for _ in range(num_paraphrases):\n",
    "    output = model.generate(inputs[\"input_ids\"], max_length=512, num_return_sequences=1, no_repeat_ngram_size=2)\n",
    "    paraphrase = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    paraphrases.append(paraphrase)\n",
    "\n",
    "# Print the generated paraphrases\n",
    "for i, paraphrase in enumerate(paraphrases):\n",
    "    print(f\"Paraphrase {i + 1}: {paraphrase}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generation of summary\n",
    "#Check if there are sections available before accessing them\n",
    "if len(sections) >= 2:\n",
    "    # Load the pre-trained T5 model and tokenizer\n",
    "    model_name = \"t5-large\"\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "    tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "    for section_index, (section_title, section_content) in enumerate(sections):\n",
    "        # Tokenize the input section\n",
    "        input_ids = tokenizer.encode(\"summarize: \" + section_content, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "\n",
    "        # Generate the summary\n",
    "        summary_ids = model.generate(input_ids, max_length=150, min_length=30, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "        summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "        # Print the summary for each section\n",
    "        print(f\"Summary for Section {section_index + 1} - Title: {section_title}\")\n",
    "        print(\"Summary:\", summary)\n",
    "        print(\"-\" * 40)\n",
    "else:\n",
    "    print(\"Not enough sections found in the list.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1: software testing is the process of evaluating software to identify defects. Keywords: Software testing, process, defects, defect detection, and defect identification.\n"
     ]
    }
   ],
   "source": [
    "# summary + keywords 0 T5 questions\n",
    "\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "# Load the T5 model and tokenizer\n",
    "model_name = \"t5-small\"\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Sample summary and keywords (replace with your own data)\n",
    "summary = \"Software testing is the process of evaluating software to identify defects.\"\n",
    "keywords = [\"Software testing\", \"process\", \"defects\"]\n",
    "\n",
    "# Prepare input for question generation\n",
    "input_text = f\"Summary: {summary} Keywords: {', '.join(keywords)}\"\n",
    "\n",
    "# Tokenize the input\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "\n",
    "# Generate questions\n",
    "output = model.generate(inputs[\"input_ids\"], max_length=512, num_return_sequences=1, no_repeat_ngram_size=2)\n",
    "\n",
    "# Decode and print the generated questions\n",
    "generated_questions = [tokenizer.decode(question, skip_special_tokens=True) for question in output]\n",
    "for i, question in enumerate(generated_questions, 1):\n",
    "    print(f\"Question {i}: {question}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# questions generation\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Load the pre-trained model and tokenizer for question generation\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"PrimeQA/mt5-base-tydi-question-generator\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"PrimeQA/mt5-base-tydi-question-generator\")\n",
    "\n",
    "# Function to generate a question for a given summary\n",
    "def generate_question(summary, max_length=64):\n",
    "    # Tokenize the input text and generate the question\n",
    "    features = tokenizer([summary], return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "    output = model.generate(input_ids=features['input_ids'], \n",
    "                            attention_mask=features['attention_mask'],\n",
    "                            max_length=max_length,\n",
    "                            num_return_sequences=1)\n",
    "    \n",
    "    return tokenizer.decode(output[0])\n",
    "\n",
    "# Example usage:\n",
    "summary = \"defect management process includes a workflow for handling individual anomalies from their discovery to their closure and rules for their classification. process must be followed by all involved stakeholders. a defect report logged during dynamic testing typically includes: Unique identifier Title with a short summary of anomaly being reported.\"\n",
    "question = generate_question(summary)\n",
    "print(\"Generated Question:\", question)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#experiment!!!\n",
    "\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "\n",
    "# Load the pre-trained BART model and tokenizer\n",
    "model_name = \"facebook/bart-large-cnn\"  # You can choose a different model size if needed\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Function to generate an answer for a given question and context\n",
    "def generate_answer(question, context, max_length=64):\n",
    "    # Prepare the input text by combining the question and context\n",
    "    input_text = f\"question: {question} context: {context}\"\n",
    "\n",
    "    # Tokenize the input text\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "\n",
    "    # Generate the answer\n",
    "    answer_ids = model.generate(input_ids, max_length=max_length, num_beams=4, early_stopping=True, num_return_sequences=1)\n",
    "\n",
    "    # Decode the generated answer\n",
    "    answer = tokenizer.decode(answer_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    return answer\n",
    "\n",
    "# Example usage:\n",
    "#context = \"defect management process includes a workflow for handling individual anomalies from their discovery to their closure and rules for their classification. process must be followed by all involved stakeholders. a defect report logged during dynamic testing typically includes: Unique identifier Title with a short summary of anomaly being reported.\"\n",
    "#question = \"What is the most common defect management process?\"\n",
    "\n",
    "# Generate the answer\n",
    "generated_answer = generate_answer(question, context)\n",
    "print(\"Generated Answer:\", generated_answer)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "\n",
    "# Load the pre-trained BART model and tokenizer\n",
    "model_name = \"facebook/bart-large-cnn\"\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Function to generate an answer for a given question and context\n",
    "def generate_answer(question, context, max_length=64):\n",
    "    # Prepare the input text by combining the question and context\n",
    "    input_text = f\"question: {question} context: {context}\"\n",
    "\n",
    "    # Tokenize the input text\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "\n",
    "    # Generate the answer\n",
    "    answer_ids = model.generate(input_ids, max_length=max_length, num_beams=4, early_stopping=True, num_return_sequences=1)\n",
    "\n",
    "    # Decode the generated answer\n",
    "    answer = tokenizer.decode(answer_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    return answer\n",
    "\n",
    "# Example usage:\n",
    "#context = \"defect management process includes a workflow for handling individual anomalies from their discovery to their closure and rules for their classification. process must be followed by all involved stakeholders. a defect report logged during dynamic testing typically includes: Unique identifier Title with a short summary of anomaly being reported.\"\n",
    "context = \"testing provides a cost-effective means of detecting defects.\"\n",
    "question = \"What provides a cost-effective means of detecting defects?\"\n",
    "\n",
    "# Generate the answer\n",
    "generated_answer = generate_answer(question, context)\n",
    "print(\"Generated Answer:\", generated_answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keywords extraction - missing so far, leads to HTTPSConnectionPool(host='cdn-lfs.huggingface.co', port=443): Read timed out. A lot of hours are required. \n",
    "# Talk to teachers\n",
    "\n",
    "# Load the pre-trained KeyBERT model\n",
    "#model = KeyBERT(\"distilbert-base-nli-mean-tokens\")\n",
    "\n",
    "# Input text (use sections[0][1] as the content of the first section)\n",
    "#section_content = sections[0][1]\n",
    "\n",
    "# Extract keywords\n",
    "#try:\n",
    "#    keywords = model.extract_keywords(section_content, keyphrase_ngram_range=(1, 2), stop_words='english', use_mmr=True, top_n=10, resume_download=True)\n",
    "    \n",
    "    # Print the extracted keywords\n",
    "#    for keyword in keywords:\n",
    "#        print(keyword)\n",
    "#except Exception as e:\n",
    "#    print(\"An error occurred:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sentences = sent_tokenize(summary) \n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Function to generate a fixed number of questions from sentences using trigrams\n",
    "def generate_questions(text, num_questions=20):\n",
    "    questions = []\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Function to generate a fixed number of questions from sentences using trigrams\n",
    "def generate_questions(text, num_questions=30):\n",
    "    questions = []\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # Define question templates with different prefixes\n",
    "    question_templates = [\"What\", \"What is\", \"What can\", \"What does\"]\n",
    "    \n",
    "    for sentence in text:\n",
    "        # Tokenize each sentence into words\n",
    "        words = nltk.word_tokenize(sentence)\n",
    "        # Generate n-grams (trigrams) from the words\n",
    "        n_grams = list(ngrams(words, 3))\n",
    "\n",
    "        # Construct questions using the trigrams and different question templates\n",
    "        for n_gram in n_grams:\n",
    "            if (\n",
    "                n_gram[-1].lower() not in stop_words \n",
    "                and n_gram[-1].lower() != n_gram[-2].lower()\n",
    "                and \"can\" not in n_gram\n",
    "            ):\n",
    "                for template in question_templates:\n",
    "                    question = f\"{template} {n_gram[0]} {n_gram[1]} {n_gram[2]}?\"\n",
    "                    questions.append(question)\n",
    "\n",
    "            # Stop generating questions if we reach the desired number\n",
    "            if len(questions) >= num_questions:\n",
    "                return questions\n",
    "\n",
    "    return questions[:num_questions]  # Return only the specified number of questions\n",
    "\n",
    "# Generate 20 questions from the sentences using trigrams and different prefixes\n",
    "questions = generate_questions(sentences, num_questions=30)\n",
    "\n",
    "# Print the generated questions\n",
    "for i, question in enumerate(questions, start=1):\n",
    "    print(f\"Question {i}: {question}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#distractors generation\n",
    "\n",
    "import random\n",
    "\n",
    "def generate_distractors(correct_answer, num_distractors, word_embedding_model=None):\n",
    "    \"\"\"\n",
    "    Generate distractors for a given correct answer.\n",
    "\n",
    "    Args:\n",
    "    - correct_answer (str): The correct answer.\n",
    "    - num_distractors (int): The number of distractors to generate.\n",
    "    - word_embedding_model: A pre-trained word embedding model (e.g., Word2Vec or GloVe).\n",
    "\n",
    "    Returns:\n",
    "    - distractors (list): A list of generated distractors.\n",
    "    \"\"\"\n",
    "    distractors = []\n",
    "    \n",
    "    # You can start by making simple modifications to the correct answer, such as replacing words.\n",
    "    for _ in range(num_distractors):\n",
    "        # Randomly select a distractor generation strategy\n",
    "        strategy = random.choice([\"replace\", \"synonym\"])\n",
    "        \n",
    "        if strategy == \"replace\":\n",
    "            # Replace a random word in the correct answer with a random word\n",
    "            correct_answer_words = correct_answer.split()\n",
    "            word_to_replace = random.choice(correct_answer_words)\n",
    "            replacement_word = \"replacement_word\"  # Replace this with logic to select a random word\n",
    "            distractor = correct_answer.replace(word_to_replace, replacement_word)\n",
    "        elif strategy == \"synonym\" and word_embedding_model:\n",
    "            # Find a synonym for a random word in the correct answer using word embeddings\n",
    "            correct_answer_words = correct_answer.split()\n",
    "            word_to_replace = random.choice(correct_answer_words)\n",
    "            synonyms = find_synonyms(word_to_replace, word_embedding_model)\n",
    "            if synonyms:\n",
    "                replacement_word = random.choice(synonyms)\n",
    "                distractor = correct_answer.replace(word_to_replace, replacement_word)\n",
    "            else:\n",
    "                # If no synonyms are found, use a fallback strategy (e.g., random replacement)\n",
    "                distractor = generate_random_distractor(correct_answer)\n",
    "        else:\n",
    "            # Use a fallback strategy (e.g., random replacement) if no other strategy is chosen\n",
    "            distractor = generate_random_distractor(correct_answer)\n",
    "\n",
    "        distractors.append(distractor)\n",
    "    \n",
    "    return distractors\n",
    "\n",
    "def find_synonyms(word, word_embedding_model):\n",
    "    \"\"\"\n",
    "    Find synonyms for a word using a word embedding model.\n",
    "\n",
    "    Args:\n",
    "    - word (str): The target word.\n",
    "    - word_embedding_model: A pre-trained word embedding model (e.g., Word2Vec or GloVe).\n",
    "\n",
    "    Returns:\n",
    "    - synonyms (list): A list of synonyms for the target word.\n",
    "    \"\"\"\n",
    "    # Replace this with code to find synonyms using the word embedding model\n",
    "    # This may involve querying the model's word vectors to find similar words.\n",
    "    synonyms = []\n",
    "    return synonyms\n",
    "\n",
    "def generate_random_distractor(correct_answer):\n",
    "    \"\"\"\n",
    "    Generate a random distractor by replacing a random word in the correct answer.\n",
    "\n",
    "    Args:\n",
    "    - correct_answer (str): The correct answer.\n",
    "\n",
    "    Returns:\n",
    "    - distractor (str): The generated distractor.\n",
    "    \"\"\"\n",
    "    correct_answer_words = correct_answer.split()\n",
    "    word_to_replace = random.choice(correct_answer_words)\n",
    "    replacement_word = \"replacement_word\"  # Replace this with logic to select a random word\n",
    "    distractor = correct_answer.replace(word_to_replace, replacement_word)\n",
    "    return distractor\n",
    "\n",
    "# Example usage:\n",
    "#correct_answer = \"testing provides a Cost-effective Means of Detecting Defects.\"\n",
    "num_distractors = 3\n",
    "distractors = generate_distractors(correct_answer, num_distractors)\n",
    "\n",
    "# Print the correct answer and distractors\n",
    "print(\"Correct Answer:\", correct_answer)\n",
    "print(\"Distractors:\", distractors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !!! possible training of T5 !!!\n",
    "\n",
    "#import torch\n",
    "#from transformers import T5Tokenizer, T5ForConditionalGeneration, T5Config\n",
    "#from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "#from datasets import load_dataset\n",
    "#\n",
    "## Load your custom dataset using the datasets library\n",
    "#dataset = load_dataset('your_custom_dataset')\n",
    "#\n",
    "## Initialize the tokenizer and model\n",
    "#tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "#config = T5Config.from_pretrained(\"t5-small\")\n",
    "#model = T5ForConditionalGeneration.from_pretrained(\"t5-small\", config=config)\n",
    "#\n",
    "## Tokenize the dataset\n",
    "#def tokenize_function(examples):\n",
    "#    return tokenizer(examples['input_text'], examples['target_text'], padding='max_length', truncation=True)\n",
    "#\n",
    "#tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "#\n",
    "## Define training arguments\n",
    "#training_args = Seq2SeqTrainingArguments(\n",
    "#    per_device_train_batch_size=4,\n",
    "#    output_dir=\"./t5-fine-tuned\",\n",
    "#    num_train_epochs=3,\n",
    "#    evaluation_strategy=\"steps\",\n",
    "#    save_steps=10_000,\n",
    "#    eval_steps=10_000,\n",
    "#    save_total_limit=2,\n",
    "#)\n",
    "#\n",
    "## Initialize the trainer\n",
    "#trainer = Seq2SeqTrainer(\n",
    "#    model=model,\n",
    "#    args=training_args,\n",
    "#    data_collator=tokenized_datasets.data_collator,\n",
    "#    train_dataset=tokenized_datasets[\"train\"],\n",
    "#    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "#)\n",
    "#\n",
    "## Fine-tune the model\n",
    "#trainer.train()\n",
    "#\n",
    "## Save the fine-tuned model\n",
    "#model.save_pretrained(\"t5-fine-tuned\")\n",
    "#tokenizer.save_pretrained(\"t5-fine-tuned\")\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here comes gradio + manual selection of correct questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained model and tokenizer\n",
    "#model_name = \"bert-large-uncased-whole-word-masking-finetuned-squad\"\n",
    "#tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "#model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "\n",
    "# Provide a passage and a question\n",
    "#passage = extracted_text\n",
    "#question = \"Which of the following statements describe a valid test objective?\"\n",
    "\n",
    "#Which of the following statements describe a valid test objective?\n",
    "#What does not work as expected?\n",
    "\n",
    "# Tokenize the passage and question\n",
    "#inputs = tokenizer(question, passage, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "# Get the answer from the model\n",
    "#start_scores, end_scores = model(**inputs, return_dict = False)\n",
    "#start_idx = torch.argmax(start_scores)\n",
    "#end_idx = torch.argmax(end_scores)\n",
    "\n",
    "# Decode the answer from the tokenized output\n",
    "#answer_tokens = inputs[\"input_ids\"][0][start_idx:end_idx + 1]\n",
    "#answer = tokenizer.decode(answer_tokens)\n",
    "\n",
    "#print(\"Answer:\", answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To process text and generate questions with answers, you can consider using pre-trained language models, such as GPT-3, GPT-4, BERT, T5, or similar models. Each of these models has its strengths and can be used for different aspects of question generation and answering:\n",
    "\n",
    "T5 (Text-to-Text Transfer Transformer): T5 is a versatile language model that can be fine-tuned for various natural language processing tasks, including question generation. You can fine-tune a pre-trained T5 model on your specific dataset to generate high-quality questions.\n",
    "\n",
    "GPT-3: OpenAI's GPT-3 is a powerful language model known for its natural language generation capabilities. You can prompt GPT-3 to generate questions based on your input text. It can produce contextually relevant questions, but it may require careful instruction and filtering of the generated output.\n",
    "\n",
    "BART (Bidirectional and Auto-Regressive Transformers): BART is another transformer-based model that can be fine-tuned for question generation tasks. It excels in text generation tasks and can produce coherent and meaningful questions.\n",
    "\n",
    "XLNet: XLNet is a transformer model that has achieved strong performance in various NLP tasks. It can be fine-tuned for question generation, and its bidirectional context modeling can lead to better question generation.\n",
    "\n",
    "BERT (Bidirectional Encoder Representations from Transformers): BERT can also be used for question generation by fine-tuning. While it was originally designed for understanding context, it can be adapted for question generation with appropriate training data.\n",
    "\n",
    "UniLM: UniLM is a model that can be used for various text generation tasks, including question generation. It combines unidirectional, bidirectional, and sequence-to-sequence learning, making it versatile for NLP tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "# from io import StringIO\n",
    "\n",
    "# # Load the pre-trained model and tokenizer\n",
    "# model_name = \"bert-large-uncased-whole-word-masking-finetuned-squad\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "\n",
    "# # Create a StringIO object with your text\n",
    "# text_io = StringIO()\n",
    "# text_io.write(\"Your text goes here.\")\n",
    "# text_io.seek(0)  # Reset the StringIO object to the beginning\n",
    "\n",
    "# # Read the text from the StringIO object and convert it to a regular string\n",
    "# text = text_io.read()\n",
    "\n",
    "# # Provide a question\n",
    "# question = \"What is the answer to my question?\"\n",
    "\n",
    "# # Tokenize the text and question\n",
    "# inputs = tokenizer(question, text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "# # Get the answer from the model\n",
    "# start_scores, end_scores = model(**inputs)\n",
    "# start_idx = torch.argmax(start_scores)\n",
    "# end_idx = torch.argmax(end_scores)\n",
    "\n",
    "# # Decode the answer from the tokenized output\n",
    "# answer_tokens = inputs[\"input_ids\"][0][start_idx:end_idx + 1]\n",
    "# answer = tokenizer.decode(answer_tokens)\n",
    "\n",
    "# print(\"Answer:\", answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Question Generation using Seq2Seq (T5)\n",
    "\n",
    "# Load the pre-trained Seq2Seq model for question generation\n",
    "# question_generation_model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "# question_generation_tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "# \n",
    "#ISTQB document (replace with your actual content)\n",
    "# istqb_document = \"\"\"\n",
    "# 1.1. What is Testing? \n",
    "# \n",
    "# Software systems are an integral part of our daily life. Most people have had experience with software \n",
    "# that did not work as expected. Software that does not work correctly can lead to many problems, \n",
    "# including loss of money, time or business reputation, and, in extreme cases, even injury or death. \n",
    "# Software testing assesses software quality and helps reducing the risk of software failure in operation. \n",
    "# \n",
    "# Software testing is a set of activities to discover defects and evaluate the quality of software artifacts. \n",
    "# These artifacts, when being tested, are known as test objects. A common misconception about testing is \n",
    "# that it only consists of executing tests (i.e., running the software and checking the test results). However, \n",
    "# software testing also includes other activities and must be aligned with the software development lifecycle \n",
    "# (see chapter 2). \n",
    "# \n",
    "# Another common misconception about testing is that testing focuses entirely on verifying the test object. \n",
    "# Whilst testing involves verification, i.e., checking whether the system meets specified requirements, it also \n",
    "# involves validation, which means checking whether the system meets users’ and other stakeholders’ \n",
    "# needs in its operational environment. \n",
    "# \"\"\"\n",
    "# \n",
    "#Generate questions from the ISTQB document\n",
    "# def generate_questions(document, max_length=64, num_questions=1):\n",
    "    # inputs = question_generation_tokenizer.encode(\"generate questions: \" + document, return_tensors=\"pt\", max_length=max_length, truncation=True)\n",
    "    # questions = question_generation_model.generate(inputs, max_length=max_length, num_return_sequences=num_questions)\n",
    "    # return [question_generation_tokenizer.decode(question, skip_special_tokens=True) for question in questions]\n",
    "# \n",
    "# generated_questions = generate_questions(istqb_document)\n",
    "# \n",
    "#Print generated questions\n",
    "# for question in generated_questions:\n",
    "    # print(\"Question:\", question)\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is template for Quize layout, needs to be re-worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to display sections\n",
    "def display_sections(section_title, sections):\n",
    "    # Find the selected section in the list based on the section title\n",
    "    selected_section = next((s for s in sections if s[0] == section_title), None)\n",
    "\n",
    "    if selected_section:\n",
    "        _, section_content = selected_section\n",
    "        return section_content\n",
    "    else:\n",
    "        return \"Section not found\"\n",
    "\n",
    "# Get a list of section titles from the sections list\n",
    "section_titles = [section for section in sections]\n",
    "\n",
    "# Create a Gradio interface with a dropdown list of section titles and a textbox for content\n",
    "iface = gr.Interface(\n",
    "    fn=display_sections,\n",
    "    inputs=[gr.inputs.Dropdown(section_titles, label=\"Select a Section\"), gr.inputs.Textbox(lines = 10, default=\"Selected Section Content\", label=\"Section Content\")],\n",
    "    outputs=\"text\",\n",
    "    flagging_options = ['yes', 'no'],\n",
    "    theme=gr.themes.Soft()\n",
    ")\n",
    "\n",
    "iface.launch(share=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JSON converter from CSVlogger() to JSON, this is when the previous step w questions selection is finished via Flag button"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set path to both files\n",
    "csv_file = 'flagged/log.csv'\n",
    "json_file = 'flagged/questions_answers.json'\n",
    "\n",
    "# empty list to store the JSON data\n",
    "json_data = []\n",
    "\n",
    "# Open the CSV file for reading\n",
    "with open(csv_file, 'r') as csvfile:\n",
    "    # Create a CSV reader object\n",
    "    csvreader = csv.DictReader(csvfile)\n",
    "    \n",
    "    # Iterate through each row in the CSV file\n",
    "    for row in csvreader:\n",
    "        # Append each row as a dictionary to the JSON data list\n",
    "        json_data.append(row)\n",
    "\n",
    "# Open the JSON file for writing and save the JSON data\n",
    "with open(json_file, 'w') as jsonfile:\n",
    "    json.dump(json_data, jsonfile, indent=4)\n",
    "\n",
    "print(f\"CSV data has been converted to JSON successfully and saved as {json_file}. The file contains final result of Quiz generator. \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draft metrics\n",
    "\n",
    "import nltk\n",
    "\n",
    "# Reference questions (human-generated)\n",
    "reference_questions = [\n",
    "    \"What is the test objective?\",\n",
    "    \"How do objectives vary?\",\n",
    "    \"What does the context include?\",\n",
    "    # Add more reference questions here\n",
    "]\n",
    "\n",
    "# Automatically generated questions\n",
    "generated_questions = [\n",
    "    \"What is test objectives?\",\n",
    "    \"What is objectives vary?\",\n",
    "    \"How do objectives depend?\",\n",
    "    # Add more generated questions here\n",
    "]\n",
    "\n",
    "# Initialize the NLTK BLEU scorer\n",
    "bleu_scorer = nltk.translate.bleu_score.SmoothingFunction()\n",
    "\n",
    "# Calculate BLEU score (a measure of similarity)\n",
    "bleu_scores = [nltk.translate.bleu_score.sentence_bleu([r.split()], g.split(), smoothing_function=bleu_scorer.method1) for r, g in zip(reference_questions, generated_questions)]\n",
    "\n",
    "# Calculate accuracy rate (percentage of questions that match reference questions)\n",
    "accuracy_rate = sum(score == 1.0 for score in bleu_scores) / len(bleu_scores) * 100\n",
    "\n",
    "print(\"Accuracy Rate: {:.2f}%\".format(accuracy_rate))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from selenium import webdriver\n",
    "\n",
    "#driver=webdriver.Chrome()\n",
    "\n",
    "#driver.get(\"https://www.facebook.com/\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip show selenium\n",
    "#!pip install selenium==4.12.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract api key\n",
    "\n",
    "#from pyChatGPT import ChatGPT\n",
    "#from selenium.webdriver.common.by import By\n",
    "#from selenium.webdriver.support.ui import WebDriverWait\n",
    "#from selenium.webdriver.support import expected_conditions as EC\n",
    "#from selenium.webdriver.common.action_chains import ActionChains\n",
    "#from selenium.webdriver.support.ui import WebDriverWait##\n",
    "\n",
    "#session_token = \"eyJhbGciOiJkaXIiLCJlbmMiOiJBMjU2R0NNIn0..RSoIbYuWt0dZzv_n.yzUd3McMevS948NpJcruJnsILZCqj30VJnxYzcJRnQ38WO9xCyF1LazX-6kARlOnQccDdoakpHjCQ_1NsMO-8MMLm1RKVFdWG3QmH5CCfFAFwZlGiVo-Fj04fVnxFZCvw3j4ouaqA2XwELxW0m9Q_fhCqy8ZLaiF4YpJCmjubt4A9HAJZ0pbrNknQq-OL62DJXJuOL92t_pE-jZpgMIZA3jTZHdvZUzZqfcqM26KKikwJg_WD5wAmLAqz_whTt2p6mNei2Yt6reJQ_uP_5Cwr6Ae9uEF3rX-h0ylmz_di8Ntexgk5nlN2dU4gHEWoNUo0Nf8tqXQMHfoQn6LS3AnFIcDEAAA5s7QTmJZ4hkyCAUk1TOXRG3afrEJD1snnXvrJkv7skXMQfDYhneBE8lUnTQpTJzRxW3KUfXbx89vRXCcboP-LvhTZ_q3adKGiQT5ZhJ6Gb5pCrxFVS34Y7996VfseEbf7duaSW58UNp8mG95YyQtQM1JwaegR9TdE37L3-oNVpoJtS2CzbH6UyS4Ddk6z6IDT-3-5EMLOzO2Bz16CV87DsB---SCsnOll62LXaRHYrdc7Y43u7JMCWgQtLeO0Zm-8H25HkfYhm5YvqBW7pUTeOo43cWZPFDTIcDulunZh68S065c3GzvokIVoAkMieZBzwZQ-KAFjisxntaO69cYVJ_OVjX7aJvXHvsCZcX0jcHwlkooIqn_dbaXgl7718We_QGgLVz6hsDvUlsT3FWfKOP77PSWY20Vvd3Vn3LMzBkxIhLIENpPLHx6kHa_BIBlLdUDJaZKTGCLKvm7GGzgv7kAaQe4bIC-uXGjZDifyP__Wj5M3xxkwY7rAF-voYEvGGoArczz5sQuX2qWBLyQ7FpvgC2lZwarkz9ZWoLBWMgv2jf7Ypm7cBuXnLhzK1TV9tGMinrkWXqjNA9cFLqU3oQ-212hVorUOBj2gG8MO5Z6vIi1GGtrUGN4zvKyDikL1WrBLFZPOPnp2pNX63i2F-6PjqMI9u1X1ZKMqN-viz8JOuFmW6KMayCUWGrKtKXn0_wk8SkB8bzOmZ4w6WbZavlsS23v8NOJkwm2sZOoovKEsHNOQEbnZGGckGZwXNKHnkK4nLJrW-00MfBs3Lq597UbDpGKGqPHh8kOCjGnaACg27CCWckmglgnNwsU6NNv360jIKgFAuT5mJFFr1h8fUFWB0T7V3QbdXOGV9J4g0NP2cIGE-tXOiQquI84kpxjjOVOE_jPAVNEVF0hrBC6Eg4xMYas0mVQ2K-fmZrRtmNKi1UMq2iQALbrPm0UA0LToAqURFhAdUdyYoXj4hGVbvmKBGN8P-0fjEi4lYKBnPsBiWkejTnogTJc_5T2RePoHkopy6_9q3lCD2JcNHkkUlJbCYfmAC7NJNsM-3N-SJ0Glb7KmYx8sY3MqRJHvLPLeo7I5QxAgKeKurgRr3lxxNJbcliL28A5hauqEjUMJnWf6BqhpplcQXLQYlsLn4u9DLuMXXdhbNUp-CvGM0SrydLpy1Y819YaZI7ds88bNB5Hq4e35AqRBo_uDuU2MY5infiyv4-2DvoWcJVl6Dcmag_KwXaGvZewm7lPbG8dxDNPfWmxMBsqDRk62rbzLe3o-EphkTkyEfTreWOMrQSTkrEwdGMzG8sXiV8hHcfbAT_qL4vzNIrs0gxsKqt0O0ecGUG1wbW6PXhArlA0Z_ot-NK7g7aP3ntgdsdaIrqw6FCnKzQnXT35oVD0InGO4EbZnIcg4YyTepjWs1LrUjtTVagWPagr3c-kICpJfjw4jERhdsvQFS1V_QFG_SjT00XW52FvU5r5IeMLjz0r7Khphq5B8XveUAvl65X36mvS-npVHGdfrCBsCIxSXpKVT8o_acGzM9vFpVtdALy32dQap92Lqv-YtQfyMCkyfYXaH4nlEYpMfxNLzD_M98nD35PTr3ovA85JtYMP7fWzV7QiQzx5sqgtsXlbBQeMTA0httZZj8qZ7VtWKGv0BbWkPTQRlh-9JbPYJS0KB791Djdw-nfdRS1sSi7972WykK91W6iCrRff_BCWXzn83-YCk1_F4rn420cDLVlB3hDvIO-SiQmCuhH9xCnX9a0PKGPT82YvCYVuO3Ets0mYHyLd4GLpqvF0KHjqGnIOAzuzU7QfECz6JNCkm1yLa8QVY978gyi68pxJAVVVAVs546FwP2pLjv3RQkt5cfzGpkP4wMt-tXVJR7SCy-Sntqfpn5L4heisOjo6EHkpgP4OTyv-kUPa5HOxkIAVrmG7g2Qwvn-SjfkEG_jZHqFkl4unsECiSKN1fZMaTTIC9qOJ8YFGPsBijhR-toUd5EmtYQJ_rW8aosszMkQmPy1MRKv0Iwab6QYm1ncWyqLzfJMTFtYRRDnyclfqr1oEwS-TbBFRJZADN8I677a7CNiq_JIphlx5w8kB7ADtk--zAHmCVlycAza6EgArS4WvgRJ18QYNjN1-e-3v1ISrxSNjXRK8PAIGqInHGJ7EpkmXDmS5I4HlbDakdtPeAiLTLuaXvctbbSWg5vSgLLIBVwskpcLBhCpotHuhou_ZLTU6q40bfee8u0Gt99mTswb5BkS9DCJlpGNAr99GVSYpvhyPSbQfnScN4EnBK-aoySeoaqnTVdLCqTlhgJqLKlVyNf6-nASXvBiEQ0pjcdk3AxQ_j_7aCGj4Eo0hhSndlhZoXaU22y_DV6CY8CxDqJrbwU2NZDKvJZjZmVza6RH4FAVHgnlwsgKCb-I7Ccpw93EhrOmgpObGD3fEIhr0TKTxVeqxkjWK9wz672kndN__FQEnOA9Q664rCQcro349u0yOs50XtNZZ6nnuV3WfQZ1pkPxlmMyfgp0T.FCUbJELvyDNKws3nIQugCg\"\n",
    "##session_token = \"eyJhbGciOiJkaXIiLCJlbmMiOiJBMjU2R0NNIn0..Tp3P5x2i7af5GTfN.y6_A8C625dFSW8oSdmTLRQvvjYS8QmcPFKEDpLl-PHD1KngeOZFs78Jmpcrx_AgAxNcK4aWi_EHRiRGHUcUx4fnpFH0artO90rhECxVFoVJ4iouZ0477aTbDpfC19-y7G9xnCdlmb3CHTbz1l7ifsq3N9HwThAuybG_uYzAkYH74HKtG28REuxEea5EfTSS-1gracdfxuFtGWb1DvXJ_xpczUvbI_7vrHeuD-wWyrnH9a1NU484r1YntPLG9hhwHmbQ8YbdQyDv-Jl7LqHM1GGSFx8Q0ebW1Y3nxYyQnZ13zEYSosudoQQ4pd80WD3a2VXbdDpSWlFIjDt5V9pXISzc85KUha45JLAEr0EPK5eJaaWRdHIgEYrg6vRWbMaGQ4kz7YWDgd_VzOyHYQeU11UMeFuqdlafQWBo-dv2FOb-GbDKy85vtwLYvIoTgRH3e7ixNWVzbnjWtR_bSBpMEBjQHJG0E7KWHFkamPoav1SyUy-SiOVJKBPbRgxnFC25LWfiJGmXgRV1L7wIYorMxJ3lJKjXH7in8cEl64dn2l7FJ5EH9eeZg1nIeIsSO5lSATkG1ueLY8cuQeEJPL6LZzVuVJV7v-K2C5vrgft0yoU9-LEyXry9bU5WUZf1PKOSr8VS37MRZwMlWmbBrLkQaqHSAB2l3HS5T9kO2ZafyGc6d8luoVvIYxeIXlPrI6j6k3PaPPclh9ACi8pIay17TmVfSbBVGgTZp2VbP37Yfri5tG0dYkZEjXDUkd-NDdxVfe1ChQLwLeN6NUHu-IgTOUK3sqPNU9HYptdz1yD3GfpJY5qOKtGt4D-OoECahIeSzxNYlo5JY4rXviSeGeW2PLvq3pkD4MN5jrgzjCZ9getdAua8yEGvT5_A-uzN8w9gFw8kdhgK3BwOhdfU1cY_OUDoDTIZ12MJbkAH9BzcMXU6P9CoNshyQM8dE1-D_WNxySwVcTSwfOuaZ9MQIhLWa8geqvhVqNDjLzYfKjaydyr6wPwDsBLh9dTqQmmp88aD3VJ-Is6aqRjUIUlSOTsA8eNL5vy-cNZufRX4OrXB9p_Az0PfFNvjSptzoMGmH-0YXiyP4yENYtNSTagawc_PlB66IuUlOmCuyXmVXlzRKTEiAbwNkVrfbBB2ezw6g4cQ6ROROEuCXfBCKh5GjQ-IJgxQRjf6OOaczo-G_YYUnZhuZK2NgEPeXfS18QT_dAKdJLB4PySxitP-MKjOGoWOWbm858Uod1NwQQUMmO7JYAzqxrfNxxFkEA60XTxM-QkZnssgsdjFy0uGEXuwBaoYuYX6_WX9k71ixZ47nNfhRVfTPvXb_8GWVuf_QmwKLYg6ZEcGgGyDiIjhdypr2bretXyR5AfYj1Cludvmc5frm2IXGEJTQIHUkx5sW70ycXtOWNqHfE5sXlt9zyM12FywccPoxUZjZM4YdSleaL-smbApxa-AKjxzTWIgPJHpd-nxputQJdRYjAHLCqPVCycNse5q1y5oVJUCkaDgd4maMA0kBRJf3ftQG1UnG_W2Ao_sf-Qqrflzu80xiv30AjTmoHwGih8u4rB-iMS6Arj2Bbcl3EO24svdqsVCCS-smAasHsDP2pVPCw2HWAXmNNv5IDTI4TlNYhabCnm1eG50rmOXWy_2Qhp9sI1trOrxFqyEmkMyK_iypLFIL-DlrpJ8KZc5KPW6s_prVv7rGRcvEvf_AZVfoJ9UWLxg0KtUH3xoFAFhpqpW1NO3CdjzeBbEbs9xKU7zLFeQwbZU3oZCzjJ3HAKc1enlU48D64aRRw2ZaTnlLHw5tJDSpchBUJX5jyAo9WXVhvjLCe3Coj-OfwqsUyPkkCal33h7O8twSIIFe7THN0rPux-Fjxk1s11GfB9QvppgibwV7L7m6ODTelMofI17eeMGxJcBCePZzVfSVIp3SbTsQhkkU-UErI27TdO3j0_r4d4ceBKSF2kGxRFVCqoKQ2O6ifv_vMdiOlDlCFHgiSs933MuAF5OrWavrGp59gWamUfJXMsX_CqeP8B8Fu-gI3nWR-qX5Xyu7gpphdkn60Dsnw_I174OY9p0RbS01kGiRGCA5wzv5OQaJYcvIOzLr9O9Pa_nWK6jIx4u7REN6Z6-B4Q6dK7Pc1nWPuUQNS1rPkxVYHq2gUDE7kLch5JvCLUImRFceTpiw1QeScTi_p3GHQXIzvrjHv0PlZ5c7fs2VTwM9wUcFEX24C-lJhUnKnPeyI3gK15hQSHe0EClUzrdf_159KKY6VgE4ig3lwAQYVU4zD_ZJTrs3clRRVNufg-XEDFEJ_MxCme-tjskTxUUglTDRKwD4Ij8Ok31yQiAOcflsF-CG6AY3HyVObVLqp3jCS9kPRzQ4gJRuhd4Zhh3QpmzVk4Q0iCjom4jfzVEUWAhtl8399LG6b3kNnbBWX_ON_1aj58JlU-QYeGHuZ7olbrKVkphljS9-lUjTMp5DJmG3D_L4yIVP25VDVceM4K1TGW4m_FaGjuKsmhLHgP4IMBRYgvHAeLoHqfwXORBnQ1M0mGAHsFt1i7kTOBVwJx1v8slmuFGdYgIJtRi7qPjOPvqaBmFg7_PT-tLC1R_8D0zZNBCkZAHYq7JQM09HV6qNj7s_zShfxN2crakn6J8VPDashRiYAysOq3V-ZB1Efai3bkWl90ae1WaWZtCH7ZVc3Z7ie3anyMSxYfLzGlaRm_dOgA0_SUws90cYbhWkzHBunGmUMCbf50PwC-InQ6-6Y1LIPEkH591wITeiqgI7SYcx30c12kA06-iRQ7yPZzD-bmWiEXd1_KQgb85vMf3wR3E-haRiVBYHVF_CHc-JM28GuVFmKrDilWt7VyEy5XwpVaZw0p4l0ukVUgkv.V0DZCdRFRZKFkglRGB0WIg\"\n",
    "#api = ChatGPT(session_token)#\n",
    "\n",
    "##textbox_xpath = '//*[@id=\"radix-:rs:\"]/div[2]/div/div[4]/button'\n",
    "##script = 'alert(\"Hello, from JavaScript!\");'\n",
    "##script = f'document.evaluate(\\'{textbox_xpath}\\', document, null, XPathResult.FIRST_ORDERED_NODE_TYPE, null).singleNodeValue.click();'\n",
    "##api.driver.execute_script(script)\n",
    "##button = WebDriverWait(driver, 10).until(\n",
    "##    EC.element_to_be_clickable((By.XPATH, '//*[@id=\"radix-:rs:\"]/div[2]/div/div[4]/button'))\n",
    "##)#\n",
    "\n",
    "## Click the button\n",
    "##button.click()#\n",
    "#\n",
    "\n",
    "## Click the button\n",
    "##button.click()\n",
    "#resp = api.send_message('Write an essay on Generative AI')#\n",
    "\n",
    "## Locate and scroll to the \"Okay, let's go\" button\n",
    "#button = api.driver.find_element(By.XPATH, '//*[@id=\"radix-:rl:\"]/div[2]/div/div[4]/button')\n",
    "##api.driver.execute_script(\"arguments[0].scrollIntoView();\", button)#\n",
    "\n",
    "## Click the button\n",
    "#button.click()#\n",
    "#\n",
    "#\n",
    "\n",
    "#resp = api.send_message(' AI')\n",
    "##button1 = api.driver.find_element(By.XPATH, '//*[@id=\"__next\"]/div[1]/div[2]/div/main/div[1]/div[2]/form/div/div[2]/div/button')\n",
    "##button.click()\n",
    "#api.driver.quit()#\n",
    "\n",
    "## Close the browser when done\n",
    "##api.driver.quit()#\n",
    "#\n",
    "\n",
    "##print(resp['message'])\n",
    "##api.refresh_auth()  # refresh the authorization token\n",
    "##api.reset_conversation()  # reset the conversation#\n",
    "\n",
    "##button_xpath = '//*[@id=\"radix-:rs:\"]/div[2]/div/div[4]/button/div'\n",
    "##button_element = driver.find_element(By.XPATH, button_xpath)\n",
    "##button_element.click()#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qgmc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
