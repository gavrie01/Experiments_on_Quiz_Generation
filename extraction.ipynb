{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we are downloading PDF file, converting it to TXT and doing some \"pre-cleaning\": removing not meaningful parts of document and leaving just the most valuable leftovers for our future generator.\n",
    "THe outcome of the below code is pre-processed but still raw data.\n",
    "\n",
    "\n",
    "\"extracted_text\" variable has \"StringIO\" type: The StringIO object is part of Python's io module and is a class that provides an in-memory file-like object that can be used for reading from or writing to strings as if they were files. It allows you to treat strings as file-like objects, which can be useful in various situations, such as when you want to read from or write to a string in a way that mimics file operations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import of libraries\n",
    "from io import StringIO # extracted_text is the main variable, contains the whole text of document in stringIO format in memory\n",
    "import requests\n",
    "import re  # provides reg. exp. support\n",
    "import math\n",
    "\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfdocument import PDFDocument\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.pdfparser import PDFParser\n",
    "import fitz\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "import sentencepiece\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer, BertForQuestionAnswering, BertTokenizer\n",
    "from transformers import AutoTokenizer\n",
    "from keybert import KeyBERT\n",
    "import gradio as gr # UI part for the quize\n",
    "import urllib3\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U pip setuptools wheel\n",
    "#!pip install -U spacy\n",
    "#!python -m spacy download en_core_web_sm\n",
    "#!pip install PyMuPDF # this is fitz\n",
    "#!pip install gradio\n",
    "#!pip install keybert\n",
    "#!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1113747"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# downloading pdf to '/data/' folder\n",
    "url = 'https://astqb.org/assets/documents/ISTQB_CTFL_Syllabus-v4.0.pdf'\n",
    "r = requests.get(url, allow_redirects=True)\n",
    "open('data/ISTQB_CTFL_Syllabus-v4.0.pdf', 'wb').write(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "r\"Page \\d{4,74} of 74\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted text for 'The Certified Tester Foundation Level in Software Testing' saved to 'data/ISTQB_CTFL_Syllabus-v4.0.txt'\n"
     ]
    }
   ],
   "source": [
    "#converting pdf to text and saving into .txt file initial version\n",
    "output_string = StringIO()\n",
    "output_file_path = 'data/ISTQB_CTFL_Syllabus-v4.0.txt'\n",
    "with open('data/ISTQB_CTFL_Syllabus-v4.0.pdf', 'rb') as in_file, open(output_file_path, 'w', encoding='utf-8') as out_file:\n",
    "    parser = PDFParser(in_file)\n",
    "    doc = PDFDocument(parser)\n",
    "    rsrcmgr = PDFResourceManager()\n",
    "    device = TextConverter(rsrcmgr, output_string, laparams=LAParams())\n",
    "    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "    for page in PDFPage.create_pages(doc):\n",
    "        interpreter.process_page(page)\n",
    "    # Getting the extracted text from StringIO, it means the entire text extracted from the PDF is stored as a single string in memory.\n",
    "    extracted_text = output_string.getvalue()\n",
    "    # Writing the extracted text to the output file\n",
    "    out_file.write(extracted_text)\n",
    "\n",
    "# Closing the stream\n",
    "output_string.close()\n",
    "\n",
    "\n",
    "# Printing message to indicate that the text has been saved to the file\n",
    "print(f\"Extracted text for 'The Certified Tester Foundation Level in Software Testing' saved to '{output_file_path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of string in bytes : 198489\n",
      "File size, document contains 70+ pages:  193.84 KB\n"
     ]
    }
   ],
   "source": [
    "# let us check size of StringIO on the full size of converted file, just out of curiosity\n",
    "size_bytes = len(extracted_text.encode('utf-8'))\n",
    "print ('The length of string in bytes : ' + str (size_bytes))\n",
    "\n",
    "# function's code is taken from stackoverflow ---\n",
    "def convert_size(size_bytes):\n",
    "   if size_bytes == 0:\n",
    "       return \"0B\"\n",
    "   size_name = (\"B\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\", \"EB\", \"ZB\", \"YB\")\n",
    "   i = int(math.floor(math.log(size_bytes, 1024)))\n",
    "   p = math.pow(1024, i)\n",
    "   s = round(size_bytes / p, 2)\n",
    "   return \"%s %s\" % (s, size_name[i])\n",
    "# ---\n",
    "print(\"File size, document contains 70+ pages: \", convert_size(size_bytes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted text for 'The Certified Tester Foundation Level in Software Testing' pre processed version 0.1 saved to 'data/ISTQB_CTFL_Syllabus-v4.0_v01.txt'\n"
     ]
    }
   ],
   "source": [
    "# Looking up for the text to remove everything before it\n",
    "target_text = \"1.1. What is Testing?\"\n",
    "\n",
    "# Finding the position of the target text in the extracted text\n",
    "start_position = extracted_text.find(target_text)\n",
    "\n",
    "# Checking if the target text was found, just in case\n",
    "if start_position != -1:\n",
    "    # Removing everything before the target text\n",
    "    extracted_text = extracted_text[start_position:]\n",
    "\n",
    "\n",
    "# let us save the content to .txt file with prefix '_v0.1' for further debugging purpose and human evaluation process\n",
    "\n",
    "output_file_path = 'data/ISTQB_CTFL_Syllabus-v4.0_v01.txt'\n",
    "with open(output_file_path, 'w', encoding='utf-8') as out_file:\n",
    "    # Writing the extracted text to the output file\n",
    "    out_file.write(extracted_text)\n",
    "\n",
    "# Closing the stream\n",
    "output_string.close()\n",
    "\n",
    "# Printing message to indicate that the text has been saved to the file\n",
    "print(f\"Extracted text for 'The Certified Tester Foundation Level in Software Testing' pre processed version 0.1 saved to '{output_file_path}'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing empty lines\n",
    "# _ - is iterator, if s.strip(): This part of the list comprehension checks whether the line s contains any non-whitespace characters. \n",
    "# If it does, the line is included in the resulting list.\n",
    "\n",
    "# extracted_text = \"\".join([_ for _ in extracted_text.strip().splitlines(True) if _.strip()])\n",
    "# \n",
    "# output_file_path = 'data/ISTQB_CTFL_Syllabus-v4.0_v02.txt'\n",
    "# with open('data/ISTQB_CTFL_Syllabus-v4.0_v01.txt', 'rb') as in_file, open(output_file_path, 'w', encoding='utf-8') as out_file:\n",
    " #   Writing the extracted text to the output file\n",
    "    # out_file.write(extracted_text)\n",
    "# \n",
    "#Closing the stream\n",
    "# output_string.close()\n",
    "# \n",
    "#Printing message to indicate that the text has been saved to the file\n",
    "# print(f\"Extracted text for 'The Certified Tester Foundation Level in Software Testing' pre processed version 0.2 saved to '{output_file_path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted text for 'The Certified Tester Foundation Level in Software Testing' pre processed version 0.1 saved to 'data/ISTQB_CTFL_Syllabus-v4.0_v03.txt'\n"
     ]
    }
   ],
   "source": [
    "# Removing text from 'Page 56 of 74' till the end of the text\n",
    "\n",
    "# Looking up for the text to remove everything after it\n",
    "target_text = \"Page 56 of 74\"\n",
    "\n",
    "# Finding the position of the target text in the extracted text\n",
    "end_position = extracted_text.find(target_text)\n",
    "\n",
    "# Checking if the target text was found, just in case\n",
    "if end_position != -1:\n",
    "    # Removing everything before the target text\n",
    "    extracted_text = extracted_text[:end_position]\n",
    "\n",
    "\n",
    "# let us save the content to .txt file with prefix '_v0.1' for further debugging purpose and human evaluation process\n",
    "\n",
    "output_file_path = 'data/ISTQB_CTFL_Syllabus-v4.0_v03.txt'\n",
    "with open(output_file_path, 'w', encoding='utf-8') as out_file:\n",
    "    # Writing the extracted text to the output file\n",
    "    out_file.write(extracted_text)\n",
    "\n",
    "# Closing the stream\n",
    "output_string.close()\n",
    "\n",
    "# Printing message to indicate that the text has been saved to the file\n",
    "print(f\"Extracted text for 'The Certified Tester Foundation Level in Software Testing' pre processed version 0.1 saved to '{output_file_path}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your stop words list\n",
    "# stop_words = [\"v4.0\", \"Page\", \"74\", \"18\", \"15\", \"of\", \"2023-04-21\", \"©\", \"Certified Tester\", \"Foundation\", \"Level\", \"International Software Testing Qualifications Board\"]\n",
    "# \n",
    "#Split the extracted_text into words\n",
    "# words = extracted_text.split()\n",
    "# \n",
    "#Filter out words that are in the stop words list\n",
    "# filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "# \n",
    "#Join the filtered words back into a text\n",
    "# extracted_text = \" \".join(filtered_words)\n",
    "# \n",
    "#Print the cleaned text\n",
    "#print(extracted_text)\n",
    "# \n",
    "# \n",
    "# output_file_path = 'data/ISTQB_CTFL_Syllabus-v4.0_v05.txt'\n",
    "# with open(output_file_path, 'w', encoding='utf-8') as out_file:\n",
    "#    Writing the extracted text to the output file\n",
    "    # out_file.write(extracted_text)\n",
    "# \n",
    "#Closing the stream\n",
    "# output_string.close()\n",
    "# \n",
    "#Printing message to indicate that the text has been saved to the file\n",
    "# print(f\"Extracted text for 'The Certified Tester Foundation Level in Software Testing' pre processed version 0.5 saved to '{output_file_path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to lower case all words in stringIO\n",
    "#extracted_text = extracted_text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#punctuation\n",
    "# Load the language model\n",
    "#nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Process the text with SpaCy\n",
    "###doc = nlp(extracted_text)\n",
    "\n",
    "# Create a list of tokens that are not punctuation\n",
    "#filtered_tokens = [token.text for token in doc if not token.is_punct]\n",
    "\n",
    "# Join the filtered tokens back into a text\n",
    "#extracted_text = \" \".join(filtered_tokens)\n",
    "\n",
    "# Print the text without punctuation\n",
    "#print(extracted_text)\n",
    "\n",
    "#output_file_path = 'data/ISTQB_CTFL_Syllabus-v4.0_v04.txt'\n",
    "#with open(output_file_path, 'w', encoding='utf-8') as out_file:\n",
    "    # Writing the extracted text to the output file\n",
    "#    out_file.write(extracted_text)\n",
    "\n",
    "# Closing the stream\n",
    "#output_string.close()\n",
    "\n",
    "# Printing message to indicate that the text has been saved to the file\n",
    "#print(f\"Extracted text for 'The Certified Tester Foundation Level in Software Testing' pre processed version 0.1 saved to '{output_file_path}'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check results, looks like some parts are not removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted text for 'The Certified Tester Foundation Level in Software Testing' pre processed version 0.5 saved to 'data/ISTQB_CTFL_Syllabus-v4.0_v05.txt'\n"
     ]
    }
   ],
   "source": [
    "# built by chatgpt on provided context from my side, I used a part of text of file above, reviewed and customized by me as well\n",
    "#stop_words = [\"buxton\", \"a\", \"about\", \"above\", \"additional\", \"an\", \"and\", \"another\", \"are\", \"as\", \"be\", \"being\", \"by\", \"can\", \"common\", \"commonly\", \"do\", \"does\", \"each\", \"even\", \"for\", \"from\", \"has\", \"have\", \"in\", \"including\", \"is\", \"it\", \"its\", \"it's\", \"many\", \"may\", \"more\", \"most\", \"not\", \"of\", \"74\", \"often\", \"on\", \"or\", \"over\", \"such\", \"than\", \"that\", \"the\", \"there\", \"these\", \"this\", \"to\", \"under\", \"was\", \"we\", \"what\", \"when\", \"which\", \"who\", \"why\", \"will\", \"with\", \"within\", \"work\", \"you\", \"2023\", \"04\", \"21\", \"v4.0\", \"page\", \"2023-04-21\", \"©\", \"international\", \"qualifications\", \"board\", \"certified\", \"tester\",  \"foundation\", \"level\", \"FL-\", \"K2\", \"see\", \"section\" , \"didn't\", \"doesn't\", \"don't\", \"i.e.\", \"it's\", \"let's\", \"that's\", \"there's\", \"they're\", \"you're\", \"e.g.\"]\n",
    "stop_words = [ \"©\", \"15\", \"16\", \"17\", \"18\", \"19\", \"20\", r\"\\b20\\b\", \"21\", \"22\", \"23\", \"24\", \"25\", \"26\", \"27\", \"28\", \"29\", \"30\", \"31\", \"32\", \"33\", \"34\", \"35\", \"36\",\n",
    "              \"37\", \"38\",\"39\", \"40\",\"41\", \"42\",\"43\", \"44\",\"45\", \"46\", \"47\", \"48\", \"49\", \"50\", \"51\", \"52\", \"53\", \"54\", \n",
    "              \"International Software Testing Qualifications Board Certified Tester Foundation Level\", \"21.04.2023\", \"01.07.2021\",\n",
    "              \"11.11.2019\", \"27.04.2018\", \"1.04.2011\", \"30.03.2010\", \"01.05.2007\", \"01.07.2005\", \"25.02.1999\", \"the\", \"market\"]\n",
    "# \n",
    "# Your stop words list\n",
    "#stop_words = [\"v4.0\", \"page\", \"of\", \"2023-04-21\", \"©\", \"International Software Testing Qualifications Board\",\n",
    " #             \"Certified\", \"Tester\", \"Foundation\", \"Level\"]\n",
    "# \n",
    "# Regular expression pattern to match phrases like \"15 74\", \"16 74\", ..., \"54 74\"\n",
    "pattern = re.compile(r\"(?s)^v4.0.*Foundation Level$\", re.DOTALL)\n",
    "# \n",
    "# Split the extracted_text into words\n",
    "words = re.split(r'\\s+', extracted_text)\n",
    "# \n",
    "# Filter out words that match the regular expression pattern or are in the stop words list\n",
    "filtered_words = [word for word in words if not re.match(pattern, word) and word.lower() not in stop_words]\n",
    "\n",
    "# \n",
    "# Join the filtered words back into a text\n",
    "extracted_text = \" \".join(filtered_words)\n",
    "\n",
    "# Define the phrase you want to remove\n",
    "phrase_to_remove = \"International Software Testing Qualifications Board Certified Tester Foundation Level\"\n",
    "\n",
    "# Replace the phrase with an empty string and comas removal (across the whole text)\n",
    "extracted_text = extracted_text.replace(phrase_to_remove, \"\")\n",
    "extracted_text = extracted_text.replace(\",\", \"\")\n",
    "\n",
    "# \n",
    "output_file_path = 'data/ISTQB_CTFL_Syllabus-v4.0_v05.txt' \n",
    "with open(output_file_path, 'w', encoding='utf-8') as out_file:\n",
    "    # Writing the extracted text to the output file\n",
    "    out_file.write(extracted_text)\n",
    "# \n",
    "# Closing the stream\n",
    "output_string.close()\n",
    "# \n",
    "# Printing message to indicate that the text has been saved to the file\n",
    "print(f\"Extracted text for 'The Certified Tester Foundation Level in Software Testing' pre processed version 0.5 saved to '{output_file_path}'\")\n",
    "# \n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pattern = r'[0-9]'\n",
    "\n",
    "# Match all digits in the string and replace them with an empty string\n",
    "#extracted_text = re.sub(pattern, '', extracted_text)\n",
    "\n",
    "#extracted_text = ''.join((x for x in extracted_text if not x.isdigit()))\n",
    "\n",
    "\n",
    "#output_file_path = 'data/ISTQB_CTFL_Syllabus-v4.0_v06.txt'\n",
    "#with open(output_file_path, 'w', encoding='utf-8') as out_file:\n",
    "    # Writing the extracted text to the output file\n",
    "#    out_file.write(extracted_text)\n",
    "# \n",
    "# Closing the stream\n",
    "#output_string.close()\n",
    "# \n",
    "# Printing message to indicate that the text has been saved to the file\n",
    "#print(f\"Extracted text for 'The Certified Tester Foundation Level in Software Testing' pre processed version 0.6 saved to '{output_file_path}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# # Load the language model\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# # Your text\n",
    "# text = extracted_text\n",
    "\n",
    "# # Process the text with SpaCy\n",
    "# doc = nlp(text)\n",
    "\n",
    "# # Create a StringIO object to store the NER results\n",
    "# output_string = io.StringIO()\n",
    "\n",
    "# # Extract named entities and write them to the StringIO object\n",
    "# for ent in doc.ents:\n",
    "#     output_string.write(f\"Entity: {ent.text}, Type: {ent.label_}\\n\")\n",
    "\n",
    "# # Get the NER results as a string\n",
    "# ner_results = output_string.getvalue()\n",
    "\n",
    "# output_file_path = 'data/NER.txt'\n",
    "# with open(output_file_path, 'w', encoding='utf-8') as out_file:\n",
    "#     # Writing the extracted text to the output file\n",
    "#       out_file.write(ner_results)\n",
    "\n",
    "# # Closing the stream\n",
    "# output_string.close()\n",
    "\n",
    "# # Printing message to indicate that the text has been saved to the file\n",
    "# print(f\"Extracted NER list for 'The Certified Tester Foundation Level in Software Testing; {output_file_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point NER dict is saved into /data folder, edited manually and now let us import this file into stop_list StringIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Check the content of stop_list_stringio\n",
    "#content = stop_list_stringio.getvalue()\n",
    "#print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Markov Chain\n",
    "# Sample text (replace with your extracted_text)\n",
    "# Tokenize the text into words\n",
    "#tokens = nltk.word_tokenize(extracted_text)\n",
    "\n",
    "# Create a dictionary to store transition probabilities\n",
    "#transition_probabilities = {}\n",
    "\n",
    "# Build the transition probability matrix\n",
    "#for i in range(len(tokens) - 1):\n",
    "#    current_token = tokens[i]\n",
    "#    next_token = tokens[i + 1]\n",
    "    \n",
    "#    if current_token in transition_probabilities:\n",
    "#        transition_probabilities[current_token].append(next_token)\n",
    "#    else:\n",
    "#        transition_probabilities[current_token] = [next_token]\n",
    "\n",
    "# Start with an initial word\n",
    "#current_word = random.choice(tokens)\n",
    "\n",
    "# Generate a sentence of a certain length\n",
    "#generated_text = [current_word]\n",
    "#sentence_length = 10\n",
    "\n",
    "#for _ in range(sentence_length - 1):\n",
    "#    if current_word in transition_probabilities:\n",
    "#        next_word = random.choice(transition_probabilities[current_word])\n",
    "#        generated_text.append(next_word)\n",
    "#        current_word = next_word\n",
    "#    else:\n",
    "#        break\n",
    "\n",
    "# Join the generated words into a sentence\n",
    "#generated_sentence = \" \".join(generated_text)\n",
    "#print(generated_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted text for 'The Certified Tester Foundation Level in Software Testing' pre processed version 0.7 saved to 'data/ISTQB_CTFL_Syllabus-v4.0_v07.txt'\n"
     ]
    }
   ],
   "source": [
    "# remove chapter 4 beginning\n",
    "\n",
    "# Define the regular expression pattern for the text to remove\n",
    "pattern = r'4\\. Test Analysis and Design – 390 minutes.*?(K3) Use acceptance test-driven development (ATDD) to derive test cases'\n",
    "\n",
    "# Use re.sub to replace the matched text with a marker (e.g., 'REMOVED')\n",
    "extracted_text = re.sub(pattern, \"\", extracted_text, flags=re.DOTALL)\n",
    "\n",
    "# Define the phrase you want to remove\n",
    "phrase_to_remove = \"Learning Objectives for Chapter 4:\"\n",
    "\n",
    "# Replace the phrase with an empty string\n",
    "extracted_text = extracted_text.replace(phrase_to_remove, \"\")\n",
    "# \n",
    "output_file_path = 'data/ISTQB_CTFL_Syllabus-v4.0_v07.txt'\n",
    "with open(output_file_path, 'w', encoding='utf-8') as out_file:\n",
    "    # Writing the extracted text to the output file\n",
    "    out_file.write(extracted_text)\n",
    "# \n",
    "# Closing the stream\n",
    "output_string.close()\n",
    "# \n",
    "# Printing message to indicate that the text has been saved to the file\n",
    "print(f\"Extracted text for 'The Certified Tester Foundation Level in Software Testing' pre processed version 0.7 saved to '{output_file_path}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted text for 'The Certified Tester Foundation Level in Software Testing' pre processed version 0.8 saved to 'data/ISTQB_CTFL_Syllabus-v4.0_v08.txt'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# remove chapter 4 beginning\n",
    "# Define the regular expression pattern to remove the desired text\n",
    "pattern = r'4\\.1 Test Techniques Overview.*?4\\.5\\.3 \\(K3\\) Use acceptance test-driven development \\(ATDD\\) to derive test cases'\n",
    "\n",
    "# Use re.sub to replace the matched text with an empty string\n",
    "extracted_text = re.sub(pattern, '', extracted_text, flags=re.DOTALL)\n",
    "\n",
    "\n",
    "\n",
    "output_file_path = 'data/ISTQB_CTFL_Syllabus-v4.0_v08.txt'\n",
    "with open(output_file_path, 'w', encoding='utf-8') as out_file:\n",
    "    # Writing the extracted text to the output file\n",
    "    out_file.write(extracted_text)\n",
    "# \n",
    "# Closing the stream\n",
    "output_string.close()\n",
    "# \n",
    "# Printing message to indicate that the text has been saved to the file\n",
    "print(f\"Extracted text for 'The Certified Tester Foundation Level in Software Testing' pre processed version 0.8 saved to '{output_file_path}'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted text for 'The Certified Tester Foundation Level in Software Testing' pre processed version 0.9 saved to 'data/ISTQB_CTFL_Syllabus-v4.0_v09.txt'\n"
     ]
    }
   ],
   "source": [
    "# remove chapter 3 beginning\n",
    "# Define the regular expression pattern for the text to remove\n",
    "pattern = r'3\\. Static Testing – 80 minutes.*?FL-3\\.2\\.5 \\(K1\\) Recall the factors that contribute to a successful review'\n",
    "\n",
    "# Use re.sub to replace the matched text with a marker (e.g., 'REMOVED')\n",
    "extracted_text = re.sub(pattern, \"\", extracted_text, flags=re.DOTALL)\n",
    "\n",
    "# Define the phrase you want to remove\n",
    "phrase_to_remove = \"Learning Objectives for Chapter 4:\"\n",
    "\n",
    "# Replace the phrase with an empty string\n",
    "extracted_text = extracted_text.replace(phrase_to_remove, \"\")\n",
    "# \n",
    "output_file_path = 'data/ISTQB_CTFL_Syllabus-v4.0_v09.txt'\n",
    "with open(output_file_path, 'w', encoding='utf-8') as out_file:\n",
    "    # Writing the extracted text to the output file\n",
    "    out_file.write(extracted_text)\n",
    "# \n",
    "# Closing the stream\n",
    "output_string.close()\n",
    "# \n",
    "# Printing message to indicate that the text has been saved to the file\n",
    "print(f\"Extracted text for 'The Certified Tester Foundation Level in Software Testing' pre processed version 0.9 saved to '{output_file_path}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted text for 'The Certified Tester Foundation Level in Software Testing' pre processed version 0.10 saved to 'data/ISTQB_CTFL_Syllabus-v4.0_v10.txt'\n"
     ]
    }
   ],
   "source": [
    "# remove chapter 2 beginning\n",
    "\n",
    "# Define the regular expression pattern for the text to remove\n",
    "pattern = r'2\\. Testing Throughout the Software Development Lifecycle.*?FL-2\\.3\\.1 \\(K2\\) Summarize maintenance testing and its triggers'\n",
    "# Use re.sub to replace the matched text with a marker (e.g., 'REMOVED')\n",
    "extracted_text = re.sub(pattern, \"\", extracted_text, flags=re.DOTALL)\n",
    "\n",
    "# Define the phrase you want to remove\n",
    "phrase_to_remove = \"Learning Objectives for Chapter 4:\"\n",
    "\n",
    "# Replace the phrase with an empty string\n",
    "extracted_text = extracted_text.replace(phrase_to_remove, \"\")\n",
    "# \n",
    "output_file_path = 'data/ISTQB_CTFL_Syllabus-v4.0_v10.txt'\n",
    "with open(output_file_path, 'w', encoding='utf-8') as out_file:\n",
    "    # Writing the extracted text to the output file\n",
    "    out_file.write(extracted_text)\n",
    "# \n",
    "# Closing the stream\n",
    "output_string.close()\n",
    "# \n",
    "# Printing message to indicate that the text has been saved to the file\n",
    "print(f\"Extracted text for 'The Certified Tester Foundation Level in Software Testing' pre processed version 0.10 saved to '{output_file_path}'\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted text for 'The Certified Tester Foundation Level in Software Testing' pre processed version 0.11 saved to 'data/ISTQB_CTFL_Syllabus-v4.0_v11.txt'\n"
     ]
    }
   ],
   "source": [
    "# remove chapter 2 beginning\n",
    "\n",
    "# Define the regular expression pattern for the text to remove\n",
    "\n",
    "pattern = r'5\\. Managing the Test Activities – 335 minutes.*?FL-5\\.5\\.1 \\(K3\\) Prepare a defect report'\n",
    "# Use re.sub to replace the matched text with a marker (e.g., 'REMOVED')\n",
    "extracted_text = re.sub(pattern, \"\", extracted_text, flags=re.DOTALL)\n",
    "\n",
    "# Define the phrase you want to remove\n",
    "phrase_to_remove = \"Learning Objectives for Chapter 4:\"\n",
    "\n",
    "# Replace the phrase with an empty string\n",
    "extracted_text = extracted_text.replace(phrase_to_remove, \"\")\n",
    "# \n",
    "output_file_path = 'data/ISTQB_CTFL_Syllabus-v4.0_v11.txt'\n",
    "with open(output_file_path, 'w', encoding='utf-8') as out_file:\n",
    "    # Writing the extracted text to the output file\n",
    "    out_file.write(extracted_text)\n",
    "# \n",
    "# Closing the stream\n",
    "output_string.close()\n",
    "# \n",
    "# Printing message to indicate that the text has been saved to the file\n",
    "print(f\"Extracted text for 'The Certified Tester Foundation Level in Software Testing' pre processed version 0.11 saved to '{output_file_path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "pattern = r'\\d+\\.\\d+\\.\\d+\\.'\n",
    "matches = re.findall(pattern, extracted_text)\n",
    "\n",
    "for match in matches:\n",
    "    match_without_dot = match[:-1]  # Remove the last dot\n",
    "    print(match_without_dot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted text for 'The Certified Tester Foundation Level in Software Testing' pre processed version 0.12 saved to 'data/ISTQB_CTFL_Syllabus-v4.0_v12.txt'\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Summary text with bullet points\n",
    "#summary = \"The typical test objectives are: • Evaluating work products such as requirements, user stories, designs, and code • Triggering failures and finding defects • Ensuring required coverage of a test object • Verifying that a test object complies with contractual, legal, and regulatory requirements • Providing information to stakeholders to allow them to make informed decisions • Building confidence in the quality of the test object • Validating whether the test object is complete and works as expected by the stakeholders\"\n",
    "\n",
    "# Remove bullet points using regular expressions\n",
    "extracted_text = re.sub(r'•', '', extracted_text)\n",
    "\n",
    "output_file_path = 'data/ISTQB_CTFL_Syllabus-v4.0_v12.txt'\n",
    "with open(output_file_path, 'w', encoding='utf-8') as out_file:\n",
    "    # Writing the extracted text to the output file\n",
    "    out_file.write(extracted_text)\n",
    "# \n",
    "# Closing the stream\n",
    "output_string.close()\n",
    "# \n",
    "# Printing message to indicate that the text has been saved to the file\n",
    "print(f\"Extracted text for 'The Certified Tester Foundation Level in Software Testing' pre processed version 0.12 saved to '{output_file_path}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Section Title: 1.1\n",
      "Section Content: What is Testing? Software systems are an integral part of our daily life. Most people have had experience with software that did not work as expected. Software that does not work correctly can lead to many problems including loss of money time or business reputation and in extreme cases even injury or death. Software testing assesses software quality and helps reducing risk of software failure in operation. Software testing is a set of activities to discover defects and evaluate quality of software artifacts. These artifacts when being tested are known as test objects. A common misconception about testing is that it only consists of executing tests (i.e. running software and checking test results). However software testing also includes other activities and must be aligned with software development lifecycle (see chapter 2). Another common misconception about testing is that testing focuses entirely on verifying test object. Whilst testing involves verification i.e. checking whether system meets specified requirements it also involves validation which means checking whether system meets users’ and other stakeholders’ needs in its operational environment. Testing may be dynamic or static. Dynamic testing involves execution of software while static testing does not. Static testing includes reviews (see chapter 3) and static analysis. Dynamic testing uses different types of test techniques and test approaches to derive test cases (see chapter 4). Testing is not only a technical activity. It also needs to be properly planned managed estimated monitored and controlled (see chapter 5). Testers use tools (see chapter 6) but it is important to remember that testing is largely an intellectual activity requiring testers to have specialized knowledge use analytical skills and apply critical thinking and systems thinking (Myers 2011 Roman 2018). ISO/IEC/IEEE 29119-1 standard provides further information about software testing concepts.\n",
      "----------------------------------------\n",
      "Section Title: 1.1.2\n",
      "Section Content: Testing and Debugging Testing and debugging are separate activities. Testing can trigger failures that are caused by defects in software (dynamic testing) or can directly find defects in test object (static testing). When dynamic testing (see chapter 4) triggers a failure debugging is concerned with finding causes of this failure (defects) analyzing these causes and eliminating them. typical debugging process in this case involves:  Reproduction of a failure  Diagnosis (finding root cause)  Fixing cause Subsequent confirmation testing checks whether fixes resolved problem. Preferably confirmation testing is done by same person who performed initial test. Subsequent regression testing can also be performed to check whether fixes are causing failures in other parts of test object (see section\n",
      "----------------------------------------\n",
      "Section Title: 1.2\n",
      "Section Content: Why is Testing Necessary? Testing as a form of quality control helps in achieving agreed upon goals within set scope time quality and budget constraints. Testing’s contribution to success should not be restricted to test team activities. Any stakeholder can use their testing skills to bring project closer to success. Testing components systems and associated documentation helps to identify defects in software\n",
      "----------------------------------------\n",
      "Section Title: 1.2.2\n",
      "Section Content: Testing and Quality Assurance (QA) While people often use terms “testing” and “quality assurance” (QA) interchangeably testing and QA are not same. Testing is a form of quality control (QC). v4.0 Page of 74 2023-04-21  QC is a product-oriented corrective approach that focuses on those activities supporting achievement of appropriate levels of quality. Testing is a major form of quality control while others include formal methods (model checking and proof of correctness) simulation and prototyping. QA is a process-oriented preventive approach that focuses on implementation and improvement of processes. It works on basis that if a good process is followed correctly then it will generate a good product. QA applies to both development and testing processes and is responsibility of everyone on a project. Test results are used by QA and QC. In QC they are used to fix defects while in QA they provide feedback on how well development and test processes are performing.\n",
      "----------------------------------------\n",
      "Section Title: 1.3\n",
      "Section Content: Testing Principles A number of testing principles offering general guidelines applicable to all testing have been suggested over years. This syllabus describes seven such principles. 1. Testing shows presence not absence of defects. Testing can show that defects are present in test object but cannot prove that there are no defects (Buxton 1970). Testing reduces probability of defects remaining undiscovered in test object but even if no defects are found testing cannot prove test object correctness. 2. Exhaustive testing is impossible. Testing everything is not feasible except in trivial cases (Manna 1978). Rather than attempting to test exhaustively test techniques (see chapter 4) test case prioritization (see section\n",
      "----------------------------------------\n",
      "Section Title: 2.2\n",
      "Section Content: 3). 6. Testing is context dependent. There is no single universally applicable approach to testing. Testing is done differently in different contexts (Kaner 2011). 7. Absence-of-defects fallacy. It is a fallacy (i.e. a misconception) to expect that software verification will ensure success of a system. Thoroughly testing all specified requirements and fixing all defects found could still produce a system that does not fulfill users’ needs and expectations that does not help in achieving customer’s business goals and that is inferior compared to other competing systems. In addition to verification validation should also be carried out (Boehm 1981).\n",
      "----------------------------------------\n",
      "Section Title: 1.4.1\n",
      "Section Content: Test Activities and Tasks A test process usually consists of main groups of activities described below. Although many of these activities may appear to follow a logical sequence they are often implemented iteratively or in parallel. These testing activities usually need to be tailored to system and project. Test planning consists of defining test objectives and then selecting an approach that best achieves objectives within constraints imposed by overall context. Test planning is further explained in section\n",
      "----------------------------------------\n",
      "Section Title: 5.3\n",
      "Section Content: Test analysis includes analyzing test basis to identify testable features and to define and prioritize associated test conditions together with related risks and risk levels (see section 5.2). test basis and test objects are also evaluated to identify defects they may contain and to assess their testability. Test analysis is often supported by use of test techniques (see chapter 4). Test analysis answers question “what to test?” in terms of measurable coverage criteria. Test design includes elaborating test conditions into test cases and other testware (e.g. test charters). This activity often involves identification of coverage items which serve as a guide to specify test case inputs. Test techniques (see chapter 4) can be used to support this activity. Test design v4.0 Page of 74 2023-04-21  also includes defining test data requirements designing test environment and identifying any other required infrastructure and tools. Test design answers question “how to test?”. Test implementation includes creating or acquiring testware necessary for test execution (e.g. test data). Test cases can be organized into test procedures and are often assembled into test suites. Manual and automated test scripts are created. Test procedures are prioritized and arranged within a test execution schedule for efficient test execution (see section\n",
      "----------------------------------------\n",
      "Section Title: 2.1\n",
      "Section Content: 6). A test completion report is created and communicated to stakeholders.\n",
      "----------------------------------------\n",
      "Section Title: 1.4.3\n",
      "Section Content: Testware Testware is created as output work products from test activities described in section\n",
      "----------------------------------------\n",
      "Section Title: 5.3\n",
      "Section Content: 2) documentation of control directives (see section 5.3) and risk information (see section 5.2).  Test analysis work products include: (prioritized) test conditions (e.g. acceptance criteria see section\n",
      "----------------------------------------\n",
      "Section Title: 5.3\n",
      "Section Content: 2) action items for improvement of subsequent projects or iterations documented lessons learned and change requests (e.g. as product backlog items).\n",
      "----------------------------------------\n",
      "Section Title: 1.1\n",
      "Section Content: 1). For example:  Traceability of test cases to requirements can verify that requirements are covered by test cases.  Traceability of test results to risks can be used to evaluate level of residual risk in a test object. In addition to evaluating coverage good traceability makes it possible to determine impact of changes facilitates test audits and helps meet IT governance criteria. Good traceability also makes test progress and completion reports more easily understandable by including status of test basis elements. This can also assist in communicating technical aspects of testing to stakeholders in an understandable manner. Traceability provides information to assess product quality process capability and project progress against business goals. v4.0 Page of 74 2023-04-21\n",
      "----------------------------------------\n",
      "Section Title: 1.5\n",
      "Section Content: Essential Skills and Good Practices in Testing Skill is ability to do something well that comes from one’s knowledge practice and aptitude. Good testers should possess some essential skills to do their job well. Good testers should be effective team players and should be able to perform testing on different levels of test independence.\n",
      "----------------------------------------\n",
      "Section Title: 1.5.2\n",
      "Section Content: Whole Team Approach One of important skills for a tester is ability to work effectively in a team context and to contribute positively to team goals. whole team approach – a practice coming from Extreme Programming (see section 2.1) – builds upon this skill. In whole-team approach any team member with necessary knowledge and skills can perform any task and everyone is responsible for quality. team members share same workspace (physical or virtual) as co-location facilitates communication and interaction. whole team approach improves team dynamics enhances communication and collaboration within team and creates synergy by allowing various skill sets within team to be leveraged for benefit of project. Testers work closely with other team members to ensure that desired quality levels are achieved. This includes collaborating with business representatives to help them create suitable acceptance tests and working with developers to agree on test strategy and decide on test automation approaches. Testers can thus transfer testing knowledge to other team members and influence development of product. Depending on context whole team approach may not always be appropriate. For instance in some situations such as safety-critical a high level of test independence may be needed.\n",
      "----------------------------------------\n",
      "Section Title: 2.1\n",
      "Section Content: 1 (K2) Explain impact of chosen software development lifecycle on testing FL-\n",
      "----------------------------------------\n",
      "Section Title: 2.1\n",
      "Section Content: 3 (K1) Recall examples of test-first approaches to development FL-\n",
      "----------------------------------------\n",
      "Section Title: 2.1\n",
      "Section Content: 5 (K2) Explain shift-left approach FL-\n",
      "----------------------------------------\n",
      "Section Title: 2.2\n",
      "Section Content: 1 (K2) Distinguish different test levels FL-\n",
      "----------------------------------------\n",
      "Section Title: 2.2\n",
      "Section Content: 3 (K2) Distinguish confirmation testing from regression testing 2.3 Maintenance Testing FL-\n",
      "----------------------------------------\n",
      "Section Title: 2.1\n",
      "Section Content: Testing in Context of a Software Development Lifecycle A software development lifecycle (SDLC) model is an abstract high-level representation of software development process. A SDLC model defines how different development phases and types of activities performed within this process relate to each other both logically and chronologically. Examples of SDLC models include: sequential development models (e.g. waterfall model V-model) iterative development models (e.g. spiral model prototyping) and incremental development models (e.g. Unified Process). Some activities within software development processes can also be described by more detailed software development methods and Agile practices. Examples include: acceptance test-driven development (ATDD) behavior-driven development (BDD) domain-driven design (DDD) extreme programming (XP) feature-driven development (FDD) Kanban Lean IT Scrum and test-driven development (TDD).\n",
      "----------------------------------------\n",
      "Section Title: 2.1.2\n",
      "Section Content: Software Development Lifecycle and Good Testing Practices Good testing practices independent of chosen SDLC model include following:  For every software development activity there is a corresponding test activity so that all development activities are subject to quality control  Different test levels (see chapter\n",
      "----------------------------------------\n",
      "Section Title: 2.1\n",
      "Section Content: 5)\n",
      "----------------------------------------\n",
      "Section Title: 2.1\n",
      "Section Content: 5) since tests are defined before code is written. They support an iterative development model. These approaches are characterized as follows: Test-Driven Development (TDD):  Directs coding through test cases (instead of extensive software design) (Beck 2003)  Tests are written first then code is written to satisfy tests and then tests and code are refactored Acceptance Test-Driven Development (ATDD) (see section\n",
      "----------------------------------------\n",
      "Section Title: 2.1.4\n",
      "Section Content: DevOps and Testing DevOps is an organizational approach aiming to create synergy by getting development (including testing) and operations to work together to achieve a set of common goals. DevOps requires a cultural shift within an organization to bridge gaps between development (including testing) and operations while treating their functions with equal value. DevOps promotes team autonomy fast feedback integrated toolchains and technical practices like continuous integration (CI) and continuous delivery (CD). This enables teams to build test and release high-quality code faster through a DevOps delivery pipeline (Kim 2016). From testing perspective some of benefits of DevOps are:  Fast feedback on code quality and whether changes adversely affect existing code  CI promotes a shift-left approach in testing (see section\n",
      "----------------------------------------\n",
      "Section Title: 2.1.5\n",
      "Section Content: Shift-Left Approach principle of early testing (see section 1.3) is sometimes referred to as shift-left because it is an approach where testing is performed earlier in SDLC. Shift-left normally suggests that testing should be done earlier (e.g. not waiting for code to be implemented or for components to be integrated) but it does not mean that testing later in SDLC should be neglected. There are some good practices that illustrate how to achieve a “shift-left” in testing which include:  Reviewing specification from perspective of testing. These review activities on specifications often find potential defects such as ambiguities incompleteness and inconsistencies  Writing test cases before code is written and have code run in a test harness during code implementation  Using CI and even better CD as it comes with fast feedback and automated component tests to accompany source code when it is submitted to code repository  Completing static analysis of source code prior to dynamic testing or as part of an automated process  Performing non-functional testing starting at component test level where possible. This is a form of shift-left as these non-functional test types tend to be performed later in SDLC when a complete system and a representative test environment are available A shift-left approach might result in extra training effort and/or costs earlier in process but is expected to save efforts and/or costs later in process. For shift-left approach it is important that stakeholders are convinced and bought into this concept.\n",
      "----------------------------------------\n",
      "Section Title: 5.3\n",
      "Section Content: 2). Retrospectives are critical for successful implementation of continuous improvement and it is important that any recommended improvements are followed up. Typical benefits for testing include:  Increased test effectiveness / efficiency (e.g. by implementing suggestions for process improvement)  Increased quality of testware (e.g. by jointly reviewing test processes)  Team bonding and learning (e.g. as a result of opportunity to raise issues and propose improvement points)  Improved quality of test basis (e.g. as deficiencies in extent and quality of requirements could be addressed and solved)  Better cooperation between development and testing (e.g. as collaboration is reviewed and optimized regularly)\n",
      "----------------------------------------\n",
      "Section Title: 2.2.1\n",
      "Section Content: Test Levels In this syllabus following five test levels are described:  Component testing (also known as unit testing) focuses on testing components in isolation. It often requires specific support such as test harnesses or unit test frameworks. Component testing is normally performed by developers in their development environments.  Component integration testing (also known as unit integration testing) focuses on testing interfaces and interactions between components. Component integration testing is heavily dependent on integration strategy approaches like bottom-up top-down or big-bang.  System testing focuses on overall behavior and capabilities of an entire system or product often including functional testing of end-to-end tasks and non-functional testing of quality characteristics. For some non-functional quality characteristics it is preferable to test them on a complete system in a representative test environment (e.g. usability). Using simulations of sub- v4.0 Page of 74 2023-04-21  systems is also possible. System testing may be performed by an independent test team and is related to specifications for system.  System integration testing focuses on testing interfaces of system under test and other systems and external services . System integration testing requires suitable test environments preferably similar to operational environment.  Acceptance testing focuses on validation and on demonstrating readiness for deployment which means that system fulfills user’s business needs. Ideally acceptance testing should be performed by intended users. main forms of acceptance testing are: user acceptance testing (UAT) operational acceptance testing contractual and regulatory acceptance testing alpha testing and beta testing. Test levels are distinguished by following non-exhaustive list of attributes to avoid overlapping of test activities:  Test object  Test objectives  Test basis  Defects and failures  Approach and responsibilities\n",
      "----------------------------------------\n",
      "Section Title: 2.2.3\n",
      "Section Content: Confirmation Testing and Regression Testing Changes are typically made to a component or system to either enhance it by adding a new feature or to fix it by removing a defect. Testing should then also include confirmation testing and regression testing. Confirmation testing confirms that an original defect has been successfully fixed. Depending on risk one can test fixed version of software in several ways including:  executing all test cases that previously have failed due to defect or also by  adding new tests to cover any changes that were needed to fix defect However when time or money is short when fixing defects confirmation testing might be restricted to simply exercising steps that should reproduce failure caused by defect and checking that failure does not occur. Regression testing confirms that no adverse consequences have been caused by a change including a fix that has already been confirmation tested. These adverse consequences could affect same component where change was made other components in same system or even other connected systems. Regression testing may not be restricted to test object itself but can also be related to environment. It is advisable first to perform an impact analysis to optimize extent of regression testing. Impact analysis shows which parts of software could be affected. Regression test suites are run many times and generally number of regression test cases will increase with each iteration or release so regression testing is a strong candidate for automation. Automation of these tests should start early in project. Where CI is used such as in DevOps (see section\n",
      "----------------------------------------\n",
      "Section Title: 2.3\n",
      "Section Content: Maintenance Testing There are different categories of maintenance it can be corrective adaptive to changes in environment or improve performance or maintainability (see ISO/IEC 14764 for details) so maintenance can involve planned releases/deployments and unplanned releases/deployments (hot fixes). Impact analysis may be done before a change is made to help decide if change should be made based on potential consequences in other areas of system. Testing changes to a system in production includes both evaluating success of implementation of change and checking for possible regressions in parts of system that remain unchanged (which is usually most of system). scope of maintenance testing typically depends on:  degree of risk of change  size of existing system  size of change triggers for maintenance and maintenance testing can be classified as follows:  Modifications such as planned enhancements (i.e. release-based) corrective changes or hot fixes.  Upgrades or migrations of operational environment such as from one platform to another which can require tests associated with new environment as well as of changed software or tests of data conversion when data from another application is migrated into system being maintained.  Retirement such as when an application reaches end of its life. When a system is retired this can require testing of data archiving if long data-retention periods are required. Testing of restore and retrieval procedures after archiving may also be needed in event that certain data is required during archiving period. v4.0 Page of 74 2023-04-21  3. Static Testing – 80 minutes Keywords anomaly dynamic testing formal review informal review inspection review static analysis static testing technical review walkthrough Learning Objectives for Chapter 3: 3.1 Static Testing Basics FL-\n",
      "----------------------------------------\n",
      "Section Title: 3.1\n",
      "Section Content: 2 (K2) Explain value of static testing FL-\n",
      "----------------------------------------\n",
      "Section Title: 3.2\n",
      "Section Content: 1 (K1) Identify benefits of early and frequent stakeholder feedback FL-\n",
      "----------------------------------------\n",
      "Section Title: 3.2\n",
      "Section Content: 3 (K1) Recall which responsibilities are assigned to principal roles when performing reviews FL-\n",
      "----------------------------------------\n",
      "Section Title: 3.2\n",
      "Section Content: 5 (K1) Recall factors that contribute to a successful review v4.0 Page of 74 2023-04-21\n",
      "----------------------------------------\n",
      "Section Title: 5.1\n",
      "Section Content: 3). Review techniques can be applied to ensure user stories are complete and understandable and include testable acceptance criteria. By asking right questions testers explore challenge and help improve proposed user stories. Static analysis can identify problems prior to dynamic testing while often requiring less effort since no test cases are required and tools (see chapter 6) are typically used. Static analysis is often incorporated into CI frameworks (see section\n",
      "----------------------------------------\n",
      "Section Title: 3.1.1\n",
      "Section Content: Work Products Examinable by Static Testing Almost any work product can be examined using static testing. Examples include requirement specification documents source code test plans test cases product backlog items test charters project documentation contracts and models. Any work product that can be read and understood can be subject of a review. However for static analysis work products need a structure against which they can be checked (e.g. models code or text with a formal syntax). Work products that are not appropriate for static testing include those that are difficult to interpret by human beings and that should not be analyzed by tools (e.g. 3rd party executable code due to legal reasons).\n",
      "----------------------------------------\n",
      "Section Title: 3.1.3\n",
      "Section Content: Differences between Static Testing and Dynamic Testing Static testing and dynamic testing practices complement each other. They have similar objectives such as supporting detection of defects in work products (see section\n",
      "----------------------------------------\n",
      "Section Title: 3.2\n",
      "Section Content: Feedback and Review Process\n",
      "----------------------------------------\n",
      "Section Title: 3.2.2\n",
      "Section Content: Review Process Activities ISO/IEC 20246 standard defines a generic review process that provides a structured but flexible framework from which a specific review process may be tailored to a particular situation. If required review is more formal then more of tasks described for different activities will be needed. size of many work products makes them too large to be covered by a single review. review process may be invoked a couple of times to complete review for entire work product. activities in review process are:  Planning. During planning phase scope of review which comprises purpose work product to be reviewed quality characteristics to be evaluated areas to focus on exit criteria supporting information such as standards effort and timeframes for review shall be defined.  Review initiation. During review initiation goal is to make sure that everyone and everything involved is prepared to start review. This includes making sure that every participant has access to work product under review understands their role and responsibilities and receives everything needed to perform review.  Individual review. Every reviewer performs an individual review to assess quality of work product under review and to identify anomalies recommendations and questions by applying one or more review techniques (e.g. checklist-based reviewing scenario-based reviewing). ISO/IEC 20246 standard provides more depth on different review techniques. reviewers log all their identified anomalies recommendations and questions.  Communication and analysis. Since anomalies identified during a review are not necessarily defects all these anomalies need to be analyzed and discussed. For every anomaly decision should be made on its status ownership and required actions. This is typically done in a review meeting during which participants also decide what quality level of reviewed work product is and what follow-up actions are required. A follow-up review may be required to complete actions.  Fixing and reporting. For every defect a defect report should be created so that corrective actions can be followed-up. Once exit criteria are reached work product can be accepted. review results are reported.\n",
      "----------------------------------------\n",
      "Section Title: 3.2.4\n",
      "Section Content: Review Types There exist many review types ranging from informal reviews to formal reviews. required level of formality depends on factors such as SDLC being followed maturity of development process criticality and complexity of work product being reviewed legal or regulatory requirements and need for an audit trail. same work product can be reviewed with different review types e.g. first an informal one and later a more formal one. Selecting right review type is key to achieving required review objectives (see section\n",
      "----------------------------------------\n",
      "Section Title: 3.2\n",
      "Section Content: 2). main objective is to find maximum number of anomalies. Other objectives are to evaluate quality build confidence in work product and to motivate and enable authors to improve. Metrics are collected and used to improve SDLC including inspection process. In inspections author cannot act as review leader or scribe.\n",
      "----------------------------------------\n",
      "Section Title: 3.2\n",
      "Section Content: 1)  Providing adequate time to participants to prepare for review  Support from management for review process  Making reviews part of organization’s culture to promote learning and process improvement  Providing adequate training for all participants so they know how to fulfil their role  Facilitating meetings v4.0 Page of 74 2023-04-21  4. Test Analysis and Design – 390 minutes Keywords acceptance criteria acceptance test-driven development black-box test technique boundary value analysis branch coverage checklist-based testing collaboration-based test approach coverage coverage item decision table testing equivalence partitioning error guessing experience-based test technique exploratory testing state transition testing statement coverage test technique white-box test technique   v4.0 Page of 74 2023-04-21\n",
      "----------------------------------------\n",
      "Section Title: 4.2\n",
      "Section Content: Black-Box Test Techniques Commonly used black-box test techniques discussed in following sections are:  Equivalence Partitioning  Boundary Value Analysis  Decision Table Testing  State Transition Testing\n",
      "----------------------------------------\n",
      "Section Title: 4.2.2\n",
      "Section Content: Boundary Value Analysis Boundary Value Analysis (BVA) is a technique based on exercising boundaries of equivalence partitions. Therefore BVA can only be used for ordered partitions. minimum and maximum values of a partition are its boundary values. In case of BVA if two elements belong to same partition all elements between them must also belong to that partition. BVA focuses on boundary values of partitions because developers are more likely to make errors with these boundary values. Typical defects found by BVA are located where implemented boundaries are misplaced to positions above or below their intended positions or are omitted altogether. This syllabus covers two versions of BVA: 2-value and 3-value BVA. They differ in terms of coverage items per boundary that need to be exercised to achieve 100% coverage. In 2-value BVA (Craig 2002 Myers 2011) for each boundary value there are two coverage items: this boundary value and its closest neighbor belonging to adjacent partition. To achieve 100% coverage with 2-value BVA test cases must exercise all coverage items i.e. all identified boundary values. Coverage is measured as number of boundary values that were exercised divided by total number of identified boundary values and is expressed as a percentage. In 3-value BVA (Koomen 2006 O’Regan 2019) for each boundary value there are three coverage items: this boundary value and both its neighbors. Therefore in 3-value BVA some of coverage items may not be boundary values. To achieve 100% coverage with 3-value BVA test cases must exercise all coverage items i.e. identified boundary values and their neighbors. Coverage is measured as number of boundary values and their neighbors exercised divided by total number of identified boundary values and their neighbors and is expressed as a percentage. 3-value BVA is more rigorous than 2-value BVA as it may detect defects overlooked by 2-value BVA. For example if decision “if (x ≤ 10) …” is incorrectly implemented as “if (x = 10) …” no test data derived from 2-value BVA (x = 10 x = 11) can detect defect. However x = 9 derived from 3-value BVA is likely to detect it. v4.0 Page of 74 2023-04-21\n",
      "----------------------------------------\n",
      "Section Title: 4.2.4\n",
      "Section Content: State Transition Testing A state transition diagram models behavior of a system by showing its possible states and valid state transitions. A transition is initiated by an event which may be additionally qualified by a guard condition. transitions are assumed to be instantaneous and may sometimes result in software taking action. common transition labeling syntax is as follows: “event [guard condition] / action”. Guard conditions and actions can be omitted if they do not exist or are irrelevant for tester. A state table is a model equivalent to a state transition diagram. Its rows represent states and its columns represent events (together with guard conditions if they exist). Table entries (cells) represent transitions and contain target state as well as resulting actions if defined. In contrast to state transition diagram state table explicitly shows invalid transitions which are represented by empty cells. A test case based on a state transition diagram or state table is usually represented as a sequence of events which results in a sequence of state changes (and actions if needed). One test case may and usually will cover several transitions between states. There exist many coverage criteria for state transition testing. This syllabus discusses three of them. v4.0 Page of 74 2023-04-21  In all states coverage coverage items are states. To achieve 100% all states coverage test cases must ensure that all states are visited. Coverage is measured as number of visited states divided by total number of states and is expressed as a percentage. In valid transitions coverage (also called 0-switch coverage) coverage items are single valid transitions. To achieve 100% valid transitions coverage test cases must exercise all valid transitions. Coverage is measured as number of exercised valid transitions divided by total number of valid transitions and is expressed as a percentage. In all transitions coverage coverage items are all transitions shown in a state table. To achieve 100% all transitions coverage test cases must exercise all valid transitions and attempt to execute invalid transitions. Testing only one invalid transition in a single test case helps to avoid fault masking i.e. a situation in which one defect prevents detection of another. Coverage is measured as number of valid and invalid transitions exercised or attempted to be covered by executed test cases divided by total number of valid and invalid transitions and is expressed as a percentage. All states coverage is weaker than valid transitions coverage because it can typically be achieved without exercising all transitions. Valid transitions coverage is most widely used coverage criterion. Achieving full valid transitions coverage guarantees full all states coverage. Achieving full all transitions coverage guarantees both full all states coverage and full valid transitions coverage and should be a minimum requirement for mission and safety-critical software.\n",
      "----------------------------------------\n",
      "Section Title: 4.3.1\n",
      "Section Content: Statement Testing and Statement Coverage In statement testing coverage items are executable statements. aim is to design test cases that exercise statements in code until an acceptable level of coverage is achieved. Coverage is measured as number of statements exercised by test cases divided by total number of executable statements in code and is expressed as a percentage. When 100% statement coverage is achieved it ensures that all executable statements in code have been exercised at least once. In particular this means that each statement with a defect will be executed which may cause a failure demonstrating presence of defect. However exercising a statement with a test case will not detect defects in all cases. For example it may not detect defects that are data dependent (e.g. a division by zero that only fails when a denominator is set to zero). Also 100% statement coverage does not ensure that all decision logic has been tested as for instance it may not exercise all branches (see chapter\n",
      "----------------------------------------\n",
      "Section Title: 4.3.2\n",
      "Section Content: Branch Testing and Branch Coverage A branch is a transfer of control between two nodes in control flow graph which shows possible sequences in which source code statements are executed in test object. Each transfer of control can be either unconditional (i.e. straight-line code) or conditional (i.e. a decision outcome). In branch testing coverage items are branches and aim is to design test cases to exercise branches in code until an acceptable level of coverage is achieved. Coverage is measured as number of branches exercised by test cases divided by total number of branches and is expressed as a percentage. When 100% branch coverage is achieved all branches in code unconditional and conditional are exercised by test cases. Conditional branches typically correspond to a true or false outcome from an “if...then” decision an outcome from a switch/case statement or a decision to exit or continue in a loop. However exercising a branch with a test case will not detect defects in all cases. For example it may not detect defects requiring execution of a specific path in a code. Branch coverage subsumes statement coverage. This means that any set of test cases achieving 100% branch coverage also achieves 100% statement coverage (but not vice versa).\n",
      "----------------------------------------\n",
      "Section Title: 4.4\n",
      "Section Content: Experience-based Test Techniques Commonly used experience-based test techniques discussed in following sections are:  Error guessing  Exploratory testing  Checklist-based testing\n",
      "----------------------------------------\n",
      "Section Title: 4.4.2\n",
      "Section Content: Exploratory Testing In exploratory testing tests are simultaneously designed executed and evaluated while tester learns about test object. testing is used to learn more about test object to explore it more deeply with focused tests and to create tests for untested areas. Exploratory testing is sometimes conducted using session-based testing to structure testing. In a session-based approach exploratory testing is conducted within a defined time-box. tester uses a test charter containing test objectives to guide testing. test session is usually followed by a debriefing that involves a discussion between tester and stakeholders interested in test results of test session. In this approach test objectives may be treated as high-level test conditions. Coverage items are identified and exercised during test session. tester may use test session sheets to document steps followed and discoveries made. Exploratory testing is useful when there are few or inadequate specifications or there is significant time pressure on testing. Exploratory testing is also useful to complement other more formal test techniques. Exploratory testing will be more effective if tester is experienced has domain knowledge and has a high degree of essential skills like analytical skills curiosity and creativeness (see section\n",
      "----------------------------------------\n",
      "Section Title: 4.4.3\n",
      "Section Content: Checklist-Based Testing In checklist-based testing a tester designs implements and executes tests to cover test conditions from a checklist. Checklists can be built based on experience knowledge about what is important for user or an understanding of why and how software fails. Checklists should not contain items that can be checked automatically items better suited as entry/exit criteria or items that are too general (Brykczynski 1999). Checklist items are often phrased in form of a question. It should be possible to check each item separately and directly. These items may refer to requirements graphical interface properties quality characteristics or other forms of test conditions. Checklists can be created to support various test types including functional and non-functional testing (e.g. 10 heuristics for usability testing (Nielsen 1994)). v4.0 Page of 74 2023-04-21  Some checklist entries may gradually become less effective over time because developers will learn to avoid making same errors. New entries may also need to be added to reflect newly found high severity defects. Therefore checklists should be regularly updated based on defect analysis. However care should be taken to avoid letting checklist become too long (Gawande 2009). In absence of detailed test cases checklist-based testing can provide guidelines and some degree of consistency for testing. If checklists are high-level some variability in actual testing is likely to occur resulting in potentially greater coverage but less repeatability.\n",
      "----------------------------------------\n",
      "Section Title: 4.5.1\n",
      "Section Content: Collaborative User Story Writing A user story represents a feature that will be valuable to either a user or purchaser of a system or software. User stories have three critical aspects (Jeffries 2000) called together “3 C’s”:  Card – medium describing a user story (e.g. an index card an entry in an electronic board)  Conversation – explains how software will be used (can be documented or verbal)  Confirmation – acceptance criteria (see section\n",
      "----------------------------------------\n",
      "Section Title: 4.5.2\n",
      "Section Content: Acceptance Criteria Acceptance criteria for a user story are conditions that an implementation of user story must meet to be accepted by stakeholders. From this perspective acceptance criteria may be viewed as test conditions that should be exercised by tests. Acceptance criteria are usually a result of Conversation (see section\n",
      "----------------------------------------\n",
      "Section Title: 4.5\n",
      "Section Content: 3) v4.0 Page of 74 2023-04-21   Allow accurate planning and estimation There are several ways to write acceptance criteria for a user story. two most common formats are:  Scenario-oriented (e.g. Given/When/Then format used in BDD see section\n",
      "----------------------------------------\n",
      "Section Title: 4.5.3\n",
      "Section Content: Acceptance Test-driven Development (ATDD) ATDD is a test-first approach (see section\n",
      "----------------------------------------\n",
      "Section Title: 5.1\n",
      "Section Content: 1 (K2) Exemplify purpose and content of a test plan FL-\n",
      "----------------------------------------\n",
      "Section Title: 5.1\n",
      "Section Content: 3 (K2) Compare and contrast entry criteria and exit criteria FL-\n",
      "----------------------------------------\n",
      "Section Title: 5.1\n",
      "Section Content: 5 (K3) Apply test case prioritization FL-\n",
      "----------------------------------------\n",
      "Section Title: 5.1\n",
      "Section Content: 7 (K2) Summarize testing quadrants and their relationships with test levels and test types 5.2 Risk Management FL-\n",
      "----------------------------------------\n",
      "Section Title: 5.2\n",
      "Section Content: 2 (K2) Distinguish between project risks and product risks FL-\n",
      "----------------------------------------\n",
      "Section Title: 5.2\n",
      "Section Content: 4 (K2) Explain what measures can be taken in response to analyzed product risks 5.3 Test Monitoring Test Control and Test Completion FL-\n",
      "----------------------------------------\n",
      "Section Title: 5.3\n",
      "Section Content: 2 (K2) Summarize purposes content and audiences for test reports FL-\n",
      "----------------------------------------\n",
      "Section Title: 5.4\n",
      "Section Content: 1 (K2) Summarize how configuration management supports testing 5.5 Defect Management FL-\n",
      "----------------------------------------\n",
      "Section Title: 5.1\n",
      "Section Content: Test Planning\n",
      "----------------------------------------\n",
      "Section Title: 5.1.2\n",
      "Section Content: Tester's Contribution to Iteration and Release Planning In iterative SDLCs typically two kinds of planning occur: release planning and iteration planning. Release planning looks ahead to release of a product defines and re-defines product backlog and may involve refining larger user stories into a set of smaller user stories. It also serves as basis for test approach and test plan across all iterations. Testers involved in release planning participate in writing testable user stories and acceptance criteria (see section 4.5) participate in project and quality risk analyses (see section 5.2) estimate test effort associated with user stories (see section\n",
      "----------------------------------------\n",
      "Section Title: 5.1.3\n",
      "Section Content: Entry Criteria and Exit Criteria Entry criteria define preconditions for undertaking a given activity. If entry criteria are not met it is likely that activity will prove to be more difficult time-consuming costly and riskier. Exit criteria define what must be achieved in order to declare an activity completed. Entry criteria and exit criteria should be defined for each test level and will differ based on test objectives. Typical entry criteria include: availability of resources (e.g. people tools environments test data budget time) availability of testware (e.g. test basis testable requirements user stories test cases) and initial quality level of a test object (e.g. all smoke tests have passed). Typical exit criteria include: measures of thoroughness (e.g. achieved level of coverage number of unresolved defects defect density number of failed test cases) and completion criteria (e.g. planned tests have been executed static testing has been performed all defects found are reported all regression tests are automated). Running out of time or budget can also be viewed as valid exit criteria. Even without other exit criteria being satisfied it can be acceptable to end testing under such circumstances if stakeholders have reviewed and accepted risk to go live without further testing. In Agile software development exit criteria are often called Definition of Done defining team’s objective metrics for a releasable item. Entry criteria that a user story must fulfill to start development and/or testing activities are called Definition of Ready.\n",
      "----------------------------------------\n",
      "Section Title: 5.1.5\n",
      "Section Content: Test Case Prioritization Once test cases and test procedures are specified and assembled into test suites these test suites can be arranged in a test execution schedule that defines order in which they are to be run. When prioritizing test cases different factors can be taken into account. most commonly used test case prioritization strategies are as follows:  Risk-based prioritization where order of test execution is based on results of risk analysis (see section\n",
      "----------------------------------------\n",
      "Section Title: 5.1.6\n",
      "Section Content: Test Pyramid test pyramid is a model showing that different tests may have different granularity. test pyramid model supports team in test automation and in test effort allocation by showing that different goals are supported by different levels of test automation. pyramid layers represent groups of tests. higher layer lower test granularity test isolation and test execution time. Tests in bottom layer are small isolated fast and check a small piece of functionality so usually a lot of them are needed to achieve a reasonable coverage. top layer represents complex high-level end-to-end tests. These high-level tests are generally slower than tests from lower layers and they typically check a large piece of functionality so usually just a few of them are needed to achieve a reasonable coverage. number and naming of layers may differ. For example original test pyramid model (Cohn 2009) defines three layers: “unit tests” “service tests” and “UI tests”. Another popular model defines unit v4.0 Page of 74 2023-04-21  (component) tests integration (component integration) tests and end-to-end tests. Other test levels (see section\n",
      "----------------------------------------\n",
      "Section Title: 5.1.7\n",
      "Section Content: Testing Quadrants testing quadrants defined by Brian Marick (Marick 2003 Crispin 2008) group test levels with appropriate test types activities test techniques and work products in Agile software development. model supports test management in visualizing these to ensure that all appropriate test types and test levels are included in SDLC and in understanding that some test types are more relevant to certain test levels than others. This model also provides a way to differentiate and describe types of tests to all stakeholders including developers testers and business representatives. In this model tests can be business facing or technology facing. Tests can also support team (i.e. guide development) or critique product (i.e. measure its behavior against expectations). combination of these two viewpoints determines four quadrants:  Quadrant Q1 (technology facing support team). This quadrant contains component and component integration tests. These tests should be automated and included in CI process.  Quadrant Q2 (business facing support team). This quadrant contains functional tests examples user story tests user experience prototypes API testing and simulations. These tests check acceptance criteria and can be manual or automated.  Quadrant Q3 (business facing critique product). This quadrant contains exploratory testing usability testing user acceptance testing. These tests are user-oriented and often manual.  Quadrant Q4 (technology facing critique product). This quadrant contains smoke tests and non-functional tests (except usability tests). These tests are often automated.\n",
      "----------------------------------------\n",
      "Section Title: 5.2\n",
      "Section Content: 3)  Risk control (consisting of risk mitigation and risk monitoring; see section\n",
      "----------------------------------------\n",
      "Section Title: 5.2.1\n",
      "Section Content: Risk Definition and Risk Attributes Risk is a potential event hazard threat or situation whose occurrence causes an adverse effect. A risk can be characterized by two factors:  Risk likelihood – probability of risk occurrence (greater than zero and less than one)  Risk impact (harm) – consequences of this occurrence v4.0 Page of 74 2023-04-21  These two factors express risk level which is a measure for risk. higher risk level more important is its treatment.\n",
      "----------------------------------------\n",
      "Section Title: 5.2.3\n",
      "Section Content: Product Risk Analysis From a testing perspective goal of product risk analysis is to provide an awareness of product risk in order to focus testing effort in a way that minimizes residual level of product risk. Ideally product risk analysis begins early in SDLC. Product risk analysis consists of risk identification and risk assessment. Risk identification is about generating a comprehensive list of risks. Stakeholders can identify risks by using various techniques and tools e.g. brainstorming workshops interviews or cause-effect diagrams. Risk assessment involves: categorization of identified risks determining their risk likelihood risk impact and level prioritizing and proposing ways to handle them. Categorization helps in assigning mitigation actions because usually risks falling into same category can be mitigated using a similar approach. Risk assessment can use a quantitative or qualitative approach or a mix of them. In quantitative approach risk level is calculated as multiplication of risk likelihood and risk impact. In qualitative approach risk level can be determined using a risk matrix. Product risk analysis may influence thoroughness and scope of testing. Its results are used to: v4.0 Page of 74 2023-04-21   Determine scope of testing to be carried out  Determine particular test levels and propose test types to be performed  Determine test techniques to be employed and coverage to be achieved  Estimate test effort required for each task  Prioritize testing in an attempt to find critical defects as early as possible  Determine whether any activities in addition to testing could be employed to reduce risk\n",
      "----------------------------------------\n",
      "Section Title: 5.3\n",
      "Section Content: Test Monitoring Test Control and Test Completion Test monitoring is concerned with gathering information about testing. This information is used to assess test progress and to measure whether test exit criteria or test tasks associated with exit criteria are satisfied such as meeting targets for coverage of product risks requirements or acceptance criteria. Test control uses information from test monitoring to provide in a form of control directives guidance and necessary corrective actions to achieve most effective and efficient testing. Examples of control directives include:  Reprioritizing tests when an identified risk becomes an issue  Re-evaluating whether a test item meets entry criteria or exit criteria due to rework  Adjusting test schedule to address a delay in delivery of test environment  Adding new resources when and where needed v4.0 Page of 74 2023-04-21  Test completion collects data from completed test activities to consolidate experience testware and any other relevant information. Test completion activities occur at project milestones such as when a test level is completed an agile iteration is finished a test project is completed (or cancelled) a software system is released or a maintenance release is completed.\n",
      "----------------------------------------\n",
      "Section Title: 5.3.2\n",
      "Section Content: Purpose Content and Audience for Test Reports Test reporting summarizes and communicates test information during and after testing. Test progress reports support ongoing control of testing and must provide enough information to make modifications to test schedule resources or test plan when such changes are needed due to deviation from plan or changed circumstances. Test completion reports summarize a specific stage of testing (e.g. test level test cycle iteration) and can give information for subsequent testing. During test monitoring and control test team generates test progress reports for stakeholders to keep them informed. Test progress reports are usually generated on a regular basis (e.g. daily weekly etc.) and include:  Test period  Test progress (e.g. ahead or behind schedule) including any notable deviations  Impediments for testing and their workarounds  Test metrics (see section\n",
      "----------------------------------------\n",
      "Section Title: 5.3.3\n",
      "Section Content: Communicating Status of Testing best means of communicating test status varies depending on test management concerns organizational test strategies regulatory standards or in case of self-organizing teams (see section\n",
      "----------------------------------------\n",
      "Section Title: 5.3\n",
      "Section Content: 2) One or more of these options can be used. More formal communication may be more appropriate for distributed teams where direct face-to-face communication is not always possible due to geographical distance or time differences. Typically different stakeholders are interested in different types of information so communication should be tailored accordingly.\n",
      "----------------------------------------\n",
      "Section Title: 2.1\n",
      "Section Content: 4) in which automated CM is normally included.\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Define a regular expression pattern to match section titles\n",
    "#section_pattern = r'\\d+\\.\\d+\\.\\d+\\.'\n",
    "\n",
    "# using combined reg. exp to extract 1.1.1. and 1.2.\n",
    "section_pattern_3d = r'\\d+\\.\\d+\\.\\d+\\.'  # Pattern for \"1.1.1.\"\n",
    "section_pattern_2d = r'\\d+\\.\\d+\\.'    # Pattern for \"1.2.\"\n",
    "\n",
    "combined_pattern = f\"({section_pattern_3d}|{section_pattern_2d})\"\n",
    "\n",
    "\n",
    "# Using re.finditer to find all section titles and their starting positions\n",
    "section_matches = re.finditer(combined_pattern, extracted_text)\n",
    "\n",
    "# Create lists to store sections\n",
    "sections = []\n",
    "\n",
    "# Iterate through section matches\n",
    "for match in section_matches:\n",
    "    start_pos = match.start()\n",
    "    end_pos = (\n",
    "        match.end()\n",
    "        if match.end() < len(extracted_text)\n",
    "        else len(extracted_text)\n",
    "    )\n",
    "    section_title = match.group().strip()\n",
    "    \n",
    "    # Remove the last dot from the section title\n",
    "    section_title = section_title[:-1]  # Remove the last dot\n",
    "    \n",
    "    # Find the corresponding section content based on section title position\n",
    "    content_start = end_pos\n",
    "    content_end = (\n",
    "        next(section_matches).start()\n",
    "        if content_start < len(extracted_text)\n",
    "        else len(extracted_text)\n",
    "    )\n",
    "    section_content = extracted_text[content_start:content_end].strip()\n",
    "    \n",
    "    sections.append((section_title, section_content))\n",
    "\n",
    "# Print the extracted sections\n",
    "if sections:\n",
    "    for section in sections:\n",
    "        print(\"Section Title:\", section[0])\n",
    "        print(\"Section Content:\", section[1])\n",
    "        print(\"-\" * 40)\n",
    "else:\n",
    "    print(\"No sections found in the text, you did something wrong check once more\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DRAFT\n",
    "\n",
    "# Sample text (replace this with your actual text)\n",
    "\n",
    "# Define a regular expression pattern to match section titles\n",
    "# section_pattern = r'\\b\\d+\\.\\d+(?:\\.\\d+)?(?=\\s)'\n",
    "# \n",
    "#Using re.finditer to find all section titles and their starting positions\n",
    "# section_matches = re.finditer(section_pattern, extracted_text)\n",
    "# \n",
    "#Create lists to store sections\n",
    "# sections = []\n",
    "# \n",
    "#Iterate through section matches\n",
    "# for match in section_matches:\n",
    "    # start_pos = match.start()\n",
    "    # end_pos = (\n",
    "        # match.end()\n",
    "        # if match.end() < len(extracted_text)\n",
    "        # else len(extracted_text)\n",
    "    # )\n",
    "    # section_title = match.group().strip()\n",
    "    # \n",
    "#   Remove the last dot from the section title\n",
    "    # section_title = section_title[:-1]  # Remove the last dot\n",
    "    # \n",
    "#    Find the corresponding section content based on section title position\n",
    "    # content_start = end_pos\n",
    "    # content_end = (\n",
    "        # next(section_matches).start()\n",
    "        # if content_start < len(extracted_text)\n",
    "        # else len(extracted_text)\n",
    "    # )\n",
    "    # section_content = extracted_text[content_start:content_end].strip()\n",
    "    # \n",
    "    # sections.append((section_title, section_content))\n",
    "# \n",
    "#Print the extracted sections\n",
    "# if sections:\n",
    "    # for section in sections:\n",
    "        # print(\"Section Title:\", section[0])\n",
    "        # print(\"Section Content:\", section[1])\n",
    "        # print(\"-\" * 40)\n",
    "# else:\n",
    "    # print(\"No sections found in the text.\")\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!Base Modeling!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: testing can trigger failures that are caused by defects in software (dynamic testing) or can directly find defects in test object (static testing) debugging is concerned with finding causes of this failure. subsequent confirmation testing checks whether fixes resolved problem.\n"
     ]
    }
   ],
   "source": [
    "#T5\n",
    "\n",
    "# Load the pre-trained T5 model and tokenizer\n",
    "model_name = \"t5-small\"\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Input text (use sections[0][1] as the content of the first section)\n",
    "section_content = sections[1][1]\n",
    "\n",
    "# Tokenize the input section\n",
    "input_ids = tokenizer.encode(\"summarize: \" + section_content, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "\n",
    "# Generate the summary\n",
    "summary_ids = model.generate(input_ids, max_length=150, min_length=30, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Print the summary\n",
    "print(\"Summary:\", summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keywords extraction - missing so far, leads to HTTPSConnectionPool(host='cdn-lfs.huggingface.co', port=443): Read timed out. A lot of hours are required. \n",
    "# Talk to teachers\n",
    "\n",
    "# Load the pre-trained KeyBERT model\n",
    "#model = KeyBERT(\"distilbert-base-nli-mean-tokens\")\n",
    "\n",
    "# Input text (use sections[0][1] as the content of the first section)\n",
    "#section_content = sections[0][1]\n",
    "\n",
    "# Extract keywords\n",
    "#try:\n",
    "#    keywords = model.extract_keywords(section_content, keyphrase_ngram_range=(1, 2), stop_words='english', use_mmr=True, top_n=10, resume_download=True)\n",
    "    \n",
    "    # Print the extracted keywords\n",
    "#    for keyword in keywords:\n",
    "#        print(keyword)\n",
    "#except Exception as e:\n",
    "#    print(\"An error occurred:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1: What is software testing?\n",
      "Question 2: What is testing is?\n",
      "Question 3: What is is a?\n",
      "Question 4: What is a set?\n",
      "Question 5: What is set of?\n",
      "Question 6: What is of activities?\n",
      "Question 7: What is activities to?\n",
      "Question 8: What is to discover?\n",
      "Question 9: What is discover defects?\n",
      "Question 10: What is defects and?\n"
     ]
    }
   ],
   "source": [
    "# Attempt of questions generation\n",
    "#Construct questions using the n-grams,\n",
    "#N-grams are continuous sequences of words or symbols or tokens in a document\n",
    "#and are defined as the neighboring sequences of items in a document.\n",
    "#https://www.scaler.com/topics/nlp/n-gram-model-in-nlp/\n",
    "\n",
    "\n",
    "# Tokenize the summary into sentences\n",
    "sentences = sent_tokenize(summary)  # section_content = sections[0][1]\n",
    "\n",
    "# Function to generate a fixed number of questions from sentences\n",
    "def generate_questions(text, num_questions=10):\n",
    "    questions = []\n",
    "    for sentence in text:\n",
    "        # Tokenize each sentence into words\n",
    "        words = nltk.word_tokenize(sentence)\n",
    "        # Generate n-grams (bi-grams) from the words\n",
    "        n_grams = list(ngrams(words, 2))\n",
    "\n",
    "        # Construct questions using the n-grams\n",
    "        for n_gram in n_grams:\n",
    "            question = f\"What is {n_gram[0]} {n_gram[1]}?\"\n",
    "            questions.append(question)\n",
    "            \n",
    "            # Stop generating questions if we reach the desired number\n",
    "            if len(questions) >= num_questions:\n",
    "                return questions\n",
    "\n",
    "    return questions[:num_questions]  # Return only the specified number of questions\n",
    "\n",
    "# Generate 10 questions from the sentences\n",
    "questions = generate_questions(sentences, num_questions=10)\n",
    "\n",
    "# Print the generated questions\n",
    "for i, question in enumerate(questions, start=1):\n",
    "    print(f\"Question {i}: {question}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here comes gradio + manual selection of correct questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained model and tokenizer\n",
    "#model_name = \"bert-large-uncased-whole-word-masking-finetuned-squad\"\n",
    "#tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "#model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "\n",
    "# Provide a passage and a question\n",
    "#passage = extracted_text\n",
    "#question = \"Which of the following statements describe a valid test objective?\"\n",
    "\n",
    "#Which of the following statements describe a valid test objective?\n",
    "#What does not work as expected?\n",
    "\n",
    "# Tokenize the passage and question\n",
    "#inputs = tokenizer(question, passage, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "# Get the answer from the model\n",
    "#start_scores, end_scores = model(**inputs, return_dict = False)\n",
    "#start_idx = torch.argmax(start_scores)\n",
    "#end_idx = torch.argmax(end_scores)\n",
    "\n",
    "# Decode the answer from the tokenized output\n",
    "#answer_tokens = inputs[\"input_ids\"][0][start_idx:end_idx + 1]\n",
    "#answer = tokenizer.decode(answer_tokens)\n",
    "\n",
    "#print(\"Answer:\", answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To process text and generate questions with answers, you can consider using pre-trained language models, such as GPT-3, GPT-4, BERT, T5, or similar models. Each of these models has its strengths and can be used for different aspects of question generation and answering:\n",
    "\n",
    "T5 (Text-to-Text Transfer Transformer): T5 is a versatile language model that can be fine-tuned for various natural language processing tasks, including question generation. You can fine-tune a pre-trained T5 model on your specific dataset to generate high-quality questions.\n",
    "\n",
    "GPT-3: OpenAI's GPT-3 is a powerful language model known for its natural language generation capabilities. You can prompt GPT-3 to generate questions based on your input text. It can produce contextually relevant questions, but it may require careful instruction and filtering of the generated output.\n",
    "\n",
    "BART (Bidirectional and Auto-Regressive Transformers): BART is another transformer-based model that can be fine-tuned for question generation tasks. It excels in text generation tasks and can produce coherent and meaningful questions.\n",
    "\n",
    "XLNet: XLNet is a transformer model that has achieved strong performance in various NLP tasks. It can be fine-tuned for question generation, and its bidirectional context modeling can lead to better question generation.\n",
    "\n",
    "BERT (Bidirectional Encoder Representations from Transformers): BERT can also be used for question generation by fine-tuning. While it was originally designed for understanding context, it can be adapted for question generation with appropriate training data.\n",
    "\n",
    "UniLM: UniLM is a model that can be used for various text generation tasks, including question generation. It combines unidirectional, bidirectional, and sequence-to-sequence learning, making it versatile for NLP tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "# from io import StringIO\n",
    "\n",
    "# # Load the pre-trained model and tokenizer\n",
    "# model_name = \"bert-large-uncased-whole-word-masking-finetuned-squad\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "\n",
    "# # Create a StringIO object with your text\n",
    "# text_io = StringIO()\n",
    "# text_io.write(\"Your text goes here.\")\n",
    "# text_io.seek(0)  # Reset the StringIO object to the beginning\n",
    "\n",
    "# # Read the text from the StringIO object and convert it to a regular string\n",
    "# text = text_io.read()\n",
    "\n",
    "# # Provide a question\n",
    "# question = \"What is the answer to my question?\"\n",
    "\n",
    "# # Tokenize the text and question\n",
    "# inputs = tokenizer(question, text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "# # Get the answer from the model\n",
    "# start_scores, end_scores = model(**inputs)\n",
    "# start_idx = torch.argmax(start_scores)\n",
    "# end_idx = torch.argmax(end_scores)\n",
    "\n",
    "# # Decode the answer from the tokenized output\n",
    "# answer_tokens = inputs[\"input_ids\"][0][start_idx:end_idx + 1]\n",
    "# answer = tokenizer.decode(answer_tokens)\n",
    "\n",
    "# print(\"Answer:\", answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Question Generation using Seq2Seq (T5)\n",
    "\n",
    "# Load the pre-trained Seq2Seq model for question generation\n",
    "# question_generation_model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "# question_generation_tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "# \n",
    "#ISTQB document (replace with your actual content)\n",
    "# istqb_document = \"\"\"\n",
    "# 1.1. What is Testing? \n",
    "# \n",
    "# Software systems are an integral part of our daily life. Most people have had experience with software \n",
    "# that did not work as expected. Software that does not work correctly can lead to many problems, \n",
    "# including loss of money, time or business reputation, and, in extreme cases, even injury or death. \n",
    "# Software testing assesses software quality and helps reducing the risk of software failure in operation. \n",
    "# \n",
    "# Software testing is a set of activities to discover defects and evaluate the quality of software artifacts. \n",
    "# These artifacts, when being tested, are known as test objects. A common misconception about testing is \n",
    "# that it only consists of executing tests (i.e., running the software and checking the test results). However, \n",
    "# software testing also includes other activities and must be aligned with the software development lifecycle \n",
    "# (see chapter 2). \n",
    "# \n",
    "# Another common misconception about testing is that testing focuses entirely on verifying the test object. \n",
    "# Whilst testing involves verification, i.e., checking whether the system meets specified requirements, it also \n",
    "# involves validation, which means checking whether the system meets users’ and other stakeholders’ \n",
    "# needs in its operational environment. \n",
    "# \"\"\"\n",
    "# \n",
    "#Generate questions from the ISTQB document\n",
    "# def generate_questions(document, max_length=64, num_questions=1):\n",
    "    # inputs = question_generation_tokenizer.encode(\"generate questions: \" + document, return_tensors=\"pt\", max_length=max_length, truncation=True)\n",
    "    # questions = question_generation_model.generate(inputs, max_length=max_length, num_return_sequences=num_questions)\n",
    "    # return [question_generation_tokenizer.decode(question, skip_special_tokens=True) for question in questions]\n",
    "# \n",
    "# generated_questions = generate_questions(istqb_document)\n",
    "# \n",
    "#Print generated questions\n",
    "# for question in generated_questions:\n",
    "    # print(\"Question:\", question)\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is template for Quize layout, needs to be re-worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'items'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Elena\\github\\Quiz_Generator_Markov_Chain\\extraction.ipynb Cell 40\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Elena/github/Quiz_Generator_Markov_Chain/extraction.ipynb#X66sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m section_titles \u001b[39m=\u001b[39m [section[\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m section \u001b[39min\u001b[39;00m sections]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Elena/github/Quiz_Generator_Markov_Chain/extraction.ipynb#X66sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39m# Create a Gradio interface with a dropdown list of section titles\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Elena/github/Quiz_Generator_Markov_Chain/extraction.ipynb#X66sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m iface \u001b[39m=\u001b[39m gr\u001b[39m.\u001b[39mInterface(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Elena/github/Quiz_Generator_Markov_Chain/extraction.ipynb#X66sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     fn\u001b[39m=\u001b[39mdisplay_sections,\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Elena/github/Quiz_Generator_Markov_Chain/extraction.ipynb#X66sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     inputs\u001b[39m=\u001b[39mgr\u001b[39m.\u001b[39minputs\u001b[39m.\u001b[39mDropdown(section\u001b[39m.\u001b[39mitems()),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Elena/github/Quiz_Generator_Markov_Chain/extraction.ipynb#X66sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     outputs\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Elena/github/Quiz_Generator_Markov_Chain/extraction.ipynb#X66sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     css\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.gradio-container \u001b[39m\u001b[39m{\u001b[39m\u001b[39mbackground-color: lightblue}\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Elena/github/Quiz_Generator_Markov_Chain/extraction.ipynb#X66sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Elena/github/Quiz_Generator_Markov_Chain/extraction.ipynb#X66sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m iface\u001b[39m.\u001b[39mlaunch(share\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'items'"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "# Replace this with your actual list of section titles and content\n",
    "sections = [\n",
    "    (\"1.1.1.\", \"Content of Section 1.1.1...\"),\n",
    "    (\"1.2.\", \"Content of Section 1.2...\"),\n",
    "    # Add more sections as needed\n",
    "]\n",
    "\n",
    "def display_sections(section):\n",
    "    # Find the selected section in the list based on the section title\n",
    "    selected_section = next((s for s in sections if s[0] == section), None)\n",
    "\n",
    "    if selected_section:\n",
    "        section_title, section_content = selected_section\n",
    "        return f\"Section Title: {section_title}\\nSection Content: {section_content}\"\n",
    "    else:\n",
    "        return \"Section not found\"\n",
    "\n",
    "# Get a list of section titles from the sections list\n",
    "section_titles = [section[0] for section in sections]\n",
    "\n",
    "# Create a Gradio interface with a dropdown list of section titles\n",
    "iface = gr.Interface(\n",
    "    fn=display_sections,\n",
    "    inputs=gr.inputs.Dropdown(section.items()),\n",
    "    outputs=\"text\",\n",
    "    css=\".gradio-container {background-color: lightblue}\"\n",
    ")\n",
    "\n",
    "iface.launch(share=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draft metrics\n",
    "\n",
    "import nltk\n",
    "\n",
    "# Reference questions (human-generated)\n",
    "reference_questions = [\n",
    "    \"What is the test objective?\",\n",
    "    \"How do objectives vary?\",\n",
    "    \"What does the context include?\",\n",
    "    # Add more reference questions here\n",
    "]\n",
    "\n",
    "# Automatically generated questions\n",
    "generated_questions = [\n",
    "    \"What is test objectives?\",\n",
    "    \"What is objectives vary?\",\n",
    "    \"How do objectives depend?\",\n",
    "    # Add more generated questions here\n",
    "]\n",
    "\n",
    "# Initialize the NLTK BLEU scorer\n",
    "bleu_scorer = nltk.translate.bleu_score.SmoothingFunction()\n",
    "\n",
    "# Calculate BLEU score (a measure of similarity)\n",
    "bleu_scores = [nltk.translate.bleu_score.sentence_bleu([r.split()], g.split(), smoothing_function=bleu_scorer.method1) for r, g in zip(reference_questions, generated_questions)]\n",
    "\n",
    "# Calculate accuracy rate (percentage of questions that match reference questions)\n",
    "accuracy_rate = sum(score == 1.0 for score in bleu_scores) / len(bleu_scores) * 100\n",
    "\n",
    "print(\"Accuracy Rate: {:.2f}%\".format(accuracy_rate))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qgmc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
