{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we are downloading PDF file, converting it to TXT and doing some \"pre-cleaning\": removing not meaningful parts of document and leaving just the most valuable leftovers for our future generator.\n",
    "THe outcome of the below code is pre-processed but still raw data.\n",
    "\n",
    "\n",
    "\"extracted_text\" variable has \"StringIO\" type: The StringIO object is part of Python's io module and is a class that provides an in-memory file-like object that can be used for reading from or writing to strings as if they were files. It allows you to treat strings as file-like objects, which can be useful in various situations, such as when you want to read from or write to a string in a way that mimics file operations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import of libraries\n",
    "from io import StringIO # extracted_text is the main variable, contains the whole text of document in stringIO format in memory\n",
    "import requests\n",
    "import re  # provides reg. exp. support\n",
    "import math\n",
    "\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfdocument import PDFDocument\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.pdfparser import PDFParser\n",
    "import fitz\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "import sentencepiece\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer, BertForQuestionAnswering, BertTokenizer\n",
    "from transformers import AutoTokenizer\n",
    "from keybert import KeyBERT\n",
    "import gradio # UI part for the quize\n",
    "import urllib3\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U pip setuptools wheel\n",
    "#!pip install -U spacy\n",
    "#!python -m spacy download en_core_web_sm\n",
    "#!pip install PyMuPDF # this is fitz\n",
    "#!pip install gradio\n",
    "#!pip install keybert\n",
    "#!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1113747"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# downloading pdf to '/data/' folder\n",
    "url = 'https://astqb.org/assets/documents/ISTQB_CTFL_Syllabus-v4.0.pdf'\n",
    "r = requests.get(url, allow_redirects=True)\n",
    "open('data/ISTQB_CTFL_Syllabus-v4.0.pdf', 'wb').write(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "r\"Page \\d{4,74} of 74\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted text for 'The Certified Tester Foundation Level in Software Testing' saved to 'data/ISTQB_CTFL_Syllabus-v4.0.txt'\n"
     ]
    }
   ],
   "source": [
    "#converting pdf to text and saving into .txt file initial version\n",
    "output_string = StringIO()\n",
    "output_file_path = 'data/ISTQB_CTFL_Syllabus-v4.0.txt'\n",
    "with open('data/ISTQB_CTFL_Syllabus-v4.0.pdf', 'rb') as in_file, open(output_file_path, 'w', encoding='utf-8') as out_file:\n",
    "    parser = PDFParser(in_file)\n",
    "    doc = PDFDocument(parser)\n",
    "    rsrcmgr = PDFResourceManager()\n",
    "    device = TextConverter(rsrcmgr, output_string, laparams=LAParams())\n",
    "    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "    for page in PDFPage.create_pages(doc):\n",
    "        interpreter.process_page(page)\n",
    "    # Getting the extracted text from StringIO, it means the entire text extracted from the PDF is stored as a single string in memory.\n",
    "    extracted_text = output_string.getvalue()\n",
    "    # Writing the extracted text to the output file\n",
    "    out_file.write(extracted_text)\n",
    "\n",
    "# Closing the stream\n",
    "output_string.close()\n",
    "\n",
    "\n",
    "# Printing message to indicate that the text has been saved to the file\n",
    "print(f\"Extracted text for 'The Certified Tester Foundation Level in Software Testing' saved to '{output_file_path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of string in bytes : 198489\n",
      "File size, document contains 70+ pages:  193.84 KB\n"
     ]
    }
   ],
   "source": [
    "# let us check size of StringIO on the full size of converted file, just out of curiosity\n",
    "size_bytes = len(extracted_text.encode('utf-8'))\n",
    "print ('The length of string in bytes : ' + str (size_bytes))\n",
    "\n",
    "# function's code is taken from stackoverflow ---\n",
    "def convert_size(size_bytes):\n",
    "   if size_bytes == 0:\n",
    "       return \"0B\"\n",
    "   size_name = (\"B\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\", \"EB\", \"ZB\", \"YB\")\n",
    "   i = int(math.floor(math.log(size_bytes, 1024)))\n",
    "   p = math.pow(1024, i)\n",
    "   s = round(size_bytes / p, 2)\n",
    "   return \"%s %s\" % (s, size_name[i])\n",
    "# ---\n",
    "print(\"File size, document contains 70+ pages: \", convert_size(size_bytes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted text for 'The Certified Tester Foundation Level in Software Testing' pre processed version 0.1 saved to 'data/ISTQB_CTFL_Syllabus-v4.0_v01.txt'\n"
     ]
    }
   ],
   "source": [
    "# Looking up for the text to remove everything before it\n",
    "target_text = \"1.1. What is Testing?\"\n",
    "\n",
    "# Finding the position of the target text in the extracted text\n",
    "start_position = extracted_text.find(target_text)\n",
    "\n",
    "# Checking if the target text was found, just in case\n",
    "if start_position != -1:\n",
    "    # Removing everything before the target text\n",
    "    extracted_text = extracted_text[start_position:]\n",
    "\n",
    "\n",
    "# let us save the content to .txt file with prefix '_v0.1' for further debugging purpose and human evaluation process\n",
    "\n",
    "output_file_path = 'data/ISTQB_CTFL_Syllabus-v4.0_v01.txt'\n",
    "with open(output_file_path, 'w', encoding='utf-8') as out_file:\n",
    "    # Writing the extracted text to the output file\n",
    "    out_file.write(extracted_text)\n",
    "\n",
    "# Closing the stream\n",
    "output_string.close()\n",
    "\n",
    "# Printing message to indicate that the text has been saved to the file\n",
    "print(f\"Extracted text for 'The Certified Tester Foundation Level in Software Testing' pre processed version 0.1 saved to '{output_file_path}'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing empty lines\n",
    "# _ - is iterator, if s.strip(): This part of the list comprehension checks whether the line s contains any non-whitespace characters. \n",
    "# If it does, the line is included in the resulting list.\n",
    "\n",
    "# extracted_text = \"\".join([_ for _ in extracted_text.strip().splitlines(True) if _.strip()])\n",
    "# \n",
    "# output_file_path = 'data/ISTQB_CTFL_Syllabus-v4.0_v02.txt'\n",
    "# with open('data/ISTQB_CTFL_Syllabus-v4.0_v01.txt', 'rb') as in_file, open(output_file_path, 'w', encoding='utf-8') as out_file:\n",
    " #   Writing the extracted text to the output file\n",
    "    # out_file.write(extracted_text)\n",
    "# \n",
    "#Closing the stream\n",
    "# output_string.close()\n",
    "# \n",
    "#Printing message to indicate that the text has been saved to the file\n",
    "# print(f\"Extracted text for 'The Certified Tester Foundation Level in Software Testing' pre processed version 0.2 saved to '{output_file_path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted text for 'The Certified Tester Foundation Level in Software Testing' pre processed version 0.1 saved to 'data/ISTQB_CTFL_Syllabus-v4.0_v03.txt'\n"
     ]
    }
   ],
   "source": [
    "# Removing text from 'Page 56 of 74' till the end of the text\n",
    "\n",
    "# Looking up for the text to remove everything after it\n",
    "target_text = \"Page 56 of 74\"\n",
    "\n",
    "# Finding the position of the target text in the extracted text\n",
    "end_position = extracted_text.find(target_text)\n",
    "\n",
    "# Checking if the target text was found, just in case\n",
    "if end_position != -1:\n",
    "    # Removing everything before the target text\n",
    "    extracted_text = extracted_text[:end_position]\n",
    "\n",
    "\n",
    "# let us save the content to .txt file with prefix '_v0.1' for further debugging purpose and human evaluation process\n",
    "\n",
    "output_file_path = 'data/ISTQB_CTFL_Syllabus-v4.0_v03.txt'\n",
    "with open(output_file_path, 'w', encoding='utf-8') as out_file:\n",
    "    # Writing the extracted text to the output file\n",
    "    out_file.write(extracted_text)\n",
    "\n",
    "# Closing the stream\n",
    "output_string.close()\n",
    "\n",
    "# Printing message to indicate that the text has been saved to the file\n",
    "print(f\"Extracted text for 'The Certified Tester Foundation Level in Software Testing' pre processed version 0.1 saved to '{output_file_path}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your stop words list\n",
    "# stop_words = [\"v4.0\", \"Page\", \"74\", \"18\", \"15\", \"of\", \"2023-04-21\", \"©\", \"Certified Tester\", \"Foundation\", \"Level\", \"International Software Testing Qualifications Board\"]\n",
    "# \n",
    "#Split the extracted_text into words\n",
    "# words = extracted_text.split()\n",
    "# \n",
    "#Filter out words that are in the stop words list\n",
    "# filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "# \n",
    "#Join the filtered words back into a text\n",
    "# extracted_text = \" \".join(filtered_words)\n",
    "# \n",
    "#Print the cleaned text\n",
    "#print(extracted_text)\n",
    "# \n",
    "# \n",
    "# output_file_path = 'data/ISTQB_CTFL_Syllabus-v4.0_v05.txt'\n",
    "# with open(output_file_path, 'w', encoding='utf-8') as out_file:\n",
    "#    Writing the extracted text to the output file\n",
    "    # out_file.write(extracted_text)\n",
    "# \n",
    "#Closing the stream\n",
    "# output_string.close()\n",
    "# \n",
    "#Printing message to indicate that the text has been saved to the file\n",
    "# print(f\"Extracted text for 'The Certified Tester Foundation Level in Software Testing' pre processed version 0.5 saved to '{output_file_path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to lower case all words in stringIO\n",
    "#extracted_text = extracted_text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#punctuation\n",
    "# Load the language model\n",
    "#nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Process the text with SpaCy\n",
    "###doc = nlp(extracted_text)\n",
    "\n",
    "# Create a list of tokens that are not punctuation\n",
    "#filtered_tokens = [token.text for token in doc if not token.is_punct]\n",
    "\n",
    "# Join the filtered tokens back into a text\n",
    "#extracted_text = \" \".join(filtered_tokens)\n",
    "\n",
    "# Print the text without punctuation\n",
    "#print(extracted_text)\n",
    "\n",
    "#output_file_path = 'data/ISTQB_CTFL_Syllabus-v4.0_v04.txt'\n",
    "#with open(output_file_path, 'w', encoding='utf-8') as out_file:\n",
    "    # Writing the extracted text to the output file\n",
    "#    out_file.write(extracted_text)\n",
    "\n",
    "# Closing the stream\n",
    "#output_string.close()\n",
    "\n",
    "# Printing message to indicate that the text has been saved to the file\n",
    "#print(f\"Extracted text for 'The Certified Tester Foundation Level in Software Testing' pre processed version 0.1 saved to '{output_file_path}'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CHeck results, looks like some parts are not removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted text for 'The Certified Tester Foundation Level in Software Testing' pre processed version 0.5 saved to 'data/ISTQB_CTFL_Syllabus-v4.0_v05.txt'\n"
     ]
    }
   ],
   "source": [
    "# built by chatgpt on provided context from my side, I used a part of text of file above, reviewed and customized by me as well\n",
    "#stop_words = [\"buxton\", \"a\", \"about\", \"above\", \"additional\", \"an\", \"and\", \"another\", \"are\", \"as\", \"be\", \"being\", \"by\", \"can\", \"common\", \"commonly\", \"do\", \"does\", \"each\", \"even\", \"for\", \"from\", \"has\", \"have\", \"in\", \"including\", \"is\", \"it\", \"its\", \"it's\", \"many\", \"may\", \"more\", \"most\", \"not\", \"of\", \"74\", \"often\", \"on\", \"or\", \"over\", \"such\", \"than\", \"that\", \"the\", \"there\", \"these\", \"this\", \"to\", \"under\", \"was\", \"we\", \"what\", \"when\", \"which\", \"who\", \"why\", \"will\", \"with\", \"within\", \"work\", \"you\", \"2023\", \"04\", \"21\", \"v4.0\", \"page\", \"2023-04-21\", \"©\", \"international\", \"qualifications\", \"board\", \"certified\", \"tester\",  \"foundation\", \"level\", \"FL-\", \"K2\", \"see\", \"section\" , \"didn't\", \"doesn't\", \"don't\", \"i.e.\", \"it's\", \"let's\", \"that's\", \"there's\", \"they're\", \"you're\", \"e.g.\"]\n",
    "stop_words = [ \"©\", \"15\", \"16\", \"17\", \"18\", \"19\", \"20\", r\"\\b20\\b\", \"21\", \"22\", \"23\", \"24\", \"25\", \"26\", \"27\", \"28\", \"29\", \"30\", \"31\", \"32\", \"33\", \"34\", \"35\", \"36\",\n",
    "              \"37\", \"38\",\"39\", \"40\",\"41\", \"42\",\"43\", \"44\",\"45\", \"46\", \"47\", \"48\", \"49\", \"50\", \"51\", \"52\", \"53\", \"54\", \n",
    "              \"International Software Testing Qualifications Board Certified Tester Foundation Level\", \"21.04.2023\", \"01.07.2021\",\n",
    "              \"11.11.2019\", \"27.04.2018\", \"1.04.2011\", \"30.03.2010\", \"01.05.2007\", \"01.07.2005\", \"25.02.1999\"]\n",
    "# \n",
    "# Your stop words list\n",
    "#stop_words = [\"v4.0\", \"page\", \"of\", \"2023-04-21\", \"©\", \"International Software Testing Qualifications Board\",\n",
    " #             \"Certified\", \"Tester\", \"Foundation\", \"Level\"]\n",
    "# \n",
    "# Regular expression pattern to match phrases like \"15 74\", \"16 74\", ..., \"54 74\"\n",
    "pattern = re.compile(r\"(?s)^v4.0.*Foundation Level$\", re.DOTALL)\n",
    "# \n",
    "# Split the extracted_text into words\n",
    "words = re.split(r'\\s+', extracted_text)\n",
    "# \n",
    "# Filter out words that match the regular expression pattern or are in the stop words list\n",
    "filtered_words = [word for word in words if not re.match(pattern, word) and word.lower() not in stop_words]\n",
    "\n",
    "# \n",
    "# Join the filtered words back into a text\n",
    "extracted_text = \" \".join(filtered_words)\n",
    "\n",
    "# Define the phrase you want to remove\n",
    "phrase_to_remove = \"International Software Testing Qualifications Board Certified Tester Foundation Level\"\n",
    "\n",
    "# Replace the phrase with an empty string\n",
    "extracted_text = extracted_text.replace(phrase_to_remove, \"\")\n",
    "\n",
    "# \n",
    "output_file_path = 'data/ISTQB_CTFL_Syllabus-v4.0_v05.txt'\n",
    "with open(output_file_path, 'w', encoding='utf-8') as out_file:\n",
    "    # Writing the extracted text to the output file\n",
    "    out_file.write(extracted_text)\n",
    "# \n",
    "# Closing the stream\n",
    "output_string.close()\n",
    "# \n",
    "# Printing message to indicate that the text has been saved to the file\n",
    "print(f\"Extracted text for 'The Certified Tester Foundation Level in Software Testing' pre processed version 0.5 saved to '{output_file_path}'\")\n",
    "# \n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pattern = r'[0-9]'\n",
    "\n",
    "# Match all digits in the string and replace them with an empty string\n",
    "#extracted_text = re.sub(pattern, '', extracted_text)\n",
    "\n",
    "#extracted_text = ''.join((x for x in extracted_text if not x.isdigit()))\n",
    "\n",
    "\n",
    "#output_file_path = 'data/ISTQB_CTFL_Syllabus-v4.0_v06.txt'\n",
    "#with open(output_file_path, 'w', encoding='utf-8') as out_file:\n",
    "    # Writing the extracted text to the output file\n",
    "#    out_file.write(extracted_text)\n",
    "# \n",
    "# Closing the stream\n",
    "#output_string.close()\n",
    "# \n",
    "# Printing message to indicate that the text has been saved to the file\n",
    "#print(f\"Extracted text for 'The Certified Tester Foundation Level in Software Testing' pre processed version 0.6 saved to '{output_file_path}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# # Load the language model\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# # Your text\n",
    "# text = extracted_text\n",
    "\n",
    "# # Process the text with SpaCy\n",
    "# doc = nlp(text)\n",
    "\n",
    "# # Create a StringIO object to store the NER results\n",
    "# output_string = io.StringIO()\n",
    "\n",
    "# # Extract named entities and write them to the StringIO object\n",
    "# for ent in doc.ents:\n",
    "#     output_string.write(f\"Entity: {ent.text}, Type: {ent.label_}\\n\")\n",
    "\n",
    "# # Get the NER results as a string\n",
    "# ner_results = output_string.getvalue()\n",
    "\n",
    "# output_file_path = 'data/NER.txt'\n",
    "# with open(output_file_path, 'w', encoding='utf-8') as out_file:\n",
    "#     # Writing the extracted text to the output file\n",
    "#       out_file.write(ner_results)\n",
    "\n",
    "# # Closing the stream\n",
    "# output_string.close()\n",
    "\n",
    "# # Printing message to indicate that the text has been saved to the file\n",
    "# print(f\"Extracted NER list for 'The Certified Tester Foundation Level in Software Testing; {output_file_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point NER dict is saved into /data folder, edited manually and now let us import this file into stop_list StringIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Check the content of stop_list_stringio\n",
    "#content = stop_list_stringio.getvalue()\n",
    "#print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Markov Chain\n",
    "# Sample text (replace with your extracted_text)\n",
    "# Tokenize the text into words\n",
    "#tokens = nltk.word_tokenize(extracted_text)\n",
    "\n",
    "# Create a dictionary to store transition probabilities\n",
    "#transition_probabilities = {}\n",
    "\n",
    "# Build the transition probability matrix\n",
    "#for i in range(len(tokens) - 1):\n",
    "#    current_token = tokens[i]\n",
    "#    next_token = tokens[i + 1]\n",
    "    \n",
    "#    if current_token in transition_probabilities:\n",
    "#        transition_probabilities[current_token].append(next_token)\n",
    "#    else:\n",
    "#        transition_probabilities[current_token] = [next_token]\n",
    "\n",
    "# Start with an initial word\n",
    "#current_word = random.choice(tokens)\n",
    "\n",
    "# Generate a sentence of a certain length\n",
    "#generated_text = [current_word]\n",
    "#sentence_length = 10\n",
    "\n",
    "#for _ in range(sentence_length - 1):\n",
    "#    if current_word in transition_probabilities:\n",
    "#        next_word = random.choice(transition_probabilities[current_word])\n",
    "#        generated_text.append(next_word)\n",
    "#        current_word = next_word\n",
    "#    else:\n",
    "#        break\n",
    "\n",
    "# Join the generated words into a sentence\n",
    "#generated_sentence = \" \".join(generated_text)\n",
    "#print(generated_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted text for 'The Certified Tester Foundation Level in Software Testing' pre processed version 0.7 saved to 'data/ISTQB_CTFL_Syllabus-v4.0_v07.txt'\n"
     ]
    }
   ],
   "source": [
    "# remove chapter 4 beginning\n",
    "\n",
    "# Define the regular expression pattern for the text to remove\n",
    "pattern = r'4\\. Test Analysis and Design – 390 minutes.*?(K3) Use acceptance test-driven development (ATDD) to derive test cases'\n",
    "\n",
    "# Use re.sub to replace the matched text with a marker (e.g., 'REMOVED')\n",
    "extracted_text = re.sub(pattern, \"\", extracted_text, flags=re.DOTALL)\n",
    "\n",
    "# Define the phrase you want to remove\n",
    "phrase_to_remove = \"Learning Objectives for Chapter 4:\"\n",
    "\n",
    "# Replace the phrase with an empty string\n",
    "extracted_text = extracted_text.replace(phrase_to_remove, \"\")\n",
    "# \n",
    "output_file_path = 'data/ISTQB_CTFL_Syllabus-v4.0_v07.txt'\n",
    "with open(output_file_path, 'w', encoding='utf-8') as out_file:\n",
    "    # Writing the extracted text to the output file\n",
    "    out_file.write(extracted_text)\n",
    "# \n",
    "# Closing the stream\n",
    "output_string.close()\n",
    "# \n",
    "# Printing message to indicate that the text has been saved to the file\n",
    "print(f\"Extracted text for 'The Certified Tester Foundation Level in Software Testing' pre processed version 0.7 saved to '{output_file_path}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted text for 'The Certified Tester Foundation Level in Software Testing' pre processed version 0.8 saved to 'data/ISTQB_CTFL_Syllabus-v4.0_v08.txt'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# remove chapter 4 beginning\n",
    "# Define the regular expression pattern to remove the desired text\n",
    "pattern = r'4\\.1 Test Techniques Overview.*?4\\.5\\.3 \\(K3\\) Use acceptance test-driven development \\(ATDD\\) to derive test cases'\n",
    "\n",
    "# Use re.sub to replace the matched text with an empty string\n",
    "extracted_text = re.sub(pattern, '', extracted_text, flags=re.DOTALL)\n",
    "\n",
    "\n",
    "\n",
    "output_file_path = 'data/ISTQB_CTFL_Syllabus-v4.0_v08.txt'\n",
    "with open(output_file_path, 'w', encoding='utf-8') as out_file:\n",
    "    # Writing the extracted text to the output file\n",
    "    out_file.write(extracted_text)\n",
    "# \n",
    "# Closing the stream\n",
    "output_string.close()\n",
    "# \n",
    "# Printing message to indicate that the text has been saved to the file\n",
    "print(f\"Extracted text for 'The Certified Tester Foundation Level in Software Testing' pre processed version 0.8 saved to '{output_file_path}'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted text for 'The Certified Tester Foundation Level in Software Testing' pre processed version 0.9 saved to 'data/ISTQB_CTFL_Syllabus-v4.0_v09.txt'\n"
     ]
    }
   ],
   "source": [
    "# remove chapter 3 beginning\n",
    "# Define the regular expression pattern for the text to remove\n",
    "pattern = r'3\\. Static Testing – 80 minutes.*?FL-3\\.2\\.5 \\(K1\\) Recall the factors that contribute to a successful review'\n",
    "\n",
    "# Use re.sub to replace the matched text with a marker (e.g., 'REMOVED')\n",
    "extracted_text = re.sub(pattern, \"\", extracted_text, flags=re.DOTALL)\n",
    "\n",
    "# Define the phrase you want to remove\n",
    "phrase_to_remove = \"Learning Objectives for Chapter 4:\"\n",
    "\n",
    "# Replace the phrase with an empty string\n",
    "extracted_text = extracted_text.replace(phrase_to_remove, \"\")\n",
    "# \n",
    "output_file_path = 'data/ISTQB_CTFL_Syllabus-v4.0_v09.txt'\n",
    "with open(output_file_path, 'w', encoding='utf-8') as out_file:\n",
    "    # Writing the extracted text to the output file\n",
    "    out_file.write(extracted_text)\n",
    "# \n",
    "# Closing the stream\n",
    "output_string.close()\n",
    "# \n",
    "# Printing message to indicate that the text has been saved to the file\n",
    "print(f\"Extracted text for 'The Certified Tester Foundation Level in Software Testing' pre processed version 0.9 saved to '{output_file_path}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted text for 'The Certified Tester Foundation Level in Software Testing' pre processed version 0.10 saved to 'data/ISTQB_CTFL_Syllabus-v4.0_v10.txt'\n"
     ]
    }
   ],
   "source": [
    "# remove chapter 2 beginning\n",
    "\n",
    "# Define the regular expression pattern for the text to remove\n",
    "pattern = r'2\\. Testing Throughout the Software Development Lifecycle.*?FL-2\\.3\\.1 \\(K2\\) Summarize maintenance testing and its triggers'\n",
    "# Use re.sub to replace the matched text with a marker (e.g., 'REMOVED')\n",
    "extracted_text = re.sub(pattern, \"\", extracted_text, flags=re.DOTALL)\n",
    "\n",
    "# Define the phrase you want to remove\n",
    "phrase_to_remove = \"Learning Objectives for Chapter 4:\"\n",
    "\n",
    "# Replace the phrase with an empty string\n",
    "extracted_text = extracted_text.replace(phrase_to_remove, \"\")\n",
    "# \n",
    "output_file_path = 'data/ISTQB_CTFL_Syllabus-v4.0_v10.txt'\n",
    "with open(output_file_path, 'w', encoding='utf-8') as out_file:\n",
    "    # Writing the extracted text to the output file\n",
    "    out_file.write(extracted_text)\n",
    "# \n",
    "# Closing the stream\n",
    "output_string.close()\n",
    "# \n",
    "# Printing message to indicate that the text has been saved to the file\n",
    "print(f\"Extracted text for 'The Certified Tester Foundation Level in Software Testing' pre processed version 0.10 saved to '{output_file_path}'\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted text for 'The Certified Tester Foundation Level in Software Testing' pre processed version 0.11 saved to 'data/ISTQB_CTFL_Syllabus-v4.0_v11.txt'\n"
     ]
    }
   ],
   "source": [
    "# remove chapter 2 beginning\n",
    "\n",
    "# Define the regular expression pattern for the text to remove\n",
    "\n",
    "pattern = r'5\\. Managing the Test Activities – 335 minutes.*?FL-5\\.5\\.1 \\(K3\\) Prepare a defect report'\n",
    "# Use re.sub to replace the matched text with a marker (e.g., 'REMOVED')\n",
    "extracted_text = re.sub(pattern, \"\", extracted_text, flags=re.DOTALL)\n",
    "\n",
    "# Define the phrase you want to remove\n",
    "phrase_to_remove = \"Learning Objectives for Chapter 4:\"\n",
    "\n",
    "# Replace the phrase with an empty string\n",
    "extracted_text = extracted_text.replace(phrase_to_remove, \"\")\n",
    "# \n",
    "output_file_path = 'data/ISTQB_CTFL_Syllabus-v4.0_v11.txt'\n",
    "with open(output_file_path, 'w', encoding='utf-8') as out_file:\n",
    "    # Writing the extracted text to the output file\n",
    "    out_file.write(extracted_text)\n",
    "# \n",
    "# Closing the stream\n",
    "output_string.close()\n",
    "# \n",
    "# Printing message to indicate that the text has been saved to the file\n",
    "print(f\"Extracted text for 'The Certified Tester Foundation Level in Software Testing' pre processed version 0.11 saved to '{output_file_path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "pattern = r'\\d+\\.\\d+\\.\\d+\\.'\n",
    "matches = re.findall(pattern, extracted_text)\n",
    "\n",
    "for match in matches:\n",
    "    match_without_dot = match[:-1]  # Remove the last dot\n",
    "    print(match_without_dot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted text for 'The Certified Tester Foundation Level in Software Testing' pre processed version 0.12 saved to 'data/ISTQB_CTFL_Syllabus-v4.0_v12.txt'\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Summary text with bullet points\n",
    "#summary = \"The typical test objectives are: • Evaluating work products such as requirements, user stories, designs, and code • Triggering failures and finding defects • Ensuring required coverage of a test object • Verifying that a test object complies with contractual, legal, and regulatory requirements • Providing information to stakeholders to allow them to make informed decisions • Building confidence in the quality of the test object • Validating whether the test object is complete and works as expected by the stakeholders\"\n",
    "\n",
    "# Remove bullet points using regular expressions\n",
    "extracted_text = re.sub(r'•', '', extracted_text)\n",
    "\n",
    "output_file_path = 'data/ISTQB_CTFL_Syllabus-v4.0_v12.txt'\n",
    "with open(output_file_path, 'w', encoding='utf-8') as out_file:\n",
    "    # Writing the extracted text to the output file\n",
    "    out_file.write(extracted_text)\n",
    "# \n",
    "# Closing the stream\n",
    "output_string.close()\n",
    "# \n",
    "# Printing message to indicate that the text has been saved to the file\n",
    "print(f\"Extracted text for 'The Certified Tester Foundation Level in Software Testing' pre processed version 0.12 saved to '{output_file_path}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Section Title: 1.1.1\n",
      "Section Content: Test Objectives The typical test objectives are:  Evaluating work products such as requirements, user stories, designs, and code  Triggering failures and finding defects  Ensuring required coverage of a test object  Reducing the level of risk of inadequate software quality  Verifying whether specified requirements have been fulfilled  Verifying that a test object complies with contractual, legal, and regulatory requirements  Providing information to stakeholders to allow them to make informed decisions  Building confidence in the quality of the test object  Validating whether the test object is complete and works as expected by the stakeholders Objectives of testing can vary, depending upon the context, which includes the work product being tested, the test level, risks, the software development lifecycle (SDLC) being followed, and factors related to the business context, e.g., corporate structure, competitive considerations, or time to market. v4.0 Page of 74 2023-04-21\n",
      "----------------------------------------\n",
      "Section Title: 1.2.1\n",
      "Section Content: Testing’s Contributions to Success Testing provides a cost-effective means of detecting defects. These defects can then be removed (by debugging – a non-testing activity), so testing indirectly contributes to higher quality test objects. Testing provides a means of directly evaluating the quality of a test object at various stages in the SDLC. These measures are used as part of a larger project management activity, contributing to decisions to move to the next stage of the SDLC, such as the release decision. Testing provides users with indirect representation on the development project. Testers ensure that their understanding of users’ needs are considered throughout the development lifecycle. The alternative is to involve a representative set of users as part of the development project, which is not usually possible due to the high costs and lack of availability of suitable users. Testing may also be required to meet contractual or legal requirements, or to comply with regulatory standards.\n",
      "----------------------------------------\n",
      "Section Title: 1.2.3\n",
      "Section Content: Errors, Defects, Failures, and Root Causes Human beings make errors (mistakes), which produce defects (faults, bugs), which in turn may result in failures. Humans make errors for various reasons, such as time pressure, complexity of work products, processes, infrastructure or interactions, or simply because they are tired or lack adequate training. Defects can be found in documentation, such as a requirements specification or a test script, in source code, or in a supporting artifact such as a build file. Defects in artifacts produced earlier in the SDLC, if undetected, often lead to defective artifacts later in the lifecycle. If a defect in code is executed, the system may fail to do what it should do, or do something it shouldn’t, causing a failure. Some defects will always result in a failure if executed, while others will only result in a failure in specific circumstances, and some may never result in a failure. Errors and defects are not the only cause of failures. Failures can also be caused by environmental conditions, such as when radiation or electromagnetic field cause defects in firmware. A root cause is a fundamental reason for the occurrence of a problem (e.g., a situation that leads to an error). Root causes are identified through root cause analysis, which is typically performed when a failure occurs or a defect is identified. It is believed that further similar failures or defects can be prevented or their frequency reduced by addressing the root cause, such as by removing it. 1.3. Testing Principles A number of testing principles offering general guidelines applicable to all testing have been suggested over the years. This syllabus describes seven such principles. 1. Testing shows the presence, not the absence of defects. Testing can show that defects are present in the test object, but cannot prove that there are no defects (Buxton 1970). Testing reduces the probability of defects remaining undiscovered in the test object, but even if no defects are found, testing cannot prove test object correctness. 2. Exhaustive testing is impossible. Testing everything is not feasible except in trivial cases (Manna 1978). Rather than attempting to test exhaustively, test techniques (see chapter 4), test case prioritization (see section 5.1.5), and risk-based testing (see section 5.2), should be used to focus test efforts. 3. Early testing saves time and money. Defects that are removed early in the process will not cause subsequent defects in derived work products. The cost of quality will be reduced since fewer failures will occur later in the SDLC (Boehm 1981). To find defects early, both static testing (see chapter 3) and dynamic testing (see chapter 4) should be started as early as possible. 4. Defects cluster together. A small number of system components usually contain most of the defects discovered or are responsible for most of the operational failures (Enders 1975). This phenomenon is an v4.0 Page of 74 2023-04-21 illustration of the Pareto principle. Predicted defect clusters, and actual defect clusters observed during testing or in operation, are an important input for risk-based testing (see section 5.2). 5. Tests wear out. If the same tests are repeated many times, they become increasingly ineffective in detecting new defects (Beizer 1990). To overcome this effect, existing tests and test data may need to be modified, and new tests may need to be written. However, in some cases, repeating the same tests can have a beneficial outcome, e.g., in automated regression testing (see section 2.2.3). 6. Testing is context dependent. There is no single universally applicable approach to testing. Testing is done differently in different contexts (Kaner 2011). 7. Absence-of-defects fallacy. It is a fallacy (i.e., a misconception) to expect that software verification will ensure the success of a system. Thoroughly testing all the specified requirements and fixing all the defects found could still produce a system that does not fulfill the users’ needs and expectations, that does not help in achieving the customer’s business goals, and that is inferior compared to other competing systems. In addition to verification, validation should also be carried out (Boehm 1981). 1.4. Test Activities, Testware and Test Roles Testing is context dependent, but, at a high level, there are common sets of test activities without which testing is less likely to achieve test objectives. These sets of test activities form a test process. The test process can be tailored to a given situation based on various factors. Which test activities are included in this test process, how they are implemented, and when they occur is normally decided as part of the test planning for the specific situation (see section 5.1). The following sections describe the general aspects of this test process in terms of test activities and tasks, the impact of context, testware, traceability between the test basis and testware, and testing roles. The ISO/IEC/IEEE 29119-2 standard provides further information about test processes.\n",
      "----------------------------------------\n",
      "Section Title: 1.4.2\n",
      "Section Content: Test Process in Context Testing is not performed in isolation. Test activities are an integral part of the development processes carried out within an organization. Testing is also funded by stakeholders and its final goal is to help fulfill the stakeholders’ business needs. Therefore, the way the testing is carried out will depend on a number of contextual factors including:  Stakeholders (needs, expectations, requirements, willingness to cooperate, etc.)  Team members (skills, knowledge, level of experience, availability, training needs, etc.)  Business domain (criticality of the test object, identified risks, market needs, specific legal regulations, etc.)  Technical factors (type of software, product architecture, technology used, etc.)  Project constraints (scope, time, budget, resources, etc.)  Organizational factors (organizational structure, existing policies, practices used, etc.)  Software development lifecycle (engineering practices, development methods, etc.)  Tools (availability, usability, compliance, etc.) These factors will have an impact on many test-related issues, including: test strategy, test techniques used, degree of test automation, required level of coverage, level of detail of test documentation, reporting, etc.\n",
      "----------------------------------------\n",
      "Section Title: 1.4.1\n",
      "Section Content: There is a significant variation in how different organizations produce, shape, name, organize and manage their v4.0 Page of 74 2023-04-21 work products. Proper configuration management (see section 5.4) ensures consistency and integrity of work products. The following list of work products is not exhaustive:  Test planning work products include: test plan, test schedule, risk register, and entry and exit criteria (see section 5.1). Risk register is a list of risks together with risk likelihood, risk impact and information about risk mitigation (see section 5.2). Test schedule, risk register and entry and exit criteria are often a part of the test plan.  Test monitoring and control work products include: test progress reports (see section 5.3.2), documentation of control directives (see section 5.3) and risk information (see section 5.2).  Test analysis work products include: (prioritized) test conditions (e.g., acceptance criteria, see section 4.5.2), and defect reports regarding defects in the test basis (if not fixed directly).  Test design work products include: (prioritized) test cases, test charters, coverage items, test data requirements and test environment requirements.  Test implementation work products include: test procedures, automated test scripts, test suites, test data, test execution schedule, and test environment elements. Examples of test environment elements include: stubs, drivers, simulators, and service virtualizations.  Test execution work products include: test logs, and defect reports (see section 5.5).  Test completion work products include: test completion report (see section 5.3.2), action items for improvement of subsequent projects or iterations, documented lessons learned, and change requests (e.g., as product backlog items).\n",
      "----------------------------------------\n",
      "Section Title: 1.4.5\n",
      "Section Content: Roles in Testing In this syllabus, two principal roles in testing are covered: a test management role and a testing role. The activities and tasks assigned to these two roles depend on factors such as the project and product context, the skills of the people in the roles, and the organization. The test management role takes overall responsibility for the test process, test team and leadership of the test activities. The test management role is mainly focused on the activities of test planning, test monitoring and control and test completion. The way in which the test management role is carried out varies depending on the context. For example, in Agile software development, some of the test management tasks may be handled by the Agile team. Tasks that span multiple teams or the entire organization may be performed by test managers outside of the development team. The testing role takes overall responsibility for the engineering (technical) aspect of testing. The testing role is mainly focused on the activities of test analysis, test design, test implementation and test execution. Different people may take on these roles at different times. For example, the test management role can be performed by a team leader, by a test manager, by a development manager, etc. It is also possible for one person to take on the roles of testing and test management at the same time. 1.5. Essential Skills and Good Practices in Testing Skill is the ability to do something well that comes from one’s knowledge, practice and aptitude. Good testers should possess some essential skills to do their job well. Good testers should be effective team players and should be able to perform testing on different levels of test independence.\n",
      "----------------------------------------\n",
      "Section Title: 1.5.2\n",
      "Section Content: Whole Team Approach One of the important skills for a tester is the ability to work effectively in a team context and to contribute positively to the team goals. The whole team approach – a practice coming from Extreme Programming (see section 2.1) – builds upon this skill. In the whole-team approach any team member with the necessary knowledge and skills can perform any task, and everyone is responsible for quality. The team members share the same workspace (physical or virtual), as co-location facilitates communication and interaction. The whole team approach improves team dynamics, enhances communication and collaboration within the team, and creates synergy by allowing the various skill sets within the team to be leveraged for the benefit of the project. Testers work closely with other team members to ensure that the desired quality levels are achieved. This includes collaborating with business representatives to help them create suitable acceptance tests and working with developers to agree on the test strategy and decide on test automation approaches. Testers can thus transfer testing knowledge to other team members and influence the development of the product. Depending on the context, the whole team approach may not always be appropriate. For instance, in some situations, such as safety-critical, a high level of test independence may be needed.\n",
      "----------------------------------------\n",
      "Section Title: 2.1.1\n",
      "Section Content: Impact of the Software Development Lifecycle on Testing Testing must be adapted to the SDLC to succeed. The choice of the SDLC impacts on the:  Scope and timing of test activities (e.g., test levels and test types)  Level of detail of test documentation  Choice of test techniques and test approach  Extent of test automation  Role and responsibilities of a tester In sequential development models, in the initial phases testers typically participate in requirement reviews, test analysis, and test design. The executable code is usually created in the later phases, so typically dynamic testing cannot be performed early in the SDLC. In some iterative and incremental development models, it is assumed that each iteration delivers a working prototype or product increment. This implies that in each iteration both static and dynamic testing may be performed at all test levels. Frequent delivery of increments requires fast feedback and extensive regression testing. Agile software development assumes that change may occur throughout the project. Therefore, lightweight work product documentation and extensive test automation to make regression testing easier are favored in agile projects. Also, most of the manual testing tends to be done using experience-based test techniques (see Section 4.4) that do not require extensive prior test analysis and design.\n",
      "----------------------------------------\n",
      "Section Title: 2.1.3\n",
      "Section Content: Testing as a Driver for Software Development TDD, ATDD and BDD are similar development approaches, where tests are defined as a means of directing development. Each of these approaches implements the principle of early testing (see section 1.3) and follows a shift-left approach (see section 2.1.5), since the tests are defined before the code is written. They support an iterative development model. These approaches are characterized as follows: Test-Driven Development (TDD):  Directs the coding through test cases (instead of extensive software design) (Beck 2003)  Tests are written first, then the code is written to satisfy the tests, and then the tests and code are refactored Acceptance Test-Driven Development (ATDD) (see section 4.5.3):  Derives tests from acceptance criteria as part of the system design process (Gärtner 2011)  Tests are written before the part of the application is developed to satisfy the tests Behavior-Driven Development (BDD):  Expresses the desired behavior of an application with test cases written in a simple form of natural language, which is easy to understand by stakeholders – usually using the Given/When/Then format. (Chelimsky 2010)  Test cases are then automatically translated into executable tests For all the above approaches, tests may persist as automated tests to ensure the code quality in future adaptions / refactoring.\n",
      "----------------------------------------\n",
      "Section Title: 2.1.5\n",
      "Section Content: Shift-Left Approach The principle of early testing (see section 1.3) is sometimes referred to as shift-left because it is an approach where testing is performed earlier in the SDLC. Shift-left normally suggests that testing should be done earlier (e.g., not waiting for code to be implemented or for components to be integrated), but it does not mean that testing later in the SDLC should be neglected. There are some good practices that illustrate how to achieve a “shift-left” in testing, which include:  Reviewing the specification from the perspective of testing. These review activities on specifications often find potential defects, such as ambiguities, incompleteness, and inconsistencies  Writing test cases before the code is written and have the code run in a test harness during code implementation  Using CI and even better CD as it comes with fast feedback and automated component tests to accompany source code when it is submitted to the code repository  Completing static analysis of source code prior to dynamic testing, or as part of an automated process  Performing non-functional testing starting at the component test level, where possible. This is a form of shift-left as these non-functional test types tend to be performed later in the SDLC when a complete system and a representative test environment are available A shift-left approach might result in extra training, effort and/or costs earlier in the process but is expected to save efforts and/or costs later in the process. For the shift-left approach it is important that stakeholders are convinced and bought into this concept.\n",
      "----------------------------------------\n",
      "Section Title: 2.2.1\n",
      "Section Content: Test Levels In this syllabus, the following five test levels are described:  Component testing (also known as unit testing) focuses on testing components in isolation. It often requires specific support, such as test harnesses or unit test frameworks. Component testing is normally performed by developers in their development environments.  Component integration testing (also known as unit integration testing) focuses on testing the interfaces and interactions between components. Component integration testing is heavily dependent on the integration strategy approaches like bottom-up, top-down or big-bang.  System testing focuses on the overall behavior and capabilities of an entire system or product, often including functional testing of end-to-end tasks and the non-functional testing of quality characteristics. For some non-functional quality characteristics, it is preferable to test them on a complete system in a representative test environment (e.g., usability). Using simulations of sub- v4.0 Page of 74 2023-04-21 systems is also possible. System testing may be performed by an independent test team, and is related to specifications for the system.  System integration testing focuses on testing the interfaces of the system under test and other systems and external services . System integration testing requires suitable test environments preferably similar to the operational environment.  Acceptance testing focuses on validation and on demonstrating readiness for deployment, which means that the system fulfills the user’s business needs. Ideally, acceptance testing should be performed by the intended users. The main forms of acceptance testing are: user acceptance testing (UAT), operational acceptance testing, contractual and regulatory acceptance testing, alpha testing and beta testing. Test levels are distinguished by the following non-exhaustive list of attributes, to avoid overlapping of test activities:  Test object  Test objectives  Test basis  Defects and failures  Approach and responsibilities\n",
      "----------------------------------------\n",
      "Section Title: 2.2.3\n",
      "Section Content: Confirmation Testing and Regression Testing Changes are typically made to a component or system to either enhance it by adding a new feature or to fix it by removing a defect. Testing should then also include confirmation testing and regression testing. Confirmation testing confirms that an original defect has been successfully fixed. Depending on the risk, one can test the fixed version of the software in several ways, including:  executing all test cases that previously have failed due to the defect, or, also by  adding new tests to cover any changes that were needed to fix the defect However, when time or money is short when fixing defects, confirmation testing might be restricted to simply exercising the steps that should reproduce the failure caused by the defect and checking that the failure does not occur. Regression testing confirms that no adverse consequences have been caused by a change, including a fix that has already been confirmation tested. These adverse consequences could affect the same component where the change was made, other components in the same system, or even other connected systems. Regression testing may not be restricted to the test object itself but can also be related to the environment. It is advisable first to perform an impact analysis to optimize the extent of the regression testing. Impact analysis shows which parts of the software could be affected. Regression test suites are run many times and generally the number of regression test cases will increase with each iteration or release, so regression testing is a strong candidate for automation. Automation of these tests should start early in the project. Where CI is used, such as in DevOps (see section 2.1.4), it is good practice to also include automated regression tests. Depending on the situation, this may include regression tests on different levels. Confirmation testing and/or regression testing for the test object are needed on all test levels if defects are fixed and/or changes are made on these test levels. v4.0 Page of 74 2023-04-21 2.3. Maintenance Testing There are different categories of maintenance, it can be corrective, adaptive to changes in the environment or improve performance or maintainability (see ISO/IEC 14764 for details), so maintenance can involve planned releases/deployments and unplanned releases/deployments (hot fixes). Impact analysis may be done before a change is made, to help decide if the change should be made, based on the potential consequences in other areas of the system. Testing the changes to a system in production includes both evaluating the success of the implementation of the change and the checking for possible regressions in parts of the system that remain unchanged (which is usually most of the system). The scope of maintenance testing typically depends on:  The degree of risk of the change  The size of the existing system  The size of the change The triggers for maintenance and maintenance testing can be classified as follows:  Modifications, such as planned enhancements (i.e., release-based), corrective changes or hot fixes.  Upgrades or migrations of the operational environment, such as from one platform to another, which can require tests associated with the new environment as well as of the changed software, or tests of data conversion when data from another application is migrated into the system being maintained.  Retirement, such as when an application reaches the end of its life. When a system is retired, this can require testing of data archiving if long data-retention periods are required. Testing of restore and retrieval procedures after archiving may also be needed in the event that certain data is required during the archiving period. v4.0 Page of 74 2023-04-21  v4.0 Page of 74 2023-04-21 3.1. Static Testing Basics In contrast to dynamic testing, in static testing the software under test does not need to be executed. Code, process specification, system architecture specification or other work products are evaluated through manual examination (e.g., reviews) or with the help of a tool (e.g., static analysis). Test objectives include improving quality, detecting defects and assessing characteristics like readability, completeness, correctness, testability and consistency. Static testing can be applied for both verification and validation. Testers, business representatives and developers work together during example mappings, collaborative user story writing and backlog refinement sessions to ensure that user stories and related work products meet defined criteria, e.g., the Definition of Ready (see section 5.1.3). Review techniques can be applied to ensure user stories are complete and understandable and include testable acceptance criteria. By asking the right questions, testers explore, challenge and help improve the proposed user stories. Static analysis can identify problems prior to dynamic testing while often requiring less effort, since no test cases are required, and tools (see chapter 6) are typically used. Static analysis is often incorporated into CI frameworks (see section 2.1.4). While largely used to detect specific code defects, static analysis is also used to evaluate maintainability and security. Spelling checkers and readability tools are other examples of static analysis tools.\n",
      "----------------------------------------\n",
      "Section Title: 3.1.2\n",
      "Section Content: Value of Static Testing Static testing can detect defects in the earliest phases of the SDLC, fulfilling the principle of early testing (see section 1.3). It can also identify defects which cannot be detected by dynamic testing (e.g., unreachable code, design patterns not implemented as desired, defects in non-executable work products). Static testing provides the ability to evaluate the quality of, and to build confidence in work products. By verifying the documented requirements, the stakeholders can also make sure that these requirements describe their actual needs. Since static testing can be performed early in the SDLC, a shared understanding can be created among the involved stakeholders. Communication will also be improved between the involved stakeholders. For this reason, it is recommended to involve a wide variety of stakeholders in static testing. Even though reviews can be costly to implement, the overall project costs are usually much lower than when no reviews are performed because less time and effort needs to be spent on fixing defects later in the project. v4.0 Page of 74 2023-04-21 Code defects can be detected using static analysis more efficiently than in dynamic testing, usually resulting in both fewer code defects and a lower overall development effort.\n",
      "----------------------------------------\n",
      "Section Title: 3.2.1\n",
      "Section Content: Benefits of Early and Frequent Stakeholder Feedback Early and frequent feedback allows for the early communication of potential quality problems. If there is little stakeholder involvement during the SDLC, the product being developed might not meet the stakeholder’s original or current vision. A failure to deliver what the stakeholder wants can result in costly rework, missed deadlines, blame games, and might even lead to complete project failure. v4.0 Page of 74 2023-04-21 Frequent stakeholder feedback throughout the SDLC can prevent misunderstandings about requirements and ensure that changes to requirements are understood and implemented earlier. This helps the development team to improve their understanding of what they are building. It allows them to focus on those features that deliver the most value to the stakeholders and that have the most positive impact on identified risks.\n",
      "----------------------------------------\n",
      "Section Title: 3.2.3\n",
      "Section Content: Roles and Responsibilities in Reviews Reviews involve various stakeholders, who may take on several roles. The principal roles and their responsibilities are:  Manager – decides what is to be reviewed and provides resources, such as staff and time for the review  Author – creates and fixes the work product under review v4.0 Page of 74 2023-04-21  Moderator (also known as the facilitator) – ensures the effective running of review meetings, including mediation, time management, and a safe review environment in which everyone can speak freely  Scribe (also known as recorder) – collates anomalies from reviewers and records review information, such as decisions and new anomalies found during the review meeting  Reviewer – performs reviews. A reviewer may be someone working on the project, a subject matter expert, or any other stakeholder  Review leader – takes overall responsibility for the review such as deciding who will be involved, and organizing when and where the review will take place Other, more detailed roles are possible, as described in the ISO/IEC 20246 standard.\n",
      "----------------------------------------\n",
      "Section Title: 3.2.5\n",
      "Section Content: Success Factors for Reviews There are several factors that determine the success of reviews, which include: v4.0 Page of 74 2023-04-21  Defining clear objectives and measurable exit criteria. Evaluation of participants should never be an objective  Choosing the appropriate review type to achieve the given objectives, and to suit the type of work product, the review participants, the project needs and context  Conducting reviews on small chunks, so that reviewers do not lose concentration during an individual review and/or the review meeting (when held)  Providing feedback from reviews to stakeholders and authors so they can improve the product and their activities (see section 3.2.1)  Providing adequate time to participants to prepare for the review  Support from management for the review process  Making reviews part of the organization’s culture, to promote learning and process improvement  Providing adequate training for all participants so they know how to fulfil their role  Facilitating meetings v4.0 Page of 74 2023-04-21 4. Test Analysis and Design – 390 minutes Keywords acceptance criteria, acceptance test-driven development, black-box test technique, boundary value analysis, branch coverage, checklist-based testing, collaboration-based test approach, coverage, coverage item, decision table testing, equivalence partitioning, error guessing, experience-based test technique, exploratory testing, state transition testing, statement coverage, test technique, white-box test technique   v4.0 Page of 74 2023-04-21 4.1. Test Techniques Overview Test techniques support the tester in test analysis (what to test) and in test design (how to test). Test techniques help to develop a relatively small, but sufficient, set of test cases in a systematic way. Test techniques also help the tester to define test conditions, identify coverage items, and identify test data during the test analysis and design. Further information on test techniques and their corresponding measures can be found in the ISO/IEC/IEEE 29119-4 standard, and in (Beizer 1990, Craig 2002, Copeland 2004, Koomen 2006, Jorgensen 2014, Ammann 2016, Forgács 2019). In this syllabus, test techniques are classified as black-box, white-box, and experience-based. Black-box test techniques (also known as specification-based techniques) are based on an analysis of the specified behavior of the test object without reference to its internal structure. Therefore, the test cases are independent of how the software is implemented. Consequently, if the implementation changes, but the required behavior stays the same, then the test cases are still useful. White-box test techniques (also known as structure-based techniques) are based on an analysis of the test object’s internal structure and processing. As the test cases are dependent on how the software is designed, they can only be created after the design or implementation of the test object. Experience-based test techniques effectively use the knowledge and experience of testers for the design and implementation of test cases. The effectiveness of these techniques depends heavily on the tester’s skills. Experience-based test techniques can detect defects that may be missed using the black- box and white-box test techniques. Hence, experience-based test techniques are complementary to the black-box and white-box test techniques. 4.2. Black-Box Test Techniques Commonly used black-box test techniques discussed in the following sections are:  Equivalence Partitioning  Boundary Value Analysis  Decision Table Testing  State Transition Testing\n",
      "----------------------------------------\n",
      "Section Title: 4.2.2\n",
      "Section Content: Boundary Value Analysis Boundary Value Analysis (BVA) is a technique based on exercising the boundaries of equivalence partitions. Therefore, BVA can only be used for ordered partitions. The minimum and maximum values of a partition are its boundary values. In the case of BVA, if two elements belong to the same partition, all elements between them must also belong to that partition. BVA focuses on the boundary values of the partitions because developers are more likely to make errors with these boundary values. Typical defects found by BVA are located where implemented boundaries are misplaced to positions above or below their intended positions or are omitted altogether. This syllabus covers two versions of the BVA: 2-value and 3-value BVA. They differ in terms of coverage items per boundary that need to be exercised to achieve 100% coverage. In 2-value BVA (Craig 2002, Myers 2011), for each boundary value there are two coverage items: this boundary value and its closest neighbor belonging to the adjacent partition. To achieve 100% coverage with 2-value BVA, test cases must exercise all coverage items, i.e., all identified boundary values. Coverage is measured as the number of boundary values that were exercised, divided by the total number of identified boundary values, and is expressed as a percentage. In 3-value BVA (Koomen 2006, O’Regan 2019), for each boundary value there are three coverage items: this boundary value and both its neighbors. Therefore, in 3-value BVA some of the coverage items may not be boundary values. To achieve 100% coverage with 3-value BVA, test cases must exercise all coverage items, i.e., identified boundary values and their neighbors. Coverage is measured as the number of boundary values and their neighbors exercised, divided by the total number of identified boundary values and their neighbors, and is expressed as a percentage. 3-value BVA is more rigorous than 2-value BVA as it may detect defects overlooked by 2-value BVA. For example, if the decision “if (x ≤ 10) …” is incorrectly implemented as “if (x = 10) …”, no test data derived from the 2-value BVA (x = 10, x = 11) can detect the defect. However, x = 9, derived from the 3-value BVA, is likely to detect it. v4.0 Page of 74 2023-04-21\n",
      "----------------------------------------\n",
      "Section Title: 4.2.4\n",
      "Section Content: State Transition Testing A state transition diagram models the behavior of a system by showing its possible states and valid state transitions. A transition is initiated by an event, which may be additionally qualified by a guard condition. The transitions are assumed to be instantaneous and may sometimes result in the software taking action. The common transition labeling syntax is as follows: “event [guard condition] / action”. Guard conditions and actions can be omitted if they do not exist or are irrelevant for the tester. A state table is a model equivalent to a state transition diagram. Its rows represent states, and its columns represent events (together with guard conditions if they exist). Table entries (cells) represent transitions, and contain the target state, as well as the resulting actions, if defined. In contrast to the state transition diagram, the state table explicitly shows invalid transitions, which are represented by empty cells. A test case based on a state transition diagram or state table is usually represented as a sequence of events, which results in a sequence of state changes (and actions, if needed). One test case may, and usually will, cover several transitions between states. There exist many coverage criteria for state transition testing. This syllabus discusses three of them. v4.0 Page of 74 2023-04-21 In all states coverage, the coverage items are the states. To achieve 100% all states coverage, test cases must ensure that all the states are visited. Coverage is measured as the number of visited states divided by the total number of states, and is expressed as a percentage. In valid transitions coverage (also called 0-switch coverage), the coverage items are single valid transitions. To achieve 100% valid transitions coverage, test cases must exercise all the valid transitions. Coverage is measured as the number of exercised valid transitions divided by the total number of valid transitions, and is expressed as a percentage. In all transitions coverage, the coverage items are all the transitions shown in a state table. To achieve 100% all transitions coverage, test cases must exercise all the valid transitions and attempt to execute invalid transitions. Testing only one invalid transition in a single test case helps to avoid fault masking, i.e., a situation in which one defect prevents the detection of another. Coverage is measured as the number of valid and invalid transitions exercised or attempted to be covered by executed test cases, divided by the total number of valid and invalid transitions, and is expressed as a percentage. All states coverage is weaker than valid transitions coverage, because it can typically be achieved without exercising all the transitions. Valid transitions coverage is the most widely used coverage criterion. Achieving full valid transitions coverage guarantees full all states coverage. Achieving full all transitions coverage guarantees both full all states coverage and full valid transitions coverage and should be a minimum requirement for mission and safety-critical software. 4.3. White-Box Test Techniques Because of their popularity and simplicity, this section focuses on two code-related white-box test techniques:  Statement testing  Branch testing There are more rigorous techniques that are used in some safety-critical, mission-critical, or high-integrity environments to achieve more thorough code coverage. There are also white-box test techniques used in higher test levels (e.g., API testing), or using coverage not related to code (e.g., neuron coverage in neural network testing). These techniques are not discussed in this syllabus.\n",
      "----------------------------------------\n",
      "Section Title: 4.3.2\n",
      "Section Content: Branch Testing and Branch Coverage A branch is a transfer of control between two nodes in the control flow graph, which shows the possible sequences in which source code statements are executed in the test object. Each transfer of control can be either unconditional (i.e., straight-line code) or conditional (i.e., a decision outcome). In branch testing the coverage items are branches and the aim is to design test cases to exercise branches in the code until an acceptable level of coverage is achieved. Coverage is measured as the number of branches exercised by the test cases divided by the total number of branches, and is expressed as a percentage. When 100% branch coverage is achieved, all branches in the code, unconditional and conditional, are exercised by test cases. Conditional branches typically correspond to a true or false outcome from an “if...then” decision, an outcome from a switch/case statement, or a decision to exit or continue in a loop. However, exercising a branch with a test case will not detect defects in all cases. For example, it may not detect defects requiring the execution of a specific path in a code. Branch coverage subsumes statement coverage. This means that any set of test cases achieving 100% branch coverage also achieves 100% statement coverage (but not vice versa).\n",
      "----------------------------------------\n",
      "Section Title: 4.4.1\n",
      "Section Content: Error Guessing Error guessing is a technique used to anticipate the occurrence of errors, defects, and failures, based on the tester’s knowledge, including:  How the application has worked in the past v4.0 Page of 74 2023-04-21  The types of errors the developers tend to make and the types of defects that result from these errors  The types of failures that have occurred in other, similar applications In general, errors, defects and failures may be related to: input (e.g., correct input not accepted, parameters wrong or missing), output (e.g., wrong format, wrong result), logic (e.g., missing cases, wrong operator), computation (e.g., incorrect operand, wrong computation), interfaces (e.g., parameter mismatch, incompatible types), or data (e.g., incorrect initialization, wrong type). Fault attacks are a methodical approach to the implementation of error guessing. This technique requires the tester to create or acquire a list of possible errors, defects and failures, and to design tests that will identify defects associated with the errors, expose the defects, or cause the failures. These lists can be built based on experience, defect and failure data, or from common knowledge about why software fails. See (Whittaker 2002, Whittaker 2003, Andrews 2006) for more information on error guessing and fault attacks.\n",
      "----------------------------------------\n",
      "Section Title: 4.4.3\n",
      "Section Content: Checklist-Based Testing In checklist-based testing, a tester designs, implements, and executes tests to cover test conditions from a checklist. Checklists can be built based on experience, knowledge about what is important for the user, or an understanding of why and how software fails. Checklists should not contain items that can be checked automatically, items better suited as entry/exit criteria, or items that are too general (Brykczynski 1999). Checklist items are often phrased in the form of a question. It should be possible to check each item separately and directly. These items may refer to requirements, graphical interface properties, quality characteristics or other forms of test conditions. Checklists can be created to support various test types, including functional and non-functional testing (e.g., 10 heuristics for usability testing (Nielsen 1994)). v4.0 Page of 74 2023-04-21 Some checklist entries may gradually become less effective over time because the developers will learn to avoid making the same errors. New entries may also need to be added to reflect newly found high severity defects. Therefore, checklists should be regularly updated based on defect analysis. However, care should be taken to avoid letting the checklist become too long (Gawande 2009). In the absence of detailed test cases, checklist-based testing can provide guidelines and some degree of consistency for the testing. If the checklists are high-level, some variability in the actual testing is likely to occur, resulting in potentially greater coverage but less repeatability. 4.5. Collaboration-based Test Approaches Each of the above-mentioned techniques (see sections 4.2, 4.3, 4.4) has a particular objective with respect to defect detection. Collaboration-based approaches, on the other hand, focus also on defect avoidance by collaboration and communication.\n",
      "----------------------------------------\n",
      "Section Title: 4.5.2\n",
      "Section Content: Acceptance Criteria Acceptance criteria for a user story are the conditions that an implementation of the user story must meet to be accepted by stakeholders. From this perspective, acceptance criteria may be viewed as the test conditions that should be exercised by the tests. Acceptance criteria are usually a result of the Conversation (see section 4.5.1). Acceptance criteria are used to:  Define the scope of the user story  Reach consensus among the stakeholders  Describe both positive and negative scenarios  Serve as a basis for the user story acceptance testing (see section 4.5.3) v4.0 Page of 74 2023-04-21  Allow accurate planning and estimation There are several ways to write acceptance criteria for a user story. The two most common formats are:  Scenario-oriented (e.g., Given/When/Then format used in BDD, see section 2.1.3)  Rule-oriented (e.g., bullet point verification list, or tabulated form of input-output mapping) Most acceptance criteria can be documented in one of these two formats. However, the team may use another, custom format, as long as the acceptance criteria are well-defined and unambiguous.\n",
      "----------------------------------------\n",
      "Section Title: 5.1.1\n",
      "Section Content: Purpose and Content of a Test Plan A test plan describes the objectives, resources and processes for a test project. A test plan:  Documents the means and schedule for achieving test objectives  Helps to ensure that the performed test activities will meet the established criteria  Serves as a means of communication with team members and other stakeholders  Demonstrates that testing will adhere to the existing test policy and test strategy (or explains why the testing will deviate from them) Test planning guides the testers’ thinking and forces the testers to confront the future challenges related to risks, schedules, people, tools, costs, effort, etc. The process of preparing a test plan is a useful way to think through the efforts needed to achieve the test project objectives. The typical content of a test plan includes:  Context of testing (e.g., scope, test objectives, constraints, test basis)  Assumptions and constraints of the test project  Stakeholders (e.g., roles, responsibilities, relevance to testing, hiring and training needs)  Communication (e.g., forms and frequency of communication, documentation templates)  Risk register (e.g., product risks, project risks)  Test approach (e.g., test levels, test types, test techniques, test deliverables, entry criteria and exit criteria, independence of testing, metrics to be collected, test data requirements, test environment requirements, deviations from the organizational test policy and test strategy)  Budget and schedule More details about the test plan and its content can be found in the ISO/IEC/IEEE 29119-3 standard.\n",
      "----------------------------------------\n",
      "Section Title: 5.1.3\n",
      "Section Content: Entry Criteria and Exit Criteria Entry criteria define the preconditions for undertaking a given activity. If entry criteria are not met, it is likely that the activity will prove to be more difficult, time-consuming, costly, and riskier. Exit criteria define what must be achieved in order to declare an activity completed. Entry criteria and exit criteria should be defined for each test level, and will differ based on the test objectives. Typical entry criteria include: availability of resources (e.g., people, tools, environments, test data, budget, time), availability of testware (e.g., test basis, testable requirements, user stories, test cases), and initial quality level of a test object (e.g., all smoke tests have passed). Typical exit criteria include: measures of thoroughness (e.g., achieved level of coverage, number of unresolved defects, defect density, number of failed test cases), and completion criteria (e.g., planned tests have been executed, static testing has been performed, all defects found are reported, all regression tests are automated). Running out of time or budget can also be viewed as valid exit criteria. Even without other exit criteria being satisfied, it can be acceptable to end testing under such circumstances, if the stakeholders have reviewed and accepted the risk to go live without further testing. In Agile software development, exit criteria are often called Definition of Done, defining the team’s objective metrics for a releasable item. Entry criteria that a user story must fulfill to start the development and/or testing activities are called Definition of Ready.\n",
      "----------------------------------------\n",
      "Section Title: 5.1.5\n",
      "Section Content: Test Case Prioritization Once the test cases and test procedures are specified and assembled into test suites, these test suites can be arranged in a test execution schedule that defines the order in which they are to be run. When prioritizing test cases, different factors can be taken into account. The most commonly used test case prioritization strategies are as follows:  Risk-based prioritization, where the order of test execution is based on the results of risk analysis (see section 5.2.3). Test cases covering the most important risks are executed first.  Coverage-based prioritization, where the order of test execution is based on coverage (e.g., statement coverage). Test cases achieving the highest coverage are executed first. In another variant, called additional coverage prioritization, the test case achieving the highest coverage is executed first; each subsequent test case is the one that achieves the highest additional coverage.  Requirements-based prioritization, where the order of test execution is based on the priorities of the requirements traced back to the corresponding test cases. Requirement priorities are defined by stakeholders. Test cases related to the most important requirements are executed first. Ideally, test cases would be ordered to run based on their priority levels, using, for example, one of the above-mentioned prioritization strategies. However, this practice may not work if the test cases or the features being tested have dependencies. If a test case with a higher priority is dependent on a test case with a lower priority, the lower priority test case must be executed first. The order of test execution must also take into account the availability of resources. For example, the required test tools, test environments or people that may only be available for a specific time window.\n",
      "----------------------------------------\n",
      "Section Title: 5.1.7\n",
      "Section Content: Testing Quadrants The testing quadrants, defined by Brian Marick (Marick 2003, Crispin 2008), group the test levels with the appropriate test types, activities, test techniques and work products in the Agile software development. The model supports test management in visualizing these to ensure that all appropriate test types and test levels are included in the SDLC and in understanding that some test types are more relevant to certain test levels than others. This model also provides a way to differentiate and describe the types of tests to all stakeholders, including developers, testers, and business representatives. In this model, tests can be business facing or technology facing. Tests can also support the team (i.e., guide the development) or critique the product (i.e., measure its behavior against the expectations). The combination of these two viewpoints determines the four quadrants:  Quadrant Q1 (technology facing, support the team). This quadrant contains component and component integration tests. These tests should be automated and included in the CI process.  Quadrant Q2 (business facing, support the team). This quadrant contains functional tests, examples, user story tests, user experience prototypes, API testing, and simulations. These tests check the acceptance criteria and can be manual or automated.  Quadrant Q3 (business facing, critique the product). This quadrant contains exploratory testing, usability testing, user acceptance testing. These tests are user-oriented and often manual.  Quadrant Q4 (technology facing, critique the product). This quadrant contains smoke tests and non-functional tests (except usability tests). These tests are often automated. 5.2. Risk Management Organizations face many internal and external factors that make it uncertain whether and when they will achieve their objectives (ISO 31000). Risk management allows the organizations to increase the likelihood of achieving objectives, improve the quality of their products and increase the stakeholders’ confidence and trust. The main risk management activities are:  Risk analysis (consisting of risk identification and risk assessment; see section 5.2.3)  Risk control (consisting of risk mitigation and risk monitoring; see section 5.2.4) The test approach, in which test activities are selected, prioritized, and managed based on risk analysis and risk control, is called risk-based testing.\n",
      "----------------------------------------\n",
      "Section Title: 5.2.2\n",
      "Section Content: Project Risks and Product Risks In software testing one is generally concerned with two types of risks: project risks and product risks. Project risks are related to the management and control of the project. Project risks include:  Organizational issues (e.g., delays in work products deliveries, inaccurate estimates, cost-cutting)  People issues (e.g., insufficient skills, conflicts, communication problems, shortage of staff)  Technical issues (e.g., scope creep, poor tool support)  Supplier issues (e.g., third-party delivery failure, bankruptcy of the supporting company) Project risks, when they occur, may have an impact on the project schedule, budget or scope, which affects the project's ability to achieve its objectives. Product risks are related to the product quality characteristics (e.g., described in the ISO 25010 quality model). Examples of product risks include: missing or wrong functionality, incorrect calculations, runtime errors, poor architecture, inefficient algorithms, inadequate response time, poor user experience, security vulnerabilities. Product risks, when they occur, may result in various negative consequences, including:  User dissatisfaction  Loss of revenue, trust, reputation  Damage to third parties  High maintenance costs, overload of the helpdesk  Criminal penalties  In extreme cases, physical damage, injuries or even death\n",
      "----------------------------------------\n",
      "Section Title: 5.2.4\n",
      "Section Content: Product Risk Control Product risk control comprises all measures that are taken in response to identified and assessed product risks. Product risk control consists of risk mitigation and risk monitoring. Risk mitigation involves implementing the actions proposed in risk assessment to reduce the risk level. The aim of risk monitoring is to ensure that the mitigation actions are effective, to obtain further information to improve risk assessment, and to identify emerging risks. With respect to product risk control, once a risk has been analyzed, several response options to risk are possible, e.g., risk mitigation by testing, risk acceptance, risk transfer, or contingency plan (Veenendaal 2012). Actions that can be taken to mitigate the product risks by testing are as follows:  Select the testers with the right level of experience and skills, suitable for a given risk type  Apply an appropriate level of independence of testing  Conduct reviews and perform static analysis  Apply the appropriate test techniques and coverage levels  Apply the appropriate test types addressing the affected quality characteristics  Perform dynamic testing, including regression testing 5.3. Test Monitoring, Test Control and Test Completion Test monitoring is concerned with gathering information about testing. This information is used to assess test progress and to measure whether the test exit criteria or the test tasks associated with the exit criteria are satisfied, such as meeting the targets for coverage of product risks, requirements, or acceptance criteria. Test control uses the information from test monitoring to provide, in a form of the control directives, guidance and the necessary corrective actions to achieve the most effective and efficient testing. Examples of control directives include:  Reprioritizing tests when an identified risk becomes an issue  Re-evaluating whether a test item meets entry criteria or exit criteria due to rework  Adjusting the test schedule to address a delay in the delivery of the test environment  Adding new resources when and where needed v4.0 Page of 74 2023-04-21 Test completion collects data from completed test activities to consolidate experience, testware, and any other relevant information. Test completion activities occur at project milestones such as when a test level is completed, an agile iteration is finished, a test project is completed (or cancelled), a software system is released, or a maintenance release is completed.\n",
      "----------------------------------------\n",
      "Section Title: 5.3.2\n",
      "Section Content: Purpose, Content and Audience for Test Reports Test reporting summarizes and communicates test information during and after testing. Test progress reports support the ongoing control of the testing and must provide enough information to make modifications to the test schedule, resources, or test plan, when such changes are needed due to deviation from the plan or changed circumstances. Test completion reports summarize a specific stage of testing (e.g., test level, test cycle, iteration) and can give information for subsequent testing. During test monitoring and control, the test team generates test progress reports for stakeholders to keep them informed. Test progress reports are usually generated on a regular basis (e.g., daily, weekly, etc.) and include:  Test period  Test progress (e.g., ahead or behind schedule), including any notable deviations  Impediments for testing, and their workarounds  Test metrics (see section 5.3.1 for examples)  New and changed risks within testing period  Testing planned for the next period v4.0 Page of 74 2023-04-21 A test completion report is prepared during test completion, when a project, test level, or test type is complete and when, ideally, its exit criteria have been met. This report uses test progress reports and other data. Typical test completion reports include:  Test summary  Testing and product quality evaluation based on the original test plan (i.e., test objectives and exit criteria)  Deviations from the test plan (e.g., differences from the planned schedule, duration, and effort).  Testing impediments and workarounds  Test metrics based on test progress reports  Unmitigated risks, defects not fixed  Lessons learned that are relevant to the testing Different audiences require different information in the reports, and influence the degree of formality and the frequency of reporting. Reporting on test progress to others in the same team is often frequent and informal, while reporting on testing for a completed project follows a set template and occurs only once. The ISO/IEC/IEEE 29119-3 standard includes templates and examples for test progress reports (called test status reports) and test completion reports.\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Define a regular expression pattern to match section titles\n",
    "section_pattern = r'\\d+\\.\\d+\\.\\d+\\.'\n",
    "\n",
    "# Using re.finditer to find all section titles and their starting positions\n",
    "section_matches = re.finditer(section_pattern, extracted_text)\n",
    "\n",
    "# Create lists to store sections\n",
    "sections = []\n",
    "\n",
    "# Iterate through section matches\n",
    "for match in section_matches:\n",
    "    start_pos = match.start()\n",
    "    end_pos = (\n",
    "        match.end()\n",
    "        if match.end() < len(extracted_text)\n",
    "        else len(extracted_text)\n",
    "    )\n",
    "    section_title = match.group().strip()\n",
    "    \n",
    "    # Remove the last dot from the section title\n",
    "    section_title = section_title[:-1]  # Remove the last dot\n",
    "    \n",
    "    # Find the corresponding section content based on section title position\n",
    "    content_start = end_pos\n",
    "    content_end = (\n",
    "        next(section_matches).start()\n",
    "        if content_start < len(extracted_text)\n",
    "        else len(extracted_text)\n",
    "    )\n",
    "    section_content = extracted_text[content_start:content_end].strip()\n",
    "    \n",
    "    sections.append((section_title, section_content))\n",
    "\n",
    "# Print the extracted sections\n",
    "if sections:\n",
    "    for section in sections:\n",
    "        print(\"Section Title:\", section[0])\n",
    "        print(\"Section Content:\", section[1])\n",
    "        print(\"-\" * 40)\n",
    "else:\n",
    "    print(\"No sections found in the text.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DRAFT\n",
    "\n",
    "# Sample text (replace this with your actual text)\n",
    "\n",
    "# Define a regular expression pattern to match section titles\n",
    "# section_pattern = r'\\b\\d+\\.\\d+(?:\\.\\d+)?(?=\\s)'\n",
    "# \n",
    "#Using re.finditer to find all section titles and their starting positions\n",
    "# section_matches = re.finditer(section_pattern, extracted_text)\n",
    "# \n",
    "#Create lists to store sections\n",
    "# sections = []\n",
    "# \n",
    "#Iterate through section matches\n",
    "# for match in section_matches:\n",
    "    # start_pos = match.start()\n",
    "    # end_pos = (\n",
    "        # match.end()\n",
    "        # if match.end() < len(extracted_text)\n",
    "        # else len(extracted_text)\n",
    "    # )\n",
    "    # section_title = match.group().strip()\n",
    "    # \n",
    "#   Remove the last dot from the section title\n",
    "    # section_title = section_title[:-1]  # Remove the last dot\n",
    "    # \n",
    "#    Find the corresponding section content based on section title position\n",
    "    # content_start = end_pos\n",
    "    # content_end = (\n",
    "        # next(section_matches).start()\n",
    "        # if content_start < len(extracted_text)\n",
    "        # else len(extracted_text)\n",
    "    # )\n",
    "    # section_content = extracted_text[content_start:content_end].strip()\n",
    "    # \n",
    "    # sections.append((section_title, section_content))\n",
    "# \n",
    "#Print the extracted sections\n",
    "# if sections:\n",
    "    # for section in sections:\n",
    "        # print(\"Section Title:\", section[0])\n",
    "        # print(\"Section Content:\", section[1])\n",
    "        # print(\"-\" * 40)\n",
    "# else:\n",
    "    # print(\"No sections found in the text.\")\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!Base Modeling!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: test objectives vary depending on the context, which includes the work product being tested, the test level, risks, the software development lifecycle being followed, and factors related to the business context, e.g., corporate structure, competitive considerations, or time to market.\n"
     ]
    }
   ],
   "source": [
    "#T5\n",
    "\n",
    "# Load the pre-trained T5 model and tokenizer\n",
    "model_name = \"t5-small\"\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Input text (use sections[0][1] as the content of the first section)\n",
    "section_content = sections[0][1]\n",
    "\n",
    "# Tokenize the input section\n",
    "input_ids = tokenizer.encode(\"summarize: \" + section_content, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "\n",
    "# Generate the summary\n",
    "summary_ids = model.generate(input_ids, max_length=150, min_length=30, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Print the summary\n",
    "print(\"Summary:\", summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keywords extraction - missing so far, leads to HTTPSConnectionPool(host='cdn-lfs.huggingface.co', port=443): Read timed out. A lot of hours are required. \n",
    "# Talk to teachers\n",
    "\n",
    "# Load the pre-trained KeyBERT model\n",
    "#model = KeyBERT(\"distilbert-base-nli-mean-tokens\")\n",
    "\n",
    "# Input text (use sections[0][1] as the content of the first section)\n",
    "#section_content = sections[0][1]\n",
    "\n",
    "# Extract keywords\n",
    "#try:\n",
    "#    keywords = model.extract_keywords(section_content, keyphrase_ngram_range=(1, 2), stop_words='english', use_mmr=True, top_n=10, resume_download=True)\n",
    "    \n",
    "    # Print the extracted keywords\n",
    "#    for keyword in keywords:\n",
    "#        print(keyword)\n",
    "#except Exception as e:\n",
    "#    print(\"An error occurred:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1: What is test objectives?\n",
      "Question 2: What is objectives vary?\n",
      "Question 3: What is vary depending?\n",
      "Question 4: What is depending on?\n",
      "Question 5: What is on the?\n",
      "Question 6: What is the context?\n",
      "Question 7: What is context ,?\n",
      "Question 8: What is , which?\n",
      "Question 9: What is which includes?\n",
      "Question 10: What is includes the?\n",
      "Question 11: What is the work?\n",
      "Question 12: What is work product?\n",
      "Question 13: What is product being?\n",
      "Question 14: What is being tested?\n",
      "Question 15: What is tested ,?\n",
      "Question 16: What is , the?\n",
      "Question 17: What is the test?\n",
      "Question 18: What is test level?\n",
      "Question 19: What is level ,?\n",
      "Question 20: What is , risks?\n",
      "Question 21: What is risks ,?\n",
      "Question 22: What is , the?\n",
      "Question 23: What is the software?\n",
      "Question 24: What is software development?\n",
      "Question 25: What is development lifecycle?\n",
      "Question 26: What is lifecycle being?\n",
      "Question 27: What is being followed?\n",
      "Question 28: What is followed ,?\n",
      "Question 29: What is , and?\n",
      "Question 30: What is and factors?\n",
      "Question 31: What is factors related?\n",
      "Question 32: What is related to?\n",
      "Question 33: What is to the?\n",
      "Question 34: What is the business?\n",
      "Question 35: What is business context?\n",
      "Question 36: What is context ,?\n",
      "Question 37: What is , e.g.?\n",
      "Question 38: What is e.g. ,?\n",
      "Question 39: What is , corporate?\n",
      "Question 40: What is corporate structure?\n",
      "Question 41: What is structure ,?\n",
      "Question 42: What is , competitive?\n",
      "Question 43: What is competitive considerations?\n",
      "Question 44: What is considerations ,?\n",
      "Question 45: What is , or?\n",
      "Question 46: What is or time?\n",
      "Question 47: What is time to?\n",
      "Question 48: What is to market?\n",
      "Question 49: What is market .?\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.util import ngrams\n",
    "\n",
    "# Sample summary\n",
    "#summary = \"The typical test objectives are: • Evaluating work products such as requirements, user stories, designs, and code • Triggering failures and finding defects • Ensuring required coverage of a test object • Verifying that a test object complies with contractual, legal, and regulatory requirements • Providing information to stakeholders to allow them to make informed decisions • Building confidence in the quality of the test object • Validating whether the test object is complete and works as expected by the stakeholders\"\n",
    "\n",
    "# Tokenize the summary into sentences\n",
    "sentences = sent_tokenize(summary)\n",
    "\n",
    "# Function to generate questions from sentences\n",
    "def generate_questions(text):\n",
    "    questions = []\n",
    "    for sentence in text:\n",
    "        # Tokenize each sentence into words\n",
    "        words = nltk.word_tokenize(sentence)\n",
    "        # Generate n-grams (bi-grams) from the words\n",
    "        n_grams = list(ngrams(words, 2))\n",
    "        \n",
    "        # Construct questions using the n-grams\n",
    "        for n_gram in n_grams:\n",
    "            question = f\"What is {n_gram[0]} {n_gram[1]}?\"\n",
    "            questions.append(question)\n",
    "    \n",
    "    return questions\n",
    "\n",
    "# Generate questions from the sentences\n",
    "questions = generate_questions(sentences)\n",
    "\n",
    "# Print the generated questions\n",
    "for i, question in enumerate(questions, start=1):\n",
    "    print(f\"Question {i}: {question}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained model and tokenizer\n",
    "#model_name = \"bert-large-uncased-whole-word-masking-finetuned-squad\"\n",
    "#tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "#model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "\n",
    "# Provide a passage and a question\n",
    "#passage = extracted_text\n",
    "#question = \"Which of the following statements describe a valid test objective?\"\n",
    "\n",
    "#Which of the following statements describe a valid test objective?\n",
    "#What does not work as expected?\n",
    "\n",
    "# Tokenize the passage and question\n",
    "#inputs = tokenizer(question, passage, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "# Get the answer from the model\n",
    "#start_scores, end_scores = model(**inputs, return_dict = False)\n",
    "#start_idx = torch.argmax(start_scores)\n",
    "#end_idx = torch.argmax(end_scores)\n",
    "\n",
    "# Decode the answer from the tokenized output\n",
    "#answer_tokens = inputs[\"input_ids\"][0][start_idx:end_idx + 1]\n",
    "#answer = tokenizer.decode(answer_tokens)\n",
    "\n",
    "#print(\"Answer:\", answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To process text and generate questions with answers, you can consider using pre-trained language models, such as GPT-3, GPT-4, BERT, T5, or similar models. Each of these models has its strengths and can be used for different aspects of question generation and answering:\n",
    "\n",
    "T5 (Text-to-Text Transfer Transformer): T5 is a versatile language model that can be fine-tuned for various natural language processing tasks, including question generation. You can fine-tune a pre-trained T5 model on your specific dataset to generate high-quality questions.\n",
    "\n",
    "GPT-3: OpenAI's GPT-3 is a powerful language model known for its natural language generation capabilities. You can prompt GPT-3 to generate questions based on your input text. It can produce contextually relevant questions, but it may require careful instruction and filtering of the generated output.\n",
    "\n",
    "BART (Bidirectional and Auto-Regressive Transformers): BART is another transformer-based model that can be fine-tuned for question generation tasks. It excels in text generation tasks and can produce coherent and meaningful questions.\n",
    "\n",
    "XLNet: XLNet is a transformer model that has achieved strong performance in various NLP tasks. It can be fine-tuned for question generation, and its bidirectional context modeling can lead to better question generation.\n",
    "\n",
    "BERT (Bidirectional Encoder Representations from Transformers): BERT can also be used for question generation by fine-tuning. While it was originally designed for understanding context, it can be adapted for question generation with appropriate training data.\n",
    "\n",
    "UniLM: UniLM is a model that can be used for various text generation tasks, including question generation. It combines unidirectional, bidirectional, and sequence-to-sequence learning, making it versatile for NLP tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "# from io import StringIO\n",
    "\n",
    "# # Load the pre-trained model and tokenizer\n",
    "# model_name = \"bert-large-uncased-whole-word-masking-finetuned-squad\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "\n",
    "# # Create a StringIO object with your text\n",
    "# text_io = StringIO()\n",
    "# text_io.write(\"Your text goes here.\")\n",
    "# text_io.seek(0)  # Reset the StringIO object to the beginning\n",
    "\n",
    "# # Read the text from the StringIO object and convert it to a regular string\n",
    "# text = text_io.read()\n",
    "\n",
    "# # Provide a question\n",
    "# question = \"What is the answer to my question?\"\n",
    "\n",
    "# # Tokenize the text and question\n",
    "# inputs = tokenizer(question, text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "# # Get the answer from the model\n",
    "# start_scores, end_scores = model(**inputs)\n",
    "# start_idx = torch.argmax(start_scores)\n",
    "# end_idx = torch.argmax(end_scores)\n",
    "\n",
    "# # Decode the answer from the tokenized output\n",
    "# answer_tokens = inputs[\"input_ids\"][0][start_idx:end_idx + 1]\n",
    "# answer = tokenizer.decode(answer_tokens)\n",
    "\n",
    "# print(\"Answer:\", answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Question Generation using Seq2Seq (T5)\n",
    "\n",
    "# Load the pre-trained Seq2Seq model for question generation\n",
    "# question_generation_model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "# question_generation_tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "# \n",
    "#ISTQB document (replace with your actual content)\n",
    "# istqb_document = \"\"\"\n",
    "# 1.1. What is Testing? \n",
    "# \n",
    "# Software systems are an integral part of our daily life. Most people have had experience with software \n",
    "# that did not work as expected. Software that does not work correctly can lead to many problems, \n",
    "# including loss of money, time or business reputation, and, in extreme cases, even injury or death. \n",
    "# Software testing assesses software quality and helps reducing the risk of software failure in operation. \n",
    "# \n",
    "# Software testing is a set of activities to discover defects and evaluate the quality of software artifacts. \n",
    "# These artifacts, when being tested, are known as test objects. A common misconception about testing is \n",
    "# that it only consists of executing tests (i.e., running the software and checking the test results). However, \n",
    "# software testing also includes other activities and must be aligned with the software development lifecycle \n",
    "# (see chapter 2). \n",
    "# \n",
    "# Another common misconception about testing is that testing focuses entirely on verifying the test object. \n",
    "# Whilst testing involves verification, i.e., checking whether the system meets specified requirements, it also \n",
    "# involves validation, which means checking whether the system meets users’ and other stakeholders’ \n",
    "# needs in its operational environment. \n",
    "# \"\"\"\n",
    "# \n",
    "#Generate questions from the ISTQB document\n",
    "# def generate_questions(document, max_length=64, num_questions=1):\n",
    "    # inputs = question_generation_tokenizer.encode(\"generate questions: \" + document, return_tensors=\"pt\", max_length=max_length, truncation=True)\n",
    "    # questions = question_generation_model.generate(inputs, max_length=max_length, num_return_sequences=num_questions)\n",
    "    # return [question_generation_tokenizer.decode(question, skip_special_tokens=True) for question in questions]\n",
    "# \n",
    "# generated_questions = generate_questions(istqb_document)\n",
    "# \n",
    "#Print generated questions\n",
    "# for question in generated_questions:\n",
    "    # print(\"Question:\", question)\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is template for Quize layout, needs to be re-worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input from keyboard, user selects section to process, user's output can be used as input for Section parameter below\n",
    "def my_function(input_text):\n",
    "    # Your processing logic here\n",
    "    return \"You entered: \" + input_text\n",
    "\n",
    "\n",
    "# Read your document sections into a list or dictionary\n",
    "document_sections = {\n",
    "    \"section1\": \"Content of Section 1...\",\n",
    "    \"section2\": \"Content of Section 2...\",\n",
    "    # Add more sections as needed\n",
    "}\n",
    "\n",
    "def generate_quiz(section):\n",
    "    # Process the input section and generate questions and answers\n",
    "    # Example: questions = generate_questions(document_sections[section])\n",
    "    \n",
    "    # Replace this with your actual quiz generation logic\n",
    "    questions = [\"Question 1\", \"Question 2\", \"Question 3\"]\n",
    "    answer_choices = [[\"Choice 1\", \"Choice 2\", \"Choice 3\"], [\"Choice 1\", \"Choice 2\", \"Choice 3\"], [\"Choice 1\", \"Choice 2\", \"Choice 3\"]]\n",
    "    correct_answers = [1, 0, 2]\n",
    "    \n",
    "    return {\n",
    "        \"questions\": questions,\n",
    "        \"answer_choices\": answer_choices,\n",
    "        \"correct_answers\": correct_answers\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "iface = gr.Interface(fn=generate_quiz, inputs=\"text\", outputs=\"json\", css=\".gradio-container {background-color: lightblue}\")\n",
    "\n",
    "iface.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draft metrics\n",
    "\n",
    "import nltk\n",
    "\n",
    "# Reference questions (human-generated)\n",
    "reference_questions = [\n",
    "    \"What is the test objective?\",\n",
    "    \"How do objectives vary?\",\n",
    "    \"What does the context include?\",\n",
    "    # Add more reference questions here\n",
    "]\n",
    "\n",
    "# Automatically generated questions\n",
    "generated_questions = [\n",
    "    \"What is test objectives?\",\n",
    "    \"What is objectives vary?\",\n",
    "    \"How do objectives depend?\",\n",
    "    # Add more generated questions here\n",
    "]\n",
    "\n",
    "# Initialize the NLTK BLEU scorer\n",
    "bleu_scorer = nltk.translate.bleu_score.SmoothingFunction()\n",
    "\n",
    "# Calculate BLEU score (a measure of similarity)\n",
    "bleu_scores = [nltk.translate.bleu_score.sentence_bleu([r.split()], g.split(), smoothing_function=bleu_scorer.method1) for r, g in zip(reference_questions, generated_questions)]\n",
    "\n",
    "# Calculate accuracy rate (percentage of questions that match reference questions)\n",
    "accuracy_rate = sum(score == 1.0 for score in bleu_scores) / len(bleu_scores) * 100\n",
    "\n",
    "print(\"Accuracy Rate: {:.2f}%\".format(accuracy_rate))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qgmc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
