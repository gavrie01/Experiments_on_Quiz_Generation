{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we are downloading PDF file, converting it to TXT and doing some \"pre-cleaning\": removing not meaningful parts of document and leaving just the most valuable leftovers for our future generator.\n",
    "THe outcome of the below code is pre-processed but still raw data.\n",
    "\n",
    "\n",
    "\"extracted_text\" variable has \"StringIO\" type: The StringIO object is part of Python's io module and is a class that provides an in-memory file-like object that can be used for reading from or writing to strings as if they were files. It allows you to treat strings as file-like objects, which can be useful in various situations, such as when you want to read from or write to a string in a way that mimics file operations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import of libraries\n",
    "from io import StringIO # extracted_text is the main variable, contains the whole text of document in stringIO format in memory\n",
    "import requests\n",
    "import re  # provides reg. exp. support\n",
    "import math\n",
    "\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfdocument import PDFDocument\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.pdfparser import PDFParser\n",
    "import fitz\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "import sentencepiece\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer, BertForQuestionAnswering, BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U pip setuptools wheel\n",
    "#!pip install -U spacy\n",
    "#!python -m spacy download en_core_web_sm\n",
    "#!pip install PyMuPDF # this is fitz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1113747"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# downloading pdf to '/data/' folder\n",
    "url = 'https://astqb.org/assets/documents/ISTQB_CTFL_Syllabus-v4.0.pdf'\n",
    "r = requests.get(url, allow_redirects=True)\n",
    "open('data/ISTQB_CTFL_Syllabus-v4.0.pdf', 'wb').write(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "r\"Page \\d{4,74} of 74\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted text for 'The Certified Tester Foundation Level in Software Testing' saved to 'data/ISTQB_CTFL_Syllabus-v4.0.txt'\n"
     ]
    }
   ],
   "source": [
    "#converting pdf to text and saving into .txt file initial version\n",
    "output_string = StringIO()\n",
    "output_file_path = 'data/ISTQB_CTFL_Syllabus-v4.0.txt'\n",
    "with open('data/ISTQB_CTFL_Syllabus-v4.0.pdf', 'rb') as in_file, open(output_file_path, 'w', encoding='utf-8') as out_file:\n",
    "    parser = PDFParser(in_file)\n",
    "    doc = PDFDocument(parser)\n",
    "    rsrcmgr = PDFResourceManager()\n",
    "    device = TextConverter(rsrcmgr, output_string, laparams=LAParams())\n",
    "    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "    for page in PDFPage.create_pages(doc):\n",
    "        interpreter.process_page(page)\n",
    "    # Getting the extracted text from StringIO, it means the entire text extracted from the PDF is stored as a single string in memory.\n",
    "    extracted_text = output_string.getvalue()\n",
    "    # Writing the extracted text to the output file\n",
    "    out_file.write(extracted_text)\n",
    "\n",
    "# Closing the stream\n",
    "output_string.close()\n",
    "\n",
    "\n",
    "# Printing message to indicate that the text has been saved to the file\n",
    "print(f\"Extracted text for 'The Certified Tester Foundation Level in Software Testing' saved to '{output_file_path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us check size of StringIO on the full size of converted file, just out of curiosity\n",
    "size_bytes = len(extracted_text.encode('utf-8'))\n",
    "print ('The length of string in bytes : ' + str (size_bytes))\n",
    "\n",
    "# function's code is taken from stackoverflow ---\n",
    "def convert_size(size_bytes):\n",
    "   if size_bytes == 0:\n",
    "       return \"0B\"\n",
    "   size_name = (\"B\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\", \"EB\", \"ZB\", \"YB\")\n",
    "   i = int(math.floor(math.log(size_bytes, 1024)))\n",
    "   p = math.pow(1024, i)\n",
    "   s = round(size_bytes / p, 2)\n",
    "   return \"%s %s\" % (s, size_name[i])\n",
    "# ---\n",
    "print(\"File size, document contains 70+ pages: \", convert_size(size_bytes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking up for the text to remove everything before it\n",
    "target_text = \"1.1. What is Testing?\"\n",
    "\n",
    "# Finding the position of the target text in the extracted text\n",
    "start_position = extracted_text.find(target_text)\n",
    "\n",
    "# Checking if the target text was found, just in case\n",
    "if start_position != -1:\n",
    "    # Removing everything before the target text\n",
    "    extracted_text = extracted_text[start_position:]\n",
    "\n",
    "\n",
    "# let us save the content to .txt file with prefix '_v0.1' for further debugging purpose and human evaluation process\n",
    "\n",
    "output_file_path = 'data/ISTQB_CTFL_Syllabus-v4.0_v01.txt'\n",
    "with open(output_file_path, 'w', encoding='utf-8') as out_file:\n",
    "    # Writing the extracted text to the output file\n",
    "    out_file.write(extracted_text)\n",
    "\n",
    "# Closing the stream\n",
    "output_string.close()\n",
    "\n",
    "# Printing message to indicate that the text has been saved to the file\n",
    "print(f\"Extracted text for 'The Certified Tester Foundation Level in Software Testing' pre processed version 0.1 saved to '{output_file_path}'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing empty lines\n",
    "# _ - is iterator, if s.strip(): This part of the list comprehension checks whether the line s contains any non-whitespace characters. \n",
    "# If it does, the line is included in the resulting list.\n",
    "\n",
    "extracted_text = \"\".join([_ for _ in extracted_text.strip().splitlines(True) if _.strip()])\n",
    "\n",
    "output_file_path = 'data/ISTQB_CTFL_Syllabus-v4.0_v02.txt'\n",
    "with open('data/ISTQB_CTFL_Syllabus-v4.0_v01.txt', 'rb') as in_file, open(output_file_path, 'w', encoding='utf-8') as out_file:\n",
    "    # Writing the extracted text to the output file\n",
    "    out_file.write(extracted_text)\n",
    "\n",
    "# Closing the stream\n",
    "output_string.close()\n",
    "\n",
    "# Printing message to indicate that the text has been saved to the file\n",
    "print(f\"Extracted text for 'The Certified Tester Foundation Level in Software Testing' pre processed version 0.2 saved to '{output_file_path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing text from 'Page 56 of 74' till the end of the text\n",
    "\n",
    "# Looking up for the text to remove everything after it\n",
    "target_text = \"Page 56 of 74\"\n",
    "\n",
    "# Finding the position of the target text in the extracted text\n",
    "end_position = extracted_text.find(target_text)\n",
    "\n",
    "# Checking if the target text was found, just in case\n",
    "if end_position != -1:\n",
    "    # Removing everything before the target text\n",
    "    extracted_text = extracted_text[:end_position]\n",
    "\n",
    "\n",
    "# let us save the content to .txt file with prefix '_v0.1' for further debugging purpose and human evaluation process\n",
    "\n",
    "output_file_path = 'data/ISTQB_CTFL_Syllabus-v4.0_v03.txt'\n",
    "with open(output_file_path, 'w', encoding='utf-8') as out_file:\n",
    "    # Writing the extracted text to the output file\n",
    "    out_file.write(extracted_text)\n",
    "\n",
    "# Closing the stream\n",
    "output_string.close()\n",
    "\n",
    "# Printing message to indicate that the text has been saved to the file\n",
    "print(f\"Extracted text for 'The Certified Tester Foundation Level in Software Testing' pre processed version 0.1 saved to '{output_file_path}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your stop words list\n",
    "stop_words = [\"v4.0\", \"Page\", \"74\", \"18\", \"15\", \"of\", \"2023-04-21\", \"©\", \"Certified Tester\", \"Foundation\", \"Level\", \"International Software Testing Qualifications Board\"]\n",
    "\n",
    "# Split the extracted_text into words\n",
    "words = extracted_text.split()\n",
    "\n",
    "# Filter out words that are in the stop words list\n",
    "filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "\n",
    "# Join the filtered words back into a text\n",
    "extracted_text = \" \".join(filtered_words)\n",
    "\n",
    "# Print the cleaned text\n",
    "#print(extracted_text)\n",
    "\n",
    "\n",
    "output_file_path = 'data/ISTQB_CTFL_Syllabus-v4.0_v05.txt'\n",
    "with open(output_file_path, 'w', encoding='utf-8') as out_file:\n",
    "    # Writing the extracted text to the output file\n",
    "    out_file.write(extracted_text)\n",
    "\n",
    "# Closing the stream\n",
    "output_string.close()\n",
    "\n",
    "# Printing message to indicate that the text has been saved to the file\n",
    "print(f\"Extracted text for 'The Certified Tester Foundation Level in Software Testing' pre processed version 0.5 saved to '{output_file_path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to lower case all words in stringIO\n",
    "#extracted_text = extracted_text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#punctuation\n",
    "# Load the language model\n",
    "#nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Process the text with SpaCy\n",
    "###doc = nlp(extracted_text)\n",
    "\n",
    "# Create a list of tokens that are not punctuation\n",
    "#filtered_tokens = [token.text for token in doc if not token.is_punct]\n",
    "\n",
    "# Join the filtered tokens back into a text\n",
    "#extracted_text = \" \".join(filtered_tokens)\n",
    "\n",
    "# Print the text without punctuation\n",
    "#print(extracted_text)\n",
    "\n",
    "#output_file_path = 'data/ISTQB_CTFL_Syllabus-v4.0_v04.txt'\n",
    "#with open(output_file_path, 'w', encoding='utf-8') as out_file:\n",
    "    # Writing the extracted text to the output file\n",
    "#    out_file.write(extracted_text)\n",
    "\n",
    "# Closing the stream\n",
    "#output_string.close()\n",
    "\n",
    "# Printing message to indicate that the text has been saved to the file\n",
    "#print(f\"Extracted text for 'The Certified Tester Foundation Level in Software Testing' pre processed version 0.1 saved to '{output_file_path}'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted text for 'The Certified Tester Foundation Level in Software Testing' pre processed version 0.5 saved to 'data/ISTQB_CTFL_Syllabus-v4.0_v05.txt'\n"
     ]
    }
   ],
   "source": [
    "# built by chatgpt on provided context from my side, I used a part of text of file above, reviewed and customized by me as well\n",
    "#stop_words = [\"buxton\", \"a\", \"about\", \"above\", \"additional\", \"an\", \"and\", \"another\", \"are\", \"as\", \"be\", \"being\", \"by\", \"can\", \"common\", \"commonly\", \"do\", \"does\", \"each\", \"even\", \"for\", \"from\", \"has\", \"have\", \"in\", \"including\", \"is\", \"it\", \"its\", \"it's\", \"many\", \"may\", \"more\", \"most\", \"not\", \"of\", \"74\", \"often\", \"on\", \"or\", \"over\", \"such\", \"than\", \"that\", \"the\", \"there\", \"these\", \"this\", \"to\", \"under\", \"was\", \"we\", \"what\", \"when\", \"which\", \"who\", \"why\", \"will\", \"with\", \"within\", \"work\", \"you\", \"2023\", \"04\", \"21\", \"v4.0\", \"page\", \"2023-04-21\", \"©\", \"international\", \"qualifications\", \"board\", \"certified\", \"tester\",  \"foundation\", \"level\", \"FL-\", \"K2\", \"see\", \"section\" , \"didn't\", \"doesn't\", \"don't\", \"i.e.\", \"it's\", \"let's\", \"that's\", \"there's\", \"they're\", \"you're\", \"e.g.\"]\n",
    "stop_words = [\"15\", \"16\", \"17\", \"18\", \"19\", \"20\", r\"\\b20\\b\", \"21\", \"22\", \"23\", \"24\", \"25\", \"26\", \"27\", \"28\", \"29\", \"30\", \"31\", \"32\", \"33\", \"34\", \"35\", \"36\",\n",
    "              \"37\", \"38\",\"39\", \"40\",\"41\", \"42\",\"43\", \"44\",\"45\", \"46\", \"47\", \"48\", \"49\", \"50\", \"51\", \"52\", \"53\", \"54\", \n",
    "              \"International Software Testing Qualifications Board Certified Tester Foundation Level\"]\n",
    "# \n",
    "# Your stop words list\n",
    "#stop_words = [\"v4.0\", \"page\", \"of\", \"2023-04-21\", \"©\", \"International Software Testing Qualifications Board\",\n",
    " #             \"Certified\", \"Tester\", \"Foundation\", \"Level\"]\n",
    "# \n",
    "# Regular expression pattern to match phrases like \"15 74\", \"16 74\", ..., \"54 74\"\n",
    "pattern = re.compile(r\"(?s)^v4.0.*Foundation Level$\", re.DOTALL)\n",
    "# \n",
    "# Split the extracted_text into words\n",
    "words = re.split(r'\\s+', extracted_text)\n",
    "# \n",
    "# Filter out words that match the regular expression pattern or are in the stop words list\n",
    "filtered_words = [word for word in words if not re.match(pattern, word) and word.lower() not in stop_words]\n",
    "\n",
    "# \n",
    "# Join the filtered words back into a text\n",
    "extracted_text = \" \".join(filtered_words)\n",
    "\n",
    "# Define the phrase you want to remove\n",
    "phrase_to_remove = \"International Software Testing Qualifications Board Certified Tester Foundation Level\"\n",
    "\n",
    "# Replace the phrase with an empty string\n",
    "extracted_text = extracted_text.replace(phrase_to_remove, \"\")\n",
    "\n",
    "# \n",
    "output_file_path = 'data/ISTQB_CTFL_Syllabus-v4.0_v05.txt'\n",
    "with open(output_file_path, 'w', encoding='utf-8') as out_file:\n",
    "    # Writing the extracted text to the output file\n",
    "    out_file.write(extracted_text)\n",
    "# \n",
    "# Closing the stream\n",
    "output_string.close()\n",
    "# \n",
    "# Printing message to indicate that the text has been saved to the file\n",
    "print(f\"Extracted text for 'The Certified Tester Foundation Level in Software Testing' pre processed version 0.5 saved to '{output_file_path}'\")\n",
    "# \n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted text for 'The Certified Tester Foundation Level in Software Testing' pre processed version 0.6 saved to 'data/ISTQB_CTFL_Syllabus-v4.0_v06.txt'\n"
     ]
    }
   ],
   "source": [
    "pattern = r'[0-9]'\n",
    "\n",
    "# Match all digits in the string and replace them with an empty string\n",
    "extracted_text = re.sub(pattern, '', extracted_text)\n",
    "\n",
    "output_file_path = 'data/ISTQB_CTFL_Syllabus-v4.0_v06.txt'\n",
    "with open(output_file_path, 'w', encoding='utf-8') as out_file:\n",
    "    # Writing the extracted text to the output file\n",
    "    out_file.write(extracted_text)\n",
    "# \n",
    "# Closing the stream\n",
    "output_string.close()\n",
    "# \n",
    "# Printing message to indicate that the text has been saved to the file\n",
    "print(f\"Extracted text for 'The Certified Tester Foundation Level in Software Testing' pre processed version 0.6 saved to '{output_file_path}'\")\n",
    "# \n",
    "# \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Load the language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Your text\n",
    "text = extracted_text\n",
    "\n",
    "# Process the text with SpaCy\n",
    "doc = nlp(text)\n",
    "\n",
    "# Create a StringIO object to store the NER results\n",
    "output_string = io.StringIO()\n",
    "\n",
    "# Extract named entities and write them to the StringIO object\n",
    "for ent in doc.ents:\n",
    "    output_string.write(f\"Entity: {ent.text}, Type: {ent.label_}\\n\")\n",
    "\n",
    "# Get the NER results as a string\n",
    "ner_results = output_string.getvalue()\n",
    "\n",
    "output_file_path = 'data/NER.txt'\n",
    "with open(output_file_path, 'w', encoding='utf-8') as out_file:\n",
    "    # Writing the extracted text to the output file\n",
    "      out_file.write(ner_results)\n",
    "\n",
    "# Closing the stream\n",
    "output_string.close()\n",
    "\n",
    "# Printing message to indicate that the text has been saved to the file\n",
    "print(f\"Extracted NER list for 'The Certified Tester Foundation Level in Software Testing; {output_file_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point NER dict is saved into /data folder, edited manually and now let us import this file into stop_list StringIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Check the content of stop_list_stringio\n",
    "content = stop_list_stringio.getvalue()\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Markov Chain\n",
    "# Sample text (replace with your extracted_text)\n",
    "# Tokenize the text into words\n",
    "#tokens = nltk.word_tokenize(extracted_text)\n",
    "\n",
    "# Create a dictionary to store transition probabilities\n",
    "#transition_probabilities = {}\n",
    "\n",
    "# Build the transition probability matrix\n",
    "#for i in range(len(tokens) - 1):\n",
    "#    current_token = tokens[i]\n",
    "#    next_token = tokens[i + 1]\n",
    "    \n",
    "#    if current_token in transition_probabilities:\n",
    "#        transition_probabilities[current_token].append(next_token)\n",
    "#    else:\n",
    "#        transition_probabilities[current_token] = [next_token]\n",
    "\n",
    "# Start with an initial word\n",
    "#current_word = random.choice(tokens)\n",
    "\n",
    "# Generate a sentence of a certain length\n",
    "#generated_text = [current_word]\n",
    "#sentence_length = 10\n",
    "\n",
    "#for _ in range(sentence_length - 1):\n",
    "#    if current_word in transition_probabilities:\n",
    "#        next_word = random.choice(transition_probabilities[current_word])\n",
    "#        generated_text.append(next_word)\n",
    "#        current_word = next_word\n",
    "#    else:\n",
    "#        break\n",
    "\n",
    "# Join the generated words into a sentence\n",
    "#generated_sentence = \" \".join(generated_text)\n",
    "#print(generated_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Defining a regular expression pattern to match section titles\n",
    "section_pattern = r'\\b\\d+(?:\\.\\d+)+\\s+'\n",
    "\n",
    "# Using re.finditer to find all section titles and their starting positions, extracted_text is a stringIO from previous code\n",
    "section_matches = re.finditer(section_pattern, extracted_text)\n",
    "\n",
    "# Create a list to store sections\n",
    "sections = []\n",
    "\n",
    "# Iterate through section matches\n",
    "for match in section_matches:\n",
    "    start_pos = match.start()\n",
    "    end_pos = section_matches.__next__().start() if match.end() < len(extracted_text) else len(extracted_text)\n",
    "    section_title = match.group()\n",
    "    section_content = extracted_text[start_pos:end_pos].strip()\n",
    "    sections.append((section_title, section_content))\n",
    "\n",
    "# Print the first section title and content\n",
    "if sections:\n",
    "    for section in sections:\n",
    "        print(section)\n",
    "else:\n",
    "    print(\"No sections found, take a look something went wrong\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained model and tokenizer\n",
    "model_name = \"bert-large-uncased-whole-word-masking-finetuned-squad\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "\n",
    "# Provide a passage and a question\n",
    "passage = extracted_text\n",
    "question = \"Which of the following statements describe a valid test objective?\"\n",
    "\n",
    "#Which of the following statements describe a valid test objective?\n",
    "#What does not work as expected?\n",
    "\n",
    "# Tokenize the passage and question\n",
    "inputs = tokenizer(question, passage, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "# Get the answer from the model\n",
    "start_scores, end_scores = model(**inputs, return_dict = False)\n",
    "start_idx = torch.argmax(start_scores)\n",
    "end_idx = torch.argmax(end_scores)\n",
    "\n",
    "# Decode the answer from the tokenized output\n",
    "answer_tokens = inputs[\"input_ids\"][0][start_idx:end_idx + 1]\n",
    "answer = tokenizer.decode(answer_tokens)\n",
    "\n",
    "print(\"Answer:\", answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To process text and generate questions with answers, you can consider using pre-trained language models, such as GPT-3, GPT-4, BERT, T5, or similar models. Each of these models has its strengths and can be used for different aspects of question generation and answering:\n",
    "\n",
    "1. **GPT-3 and GPT-4 (Generative Pre-trained Transformers):** These models are known for their ability to generate coherent and contextually relevant text. They can be fine-tuned for question generation and answer generation tasks. You can use them to generate questions based on a given text and answer those questions by extracting relevant information from the text. These models are more versatile for generating human-like text.\n",
    "\n",
    "2. **BERT (Bidirectional Encoder Representations from Transformers):** BERT is a popular model for various NLP tasks, including question-answering. It can be fine-tuned for specific question-answering tasks and can provide highly accurate answers. BERT models are effective at understanding the context of a text passage and extracting information.\n",
    "\n",
    "3. **T5 (Text-to-Text Transfer Transformer):** T5 is designed to convert all NLP tasks into a text-to-text format. You can fine-tune a T5 model to perform question generation and answer generation tasks in this format. It's known for its versatility and performance on various NLP tasks.\n",
    "\n",
    "4. **Dedicated Question-Answering Models:** There are models specifically designed for question-answering tasks, such as the Stanford Question Answering Dataset (SQuAD) models. These models are fine-tuned on question-answering datasets and are optimized for extracting answers from text given specific questions.\n",
    "\n",
    "When choosing a model, consider the specific requirements of your project, such as the complexity of questions, the size of your dataset, and the desired level of accuracy. You may need to fine-tune these models on your specific task and dataset to achieve the best results. Additionally, you can use NLP libraries like Hugging Face Transformers or the OpenAI API, which provide access to pre-trained models for question generation and answering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "# from io import StringIO\n",
    "\n",
    "# # Load the pre-trained model and tokenizer\n",
    "# model_name = \"bert-large-uncased-whole-word-masking-finetuned-squad\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "\n",
    "# # Create a StringIO object with your text\n",
    "# text_io = StringIO()\n",
    "# text_io.write(\"Your text goes here.\")\n",
    "# text_io.seek(0)  # Reset the StringIO object to the beginning\n",
    "\n",
    "# # Read the text from the StringIO object and convert it to a regular string\n",
    "# text = text_io.read()\n",
    "\n",
    "# # Provide a question\n",
    "# question = \"What is the answer to my question?\"\n",
    "\n",
    "# # Tokenize the text and question\n",
    "# inputs = tokenizer(question, text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "# # Get the answer from the model\n",
    "# start_scores, end_scores = model(**inputs)\n",
    "# start_idx = torch.argmax(start_scores)\n",
    "# end_idx = torch.argmax(end_scores)\n",
    "\n",
    "# # Decode the answer from the tokenized output\n",
    "# answer_tokens = inputs[\"input_ids\"][0][start_idx:end_idx + 1]\n",
    "# answer = tokenizer.decode(answer_tokens)\n",
    "\n",
    "# print(\"Answer:\", answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "seq2seq to bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install sentencepiece\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Question Generation using Seq2Seq (T5)\n",
    "\n",
    "# Load the pre-trained Seq2Seq model for question generation\n",
    "question_generation_model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "question_generation_tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "\n",
    "# ISTQB document (replace with your actual content)\n",
    "istqb_document = \"\"\"\n",
    "1.1. What is Testing? \n",
    "\n",
    "Software systems are an integral part of our daily life. Most people have had experience with software \n",
    "that did not work as expected. Software that does not work correctly can lead to many problems, \n",
    "including loss of money, time or business reputation, and, in extreme cases, even injury or death. \n",
    "Software testing assesses software quality and helps reducing the risk of software failure in operation. \n",
    "\n",
    "Software testing is a set of activities to discover defects and evaluate the quality of software artifacts. \n",
    "These artifacts, when being tested, are known as test objects. A common misconception about testing is \n",
    "that it only consists of executing tests (i.e., running the software and checking the test results). However, \n",
    "software testing also includes other activities and must be aligned with the software development lifecycle \n",
    "(see chapter 2). \n",
    "\n",
    "Another common misconception about testing is that testing focuses entirely on verifying the test object. \n",
    "Whilst testing involves verification, i.e., checking whether the system meets specified requirements, it also \n",
    "involves validation, which means checking whether the system meets users’ and other stakeholders’ \n",
    "needs in its operational environment. \n",
    "\"\"\"\n",
    "\n",
    "# Generate questions from the ISTQB document\n",
    "def generate_questions(document, max_length=64, num_questions=1):\n",
    "    inputs = question_generation_tokenizer.encode(\"generate questions: \" + document, return_tensors=\"pt\", max_length=max_length, truncation=True)\n",
    "    questions = question_generation_model.generate(inputs, max_length=max_length, num_return_sequences=num_questions)\n",
    "    return [question_generation_tokenizer.decode(question, skip_special_tokens=True) for question in questions]\n",
    "\n",
    "generated_questions = generate_questions(istqb_document)\n",
    "\n",
    "# Print generated questions\n",
    "for question in generated_questions:\n",
    "    print(\"Question:\", question)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qgmc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
