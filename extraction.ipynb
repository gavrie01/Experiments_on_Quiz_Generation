{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we are downloading PDF file, converting it to TXT and doing some \"pre-cleaning\": removing not meaningful parts of document and leaving just the most valuable leftovers for our future generator.\n",
    "THe outcome of the below code is pre-processed but still raw data.\n",
    "\n",
    "\n",
    "\"extracted_text\" variable has \"StringIO\" type: The StringIO object is part of Python's io module and is a class that provides an in-memory file-like object that can be used for reading from or writing to strings as if they were files. It allows you to treat strings as file-like objects, which can be useful in various situations, such as when you want to read from or write to a string in a way that mimics file operations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import of libraries\n",
    "from io import StringIO # extracted_text is the main variable, contains the whole text of document in stringIO format in memory\n",
    "import requests\n",
    "import re  # provides reg. exp. support\n",
    "import math\n",
    "\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfdocument import PDFDocument\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.pdfparser import PDFParser\n",
    "import fitz\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import spacy\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "import sentencepiece\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer, BertForQuestionAnswering, BertTokenizer\n",
    "from transformers import AutoTokenizer\n",
    "from keybert import KeyBERT\n",
    "import gradio as gr # UI part for the quize\n",
    "import urllib3\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# libraries for conversion from csv to json, results of \"Flag\" button click CSV -> JSON\n",
    "import csv\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U pip setuptools wheel\n",
    "#!pip install -U spacy\n",
    "#!python -m spacy download en_core_web_sm\n",
    "#!pip install PyMuPDF # this is fitz\n",
    "#!pip install gradio\n",
    "#!pip install keybert\n",
    "#!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1113747"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# downloading pdf to '/data/' folder\n",
    "url = 'https://astqb.org/assets/documents/ISTQB_CTFL_Syllabus-v4.0.pdf'\n",
    "r = requests.get(url, allow_redirects=True)\n",
    "open('data/ISTQB_CTFL_Syllabus-v4.0.pdf', 'wb').write(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "r\"Page \\d{4,74} of 74\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted text for 'The Certified Tester Foundation Level in Software Testing' saved to 'data/ISTQB_CTFL_Syllabus-v4.0.txt'\n"
     ]
    }
   ],
   "source": [
    "#converting pdf to text and saving into .txt file initial version\n",
    "output_string = StringIO()\n",
    "output_file_path = 'data/ISTQB_CTFL_Syllabus-v4.0.txt'\n",
    "with open('data/ISTQB_CTFL_Syllabus-v4.0.pdf', 'rb') as in_file, open(output_file_path, 'w', encoding='utf-8') as out_file:\n",
    "    parser = PDFParser(in_file)\n",
    "    doc = PDFDocument(parser)\n",
    "    rsrcmgr = PDFResourceManager()\n",
    "    device = TextConverter(rsrcmgr, output_string, laparams=LAParams())\n",
    "    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "    for page in PDFPage.create_pages(doc):\n",
    "        interpreter.process_page(page)\n",
    "    # Getting the extracted text from StringIO, it means the entire text extracted from the PDF is stored as a single string in memory.\n",
    "    extracted_text = output_string.getvalue()\n",
    "    # Writing the extracted text to the output file\n",
    "    out_file.write(extracted_text)\n",
    "\n",
    "# Closing the stream\n",
    "output_string.close()\n",
    "\n",
    "\n",
    "# Printing message to indicate that the text has been saved to the file\n",
    "print(f\"Extracted text for 'The Certified Tester Foundation Level in Software Testing' saved to '{output_file_path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of string in bytes : 198489\n",
      "File size, document contains 70+ pages:  193.84 KB\n"
     ]
    }
   ],
   "source": [
    "# let us check size of StringIO on the full size of converted file, just out of curiosity\n",
    "size_bytes = len(extracted_text.encode('utf-8'))\n",
    "print ('The length of string in bytes : ' + str (size_bytes))\n",
    "\n",
    "# function's code is taken from stackoverflow ---\n",
    "def convert_size(size_bytes):\n",
    "   if size_bytes == 0:\n",
    "       return \"0B\"\n",
    "   size_name = (\"B\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\", \"EB\", \"ZB\", \"YB\")\n",
    "   i = int(math.floor(math.log(size_bytes, 1024)))\n",
    "   p = math.pow(1024, i)\n",
    "   s = round(size_bytes / p, 2)\n",
    "   return \"%s %s\" % (s, size_name[i])\n",
    "# ---\n",
    "print(\"File size, document contains 70+ pages: \", convert_size(size_bytes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted text for 'The Certified Tester Foundation Level in Software Testing' pre processed version 0.1 saved to 'data/ISTQB_CTFL_Syllabus-v4.0_v01.txt'\n"
     ]
    }
   ],
   "source": [
    "# Looking up for the text to remove everything before it\n",
    "target_text = \"1.1. What is Testing?\"\n",
    "\n",
    "# Finding the position of the target text in the extracted text\n",
    "start_position = extracted_text.find(target_text)\n",
    "\n",
    "# Checking if the target text was found, just in case\n",
    "if start_position != -1:\n",
    "    # Removing everything before the target text\n",
    "    extracted_text = extracted_text[start_position:]\n",
    "\n",
    "\n",
    "# let us save the content to .txt file with prefix '_v0.1' for further debugging purpose and human evaluation process\n",
    "\n",
    "output_file_path = 'data/ISTQB_CTFL_Syllabus-v4.0_v01.txt'\n",
    "with open(output_file_path, 'w', encoding='utf-8') as out_file:\n",
    "    # Writing the extracted text to the output file\n",
    "    out_file.write(extracted_text)\n",
    "\n",
    "# Closing the stream\n",
    "output_string.close()\n",
    "\n",
    "# Printing message to indicate that the text has been saved to the file\n",
    "print(f\"Extracted text for 'The Certified Tester Foundation Level in Software Testing' pre processed version 0.1 saved to '{output_file_path}'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing empty lines\n",
    "# _ - is iterator, if s.strip(): This part of the list comprehension checks whether the line s contains any non-whitespace characters. \n",
    "# If it does, the line is included in the resulting list.\n",
    "\n",
    "# extracted_text = \"\".join([_ for _ in extracted_text.strip().splitlines(True) if _.strip()])\n",
    "# \n",
    "# output_file_path = 'data/ISTQB_CTFL_Syllabus-v4.0_v02.txt'\n",
    "# with open('data/ISTQB_CTFL_Syllabus-v4.0_v01.txt', 'rb') as in_file, open(output_file_path, 'w', encoding='utf-8') as out_file:\n",
    " #   Writing the extracted text to the output file\n",
    "    # out_file.write(extracted_text)\n",
    "# \n",
    "#Closing the stream\n",
    "# output_string.close()\n",
    "# \n",
    "#Printing message to indicate that the text has been saved to the file\n",
    "# print(f\"Extracted text for 'The Certified Tester Foundation Level in Software Testing' pre processed version 0.2 saved to '{output_file_path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted text for 'The Certified Tester Foundation Level in Software Testing' pre processed version 0.1 saved to 'data/ISTQB_CTFL_Syllabus-v4.0_v03.txt'\n"
     ]
    }
   ],
   "source": [
    "# Removing text from 'Page 56 of 74' till the end of the text\n",
    "\n",
    "# Looking up for the text to remove everything after it\n",
    "target_text = \"Page 56 of 74\"\n",
    "\n",
    "# Finding the position of the target text in the extracted text\n",
    "end_position = extracted_text.find(target_text)\n",
    "\n",
    "# Checking if the target text was found, just in case\n",
    "if end_position != -1:\n",
    "    # Removing everything before the target text\n",
    "    extracted_text = extracted_text[:end_position]\n",
    "\n",
    "\n",
    "# let us save the content to .txt file with prefix '_v0.1' for further debugging purpose and human evaluation process\n",
    "\n",
    "output_file_path = 'data/ISTQB_CTFL_Syllabus-v4.0_v03.txt'\n",
    "with open(output_file_path, 'w', encoding='utf-8') as out_file:\n",
    "    # Writing the extracted text to the output file\n",
    "    out_file.write(extracted_text)\n",
    "\n",
    "# Closing the stream\n",
    "output_string.close()\n",
    "\n",
    "# Printing message to indicate that the text has been saved to the file\n",
    "print(f\"Extracted text for 'The Certified Tester Foundation Level in Software Testing' pre processed version 0.1 saved to '{output_file_path}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your stop words list\n",
    "# stop_words = [\"v4.0\", \"Page\", \"74\", \"18\", \"15\", \"of\", \"2023-04-21\", \"©\", \"Certified Tester\", \"Foundation\", \"Level\", \"International Software Testing Qualifications Board\"]\n",
    "# \n",
    "#Split the extracted_text into words\n",
    "# words = extracted_text.split()\n",
    "# \n",
    "#Filter out words that are in the stop words list\n",
    "# filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "# \n",
    "#Join the filtered words back into a text\n",
    "# extracted_text = \" \".join(filtered_words)\n",
    "# \n",
    "#Print the cleaned text\n",
    "#print(extracted_text)\n",
    "# \n",
    "# \n",
    "# output_file_path = 'data/ISTQB_CTFL_Syllabus-v4.0_v05.txt'\n",
    "# with open(output_file_path, 'w', encoding='utf-8') as out_file:\n",
    "#    Writing the extracted text to the output file\n",
    "    # out_file.write(extracted_text)\n",
    "# \n",
    "#Closing the stream\n",
    "# output_string.close()\n",
    "# \n",
    "#Printing message to indicate that the text has been saved to the file\n",
    "# print(f\"Extracted text for 'The Certified Tester Foundation Level in Software Testing' pre processed version 0.5 saved to '{output_file_path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to lower case all words in stringIO\n",
    "#extracted_text = extracted_text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#punctuation\n",
    "# Load the language model\n",
    "#nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Process the text with SpaCy\n",
    "###doc = nlp(extracted_text)\n",
    "\n",
    "# Create a list of tokens that are not punctuation\n",
    "#filtered_tokens = [token.text for token in doc if not token.is_punct]\n",
    "\n",
    "# Join the filtered tokens back into a text\n",
    "#extracted_text = \" \".join(filtered_tokens)\n",
    "\n",
    "# Print the text without punctuation\n",
    "#print(extracted_text)\n",
    "\n",
    "#output_file_path = 'data/ISTQB_CTFL_Syllabus-v4.0_v04.txt'\n",
    "#with open(output_file_path, 'w', encoding='utf-8') as out_file:\n",
    "    # Writing the extracted text to the output file\n",
    "#    out_file.write(extracted_text)\n",
    "\n",
    "# Closing the stream\n",
    "#output_string.close()\n",
    "\n",
    "# Printing message to indicate that the text has been saved to the file\n",
    "#print(f\"Extracted text for 'The Certified Tester Foundation Level in Software Testing' pre processed version 0.1 saved to '{output_file_path}'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check results, looks like some parts are not removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted text for 'The Certified Tester Foundation Level in Software Testing' pre processed version 0.5 saved to 'data/ISTQB_CTFL_Syllabus-v4.0_v05.txt'\n"
     ]
    }
   ],
   "source": [
    "# built by chatgpt on provided context from my side, I used a part of text of file above, reviewed and customized by me as well\n",
    "#stop_words = [\"buxton\", \"a\", \"about\", \"above\", \"additional\", \"an\", \"and\", \"another\", \"are\", \"as\", \"be\", \"being\", \"by\", \"can\", \"common\", \"commonly\", \"do\", \"does\", \"each\", \"even\", \"for\", \"from\", \"has\", \"have\", \"in\", \"including\", \"is\", \"it\", \"its\", \"it's\", \"many\", \"may\", \"more\", \"most\", \"not\", \"of\", \"74\", \"often\", \"on\", \"or\", \"over\", \"such\", \"than\", \"that\", \"the\", \"there\", \"these\", \"this\", \"to\", \"under\", \"was\", \"we\", \"what\", \"when\", \"which\", \"who\", \"why\", \"will\", \"with\", \"within\", \"work\", \"you\", \"2023\", \"04\", \"21\", \"v4.0\", \"page\", \"2023-04-21\", \"©\", \"international\", \"qualifications\", \"board\", \"certified\", \"tester\",  \"foundation\", \"level\", \"FL-\", \"K2\", \"see\", \"section\" , \"didn't\", \"doesn't\", \"don't\", \"i.e.\", \"it's\", \"let's\", \"that's\", \"there's\", \"they're\", \"you're\", \"e.g.\"]\n",
    "stop_words = [ \"©\", \"15\", \"16\", \"17\", \"18\", \"19\", \"20\", r\"\\b20\\b\", \"21\", \"22\", \"23\", \"24\", \"25\", \"26\", \"27\", \"28\", \"29\", \"30\", \"31\", \"32\", \"33\", \"34\", \"35\", \"36\",\n",
    "              \"37\", \"38\",\"39\", \"40\",\"41\", \"42\",\"43\", \"44\",\"45\", \"46\", \"47\", \"48\", \"49\", \"50\", \"51\", \"52\", \"53\", \"54\", \n",
    "              \"International Software Testing Qualifications Board Certified Tester Foundation Level\", \"21.04.2023\", \"01.07.2021\",\n",
    "              \"11.11.2019\", \"27.04.2018\", \"1.04.2011\", \"30.03.2010\", \"01.05.2007\", \"01.07.2005\", \"25.02.1999\", \"the\", \"market\", \"(\", \")\", \"in\", \"or\"]\n",
    " \n",
    "# Regular expression pattern to match phrases like \"15 74\", \"16 74\", ..., \"54 74\"\n",
    "pattern = re.compile(r\"(?s)^v4.0.*Foundation Level$\", re.DOTALL)\n",
    "# Split the extracted_text into words\n",
    "words = re.split(r'\\s+', extracted_text)\n",
    "# \n",
    "# Filter out words that match the regular expression pattern or are in the stop words list\n",
    "filtered_words = [word for word in words if not re.match(pattern, word) and word.lower() not in stop_words]\n",
    "# Join the filtered words back into a text\n",
    "extracted_text = \" \".join(filtered_words)\n",
    "\n",
    "# Ph. removal\n",
    "phrase_to_remove = \"International Software Testing Qualifications Board Certified Tester Foundation Level\"\n",
    "\n",
    "# Replace the phrase with an empty string and comas removal (across the whole text)\n",
    "extracted_text = extracted_text.replace(phrase_to_remove, \"\")\n",
    "extracted_text = extracted_text.replace(\",\", \"\")\n",
    "\n",
    "# Regular expression pattern to match and remove text inside brackets and brackets as well\n",
    "pattern_brackets = r'\\([^)]*\\)'\n",
    "\n",
    "# removal of text inside brackets and brackets\n",
    "extracted_text = re.sub(pattern_brackets, '', extracted_text)\n",
    "\n",
    "output_file_path = 'data/ISTQB_CTFL_Syllabus-v4.0_v05.txt' \n",
    "with open(output_file_path, 'w', encoding='utf-8') as out_file:\n",
    "    # Writing the extracted text to the output file\n",
    "    out_file.write(extracted_text)\n",
    "output_string.close()\n",
    " \n",
    "# Printing message to indicate that the text has been saved to the file\n",
    "print(f\"Extracted text for 'The Certified Tester Foundation Level in Software Testing' pre processed version 0.5 saved to '{output_file_path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pattern = r'[0-9]'\n",
    "\n",
    "# Match all digits in the string and replace them with an empty string\n",
    "#extracted_text = re.sub(pattern, '', extracted_text)\n",
    "\n",
    "#extracted_text = ''.join((x for x in extracted_text if not x.isdigit()))\n",
    "\n",
    "\n",
    "#output_file_path = 'data/ISTQB_CTFL_Syllabus-v4.0_v06.txt'\n",
    "#with open(output_file_path, 'w', encoding='utf-8') as out_file:\n",
    "    # Writing the extracted text to the output file\n",
    "#    out_file.write(extracted_text)\n",
    "# \n",
    "# Closing the stream\n",
    "#output_string.close()\n",
    "# \n",
    "# Printing message to indicate that the text has been saved to the file\n",
    "#print(f\"Extracted text for 'The Certified Tester Foundation Level in Software Testing' pre processed version 0.6 saved to '{output_file_path}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# # Load the language model\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# # Your text\n",
    "# text = extracted_text\n",
    "\n",
    "# # Process the text with SpaCy\n",
    "# doc = nlp(text)\n",
    "\n",
    "# # Create a StringIO object to store the NER results\n",
    "# output_string = io.StringIO()\n",
    "\n",
    "# # Extract named entities and write them to the StringIO object\n",
    "# for ent in doc.ents:\n",
    "#     output_string.write(f\"Entity: {ent.text}, Type: {ent.label_}\\n\")\n",
    "\n",
    "# # Get the NER results as a string\n",
    "# ner_results = output_string.getvalue()\n",
    "\n",
    "# output_file_path = 'data/NER.txt'\n",
    "# with open(output_file_path, 'w', encoding='utf-8') as out_file:\n",
    "#     # Writing the extracted text to the output file\n",
    "#       out_file.write(ner_results)\n",
    "\n",
    "# # Closing the stream\n",
    "# output_string.close()\n",
    "\n",
    "# # Printing message to indicate that the text has been saved to the file\n",
    "# print(f\"Extracted NER list for 'The Certified Tester Foundation Level in Software Testing; {output_file_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point NER dict is saved into /data folder, edited manually and now let us import this file into stop_list StringIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Check the content of stop_list_stringio\n",
    "#content = stop_list_stringio.getvalue()\n",
    "#print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Markov Chain\n",
    "# Sample text (replace with your extracted_text)\n",
    "# Tokenize the text into words\n",
    "#tokens = nltk.word_tokenize(extracted_text)\n",
    "\n",
    "# Create a dictionary to store transition probabilities\n",
    "#transition_probabilities = {}\n",
    "\n",
    "# Build the transition probability matrix\n",
    "#for i in range(len(tokens) - 1):\n",
    "#    current_token = tokens[i]\n",
    "#    next_token = tokens[i + 1]\n",
    "    \n",
    "#    if current_token in transition_probabilities:\n",
    "#        transition_probabilities[current_token].append(next_token)\n",
    "#    else:\n",
    "#        transition_probabilities[current_token] = [next_token]\n",
    "\n",
    "# Start with an initial word\n",
    "#current_word = random.choice(tokens)\n",
    "\n",
    "# Generate a sentence of a certain length\n",
    "#generated_text = [current_word]\n",
    "#sentence_length = 10\n",
    "\n",
    "#for _ in range(sentence_length - 1):\n",
    "#    if current_word in transition_probabilities:\n",
    "#        next_word = random.choice(transition_probabilities[current_word])\n",
    "#        generated_text.append(next_word)\n",
    "#        current_word = next_word\n",
    "#    else:\n",
    "#        break\n",
    "\n",
    "# Join the generated words into a sentence\n",
    "#generated_sentence = \" \".join(generated_text)\n",
    "#print(generated_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted text for 'The Certified Tester Foundation Level in Software Testing' pre processed version 0.7 saved to 'data/ISTQB_CTFL_Syllabus-v4.0_v07.txt'\n"
     ]
    }
   ],
   "source": [
    "# remove chapter 4 beginning\n",
    "\n",
    "# Define the regular expression pattern for the text to remove\n",
    "pattern = r'4\\. Test Analysis and Design – 390 minutes.*?(K3) Use acceptance test-driven development (ATDD) to derive test cases'\n",
    "\n",
    "# Use re.sub to replace the matched text with a marker (e.g., 'REMOVED')\n",
    "extracted_text = re.sub(pattern, \"\", extracted_text, flags=re.DOTALL)\n",
    "\n",
    "# Define the phrase you want to remove\n",
    "phrase_to_remove = \"Learning Objectives for Chapter 4:\"\n",
    "\n",
    "# Replace the phrase with an empty string\n",
    "extracted_text = extracted_text.replace(phrase_to_remove, \"\")\n",
    "# \n",
    "output_file_path = 'data/ISTQB_CTFL_Syllabus-v4.0_v07.txt'\n",
    "with open(output_file_path, 'w', encoding='utf-8') as out_file:\n",
    "    # Writing the extracted text to the output file\n",
    "    out_file.write(extracted_text)\n",
    "# \n",
    "# Closing the stream\n",
    "output_string.close()\n",
    "# \n",
    "# Printing message to indicate that the text has been saved to the file\n",
    "print(f\"Extracted text for 'The Certified Tester Foundation Level in Software Testing' pre processed version 0.7 saved to '{output_file_path}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted text for 'The Certified Tester Foundation Level in Software Testing' pre processed version 0.8 saved to 'data/ISTQB_CTFL_Syllabus-v4.0_v08.txt'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# remove chapter 4 beginning\n",
    "# Define the regular expression pattern to remove the desired text\n",
    "pattern = r'4\\.1 Test Techniques Overview.*?4\\.5\\.3 \\(K3\\) Use acceptance test-driven development \\(ATDD\\) to derive test cases'\n",
    "\n",
    "# Use re.sub to replace the matched text with an empty string\n",
    "extracted_text = re.sub(pattern, '', extracted_text, flags=re.DOTALL)\n",
    "\n",
    "\n",
    "\n",
    "output_file_path = 'data/ISTQB_CTFL_Syllabus-v4.0_v08.txt'\n",
    "with open(output_file_path, 'w', encoding='utf-8') as out_file:\n",
    "    # Writing the extracted text to the output file\n",
    "    out_file.write(extracted_text)\n",
    "# \n",
    "# Closing the stream\n",
    "output_string.close()\n",
    "# \n",
    "# Printing message to indicate that the text has been saved to the file\n",
    "print(f\"Extracted text for 'The Certified Tester Foundation Level in Software Testing' pre processed version 0.8 saved to '{output_file_path}'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted text for 'The Certified Tester Foundation Level in Software Testing' pre processed version 0.9 saved to 'data/ISTQB_CTFL_Syllabus-v4.0_v09.txt'\n"
     ]
    }
   ],
   "source": [
    "# remove chapter 3 beginning\n",
    "# Define the regular expression pattern for the text to remove\n",
    "pattern = r'3\\. Static Testing – 80 minutes.*?FL-3\\.2\\.5 \\(K1\\) Recall the factors that contribute to a successful review'\n",
    "\n",
    "# Use re.sub to replace the matched text with a marker (e.g., 'REMOVED')\n",
    "extracted_text = re.sub(pattern, \"\", extracted_text, flags=re.DOTALL)\n",
    "\n",
    "# Define the phrase you want to remove\n",
    "phrase_to_remove = \"Learning Objectives for Chapter 4:\"\n",
    "\n",
    "# Replace the phrase with an empty string\n",
    "extracted_text = extracted_text.replace(phrase_to_remove, \"\")\n",
    "# \n",
    "output_file_path = 'data/ISTQB_CTFL_Syllabus-v4.0_v09.txt'\n",
    "with open(output_file_path, 'w', encoding='utf-8') as out_file:\n",
    "    # Writing the extracted text to the output file\n",
    "    out_file.write(extracted_text)\n",
    "# \n",
    "# Closing the stream\n",
    "output_string.close()\n",
    "# \n",
    "# Printing message to indicate that the text has been saved to the file\n",
    "print(f\"Extracted text for 'The Certified Tester Foundation Level in Software Testing' pre processed version 0.9 saved to '{output_file_path}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted text for 'The Certified Tester Foundation Level in Software Testing' pre processed version 0.10 saved to 'data/ISTQB_CTFL_Syllabus-v4.0_v10.txt'\n"
     ]
    }
   ],
   "source": [
    "# remove chapter 2 beginning\n",
    "\n",
    "# Define the regular expression pattern for the text to remove\n",
    "pattern = r'2\\. Testing Throughout the Software Development Lifecycle.*?FL-2\\.3\\.1 \\(K2\\) Summarize maintenance testing and its triggers'\n",
    "# Use re.sub to replace the matched text with a marker (e.g., 'REMOVED')\n",
    "extracted_text = re.sub(pattern, \"\", extracted_text, flags=re.DOTALL)\n",
    "\n",
    "# Define the phrase you want to remove\n",
    "phrase_to_remove = \"Learning Objectives for Chapter 4:\"\n",
    "\n",
    "# Replace the phrase with an empty string\n",
    "extracted_text = extracted_text.replace(phrase_to_remove, \"\")\n",
    "# \n",
    "output_file_path = 'data/ISTQB_CTFL_Syllabus-v4.0_v10.txt'\n",
    "with open(output_file_path, 'w', encoding='utf-8') as out_file:\n",
    "    # Writing the extracted text to the output file\n",
    "    out_file.write(extracted_text)\n",
    "# \n",
    "# Closing the stream\n",
    "output_string.close()\n",
    "# \n",
    "# Printing message to indicate that the text has been saved to the file\n",
    "print(f\"Extracted text for 'The Certified Tester Foundation Level in Software Testing' pre processed version 0.10 saved to '{output_file_path}'\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted text for 'The Certified Tester Foundation Level in Software Testing' pre processed version 0.11 saved to 'data/ISTQB_CTFL_Syllabus-v4.0_v11.txt'\n"
     ]
    }
   ],
   "source": [
    "# remove chapter 2 beginning\n",
    "\n",
    "# Define the regular expression pattern for the text to remove\n",
    "\n",
    "pattern = r'5\\. Managing the Test Activities – 335 minutes.*?FL-5\\.5\\.1 \\(K3\\) Prepare a defect report'\n",
    "# Use re.sub to replace the matched text with a marker (e.g., 'REMOVED')\n",
    "extracted_text = re.sub(pattern, \"\", extracted_text, flags=re.DOTALL)\n",
    "\n",
    "# Define the phrase you want to remove\n",
    "phrase_to_remove = \"Learning Objectives for Chapter 4:\"\n",
    "\n",
    "# Replace the phrase with an empty string\n",
    "extracted_text = extracted_text.replace(phrase_to_remove, \"\")\n",
    "# \n",
    "output_file_path = 'data/ISTQB_CTFL_Syllabus-v4.0_v11.txt'\n",
    "with open(output_file_path, 'w', encoding='utf-8') as out_file:\n",
    "    # Writing the extracted text to the output file\n",
    "    out_file.write(extracted_text)\n",
    "# \n",
    "# Closing the stream\n",
    "output_string.close()\n",
    "# \n",
    "# Printing message to indicate that the text has been saved to the file\n",
    "print(f\"Extracted text for 'The Certified Tester Foundation Level in Software Testing' pre processed version 0.11 saved to '{output_file_path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#pattern = r'\\d+\\.\\d+\\.\\d+\\.'\n",
    "#matches = re.findall(pattern, extracted_text)\n",
    "\n",
    "#for match in matches:\n",
    "#    match_without_dot = match[:-1]  # Remove the last dot\n",
    "#    print(match_without_dot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted text for 'The Certified Tester Foundation Level in Software Testing' pre processed version 0.12 saved to 'data/ISTQB_CTFL_Syllabus-v4.0_v12.txt'\n"
     ]
    }
   ],
   "source": [
    "# Remove bullet points using regular expressions\n",
    "extracted_text = re.sub(r'•', '', extracted_text)\n",
    "\n",
    "output_file_path = 'data/ISTQB_CTFL_Syllabus-v4.0_v12.txt'\n",
    "with open(output_file_path, 'w', encoding='utf-8') as out_file:\n",
    "    # Writing the extracted text to the output file\n",
    "    out_file.write(extracted_text)\n",
    "# \n",
    "# Closing the stream\n",
    "output_string.close()\n",
    "# \n",
    "# Printing message to indicate that the text has been saved to the file\n",
    "print(f\"Extracted text for 'The Certified Tester Foundation Level in Software Testing' pre processed version 0.12 saved to '{output_file_path}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Section Title: 1.1\n",
      "Section Content: What is Testing? Software systems are an integral part of our daily life. Most people have had experience with software that did not work as expected. Software that does not work correctly can lead to many problems including loss of money time business reputation and extreme cases even injury death. Software testing assesses software quality and helps reducing risk of software failure operation. Software testing is a set of activities to discover defects and evaluate quality of software artifacts. These artifacts when being tested are known as test objects. A common misconception about testing is that it only consists of executing tests . However software testing also includes other activities and must be aligned with software development lifecycle . Another common misconception about testing is that testing focuses entirely on verifying test object. Whilst testing involves verification i.e. checking whether system meets specified requirements it also involves validation which means checking whether system meets users’ and other stakeholders’ needs its operational environment. Testing may be dynamic static. Dynamic testing involves execution of software while static testing does not. Static testing includes reviews and static analysis. Dynamic testing uses different types of test techniques and test approaches to derive test cases . Testing is not only a technical activity. It also needs to be properly planned managed estimated monitored and controlled . Testers use tools but it is important to remember that testing is largely an intellectual activity requiring testers to have specialized knowledge use analytical skills and apply critical thinking and systems thinking . ISO/IEC/IEEE 29119-1 standard provides further information about software testing concepts.\n",
      "----------------------------------------\n",
      "Section Title: 1.1.2\n",
      "Section Content: Testing and Debugging Testing and debugging are separate activities. Testing can trigger failures that are caused by defects software can directly find defects test object . When dynamic testing triggers a failure debugging is concerned with finding causes of this failure analyzing these causes and eliminating them. typical debugging process this case involves: Reproduction of a failure Diagnosis Fixing cause Subsequent confirmation testing checks whether fixes resolved problem. Preferably confirmation testing is done by same person who performed initial test. Subsequent regression testing can also be performed to check whether fixes are causing failures other parts of test object . When static testing identifies a defect debugging is concerned with removing it. There is no need for reproduction diagnosis since static testing directly finds defects and cannot cause failures .\n",
      "----------------------------------------\n",
      "Section Title: 1.2.1\n",
      "Section Content: Testing’s Contributions to Success Testing provides a cost-effective means of detecting defects. These defects can then be removed so testing indirectly contributes to higher quality test objects. Testing provides a means of directly evaluating quality of a test object at various stages SDLC. These measures are used as part of a larger project management activity contributing to decisions to move to next stage of SDLC such as release decision. Testing provides users with indirect representation on development project. Testers ensure that their understanding of users’ needs are considered throughout development lifecycle. alternative is to involve a representative set of users as part of development project which is not usually possible due to high costs and lack of availability of suitable users. Testing may also be required to meet contractual legal requirements to comply with regulatory standards.\n",
      "----------------------------------------\n",
      "Section Title: 1.2.3\n",
      "Section Content: Errors Defects Failures and Root Causes Human beings make errors which produce defects which turn may result failures. Humans make errors for various reasons such as time pressure complexity of work products processes infrastructure interactions simply because they are tired lack adequate training. Defects can be found documentation such as a requirements specification a test script source code a supporting artifact such as a build file. Defects artifacts produced earlier SDLC if undetected often lead to defective artifacts later lifecycle. If a defect code is executed system may fail to do what it should do do something it shouldn’t causing a failure. Some defects will always result a failure if executed while others will only result a failure specific circumstances and some may never result a failure. Errors and defects are not only cause of failures. Failures can also be caused by environmental conditions such as when radiation electromagnetic field cause defects firmware. A root cause is a fundamental reason for occurrence of a problem . Root causes are identified through root cause analysis which is typically performed when a failure occurs a defect is identified. It is believed that further similar failures defects can be prevented their frequency reduced by addressing root cause such as by removing it.\n",
      "----------------------------------------\n",
      "Section Title: 1.4\n",
      "Section Content: Test Activities Testware and Test Roles Testing is context dependent but at a high level there are common sets of test activities without which testing is less likely to achieve test objectives. These sets of test activities form a test process. test process can be tailored to a given situation based on various factors. Which test activities are included this test process how they are implemented and when they occur is normally decided as part of test planning for specific situation . following sections describe general aspects of this test process terms of test activities and tasks impact of context testware traceability between test basis and testware and testing roles. ISO/IEC/IEEE 29119-2 standard provides further information about test processes.\n",
      "----------------------------------------\n",
      "Section Title: 5.1\n",
      "Section Content: Test monitoring and control. Test monitoring involves ongoing checking of all test activities and comparison of actual progress against plan. Test control involves taking actions necessary to meet objectives of testing. Test monitoring and control are further explained section\n",
      "----------------------------------------\n",
      "Section Title: 1.4.2\n",
      "Section Content: Test Process Context Testing is not performed isolation. Test activities are an integral part of development processes carried out within an organization. Testing is also funded by stakeholders and its final goal is to help fulfill stakeholders’ business needs. Therefore way testing is carried out will depend on a number of contextual factors including: Stakeholders Team members Business domain Technical factors Project constraints Organizational factors Software development lifecycle Tools These factors will have an impact on many test-related issues including: test strategy test techniques used degree of test automation required level of coverage level of detail of test documentation reporting etc.\n",
      "----------------------------------------\n",
      "Section Title: 1.4.1\n",
      "Section Content: There is a significant variation how different organizations produce shape name organize and manage their v4.0 Page of 74 2023-04-21 work products. Proper configuration management ensures consistency and integrity of work products. following list of work products is not exhaustive: Test planning work products include: test plan test schedule risk register and entry and exit criteria . Risk register is a list of risks together with risk likelihood risk impact and information about risk mitigation . Test schedule risk register and entry and exit criteria are often a part of test plan. Test monitoring and control work products include: test progress reports documentation of control directives and risk information . Test analysis work products include: test conditions and defect reports regarding defects test basis . Test design work products include: test cases test charters coverage items test data requirements and test environment requirements. Test implementation work products include: test procedures automated test scripts test suites test data test execution schedule and test environment elements. Examples of test environment elements include: stubs drivers simulators and service virtualizations. Test execution work products include: test logs and defect reports . Test completion work products include: test completion report action items for improvement of subsequent projects iterations documented lessons learned and change requests .\n",
      "----------------------------------------\n",
      "Section Title: 1.4.5\n",
      "Section Content: Roles Testing this syllabus two principal roles testing are covered: a test management role and a testing role. activities and tasks assigned to these two roles depend on factors such as project and product context skills of people roles and organization. test management role takes overall responsibility for test process test team and leadership of test activities. test management role is mainly focused on activities of test planning test monitoring and control and test completion. way which test management role is carried out varies depending on context. For example Agile software development some of test management tasks may be handled by Agile team. Tasks that span multiple teams entire organization may be performed by test managers outside of development team. testing role takes overall responsibility for engineering aspect of testing. testing role is mainly focused on activities of test analysis test design test implementation and test execution. Different people may take on these roles at different times. For example test management role can be performed by a team leader by a test manager by a development manager etc. It is also possible for one person to take on roles of testing and test management at same time.\n",
      "----------------------------------------\n",
      "Section Title: 1.5.1\n",
      "Section Content: Generic Skills Required for Testing While being generic following skills are particularly relevant for testers: Testing knowledge Thoroughness carefulness curiosity attention to details being methodical Good communication skills active listening being a team player Analytical thinking critical thinking creativity Technical knowledge Domain knowledge Testers are often bearers of bad news. It is a common human trait to blame bearer of bad news. This makes communication skills crucial for testers. Communicating test results may be perceived as criticism of product and of its author. Confirmation bias can make it difficult to accept information that disagrees with currently held beliefs. Some people may perceive testing as a destructive activity even though it contributes greatly to project success and product quality. To try to improve this view information about defects and failures should be communicated a constructive way. v4.0 Page of 74 2023-04-21\n",
      "----------------------------------------\n",
      "Section Title: 1.5.3\n",
      "Section Content: Independence of Testing A certain degree of independence makes tester more effective at finding defects due to differences between author’s and tester’s cognitive biases . Independence is not however a replacement for familiarity e.g. developers can efficiently find many defects their own code. Work products can be tested by their author by author's peers from same team by testers from outside author's team but within organization by testers from outside organization . For most projects it is usually best to carry out testing with multiple levels of independence . main benefit of independence of testing is that independent testers are likely to recognize different kinds of failures and defects compared to developers because of their different backgrounds technical perspectives and biases. Moreover an independent tester can verify challenge disprove assumptions made by stakeholders during specification and implementation of system. However there are also some drawbacks. Independent testers may be isolated from development team which may lead to a lack of collaboration communication problems an adversarial relationship with development team. Developers may lose a sense of responsibility for quality. Independent testers may be seen as a bottleneck be blamed for delays release. v4.0 Page of 74 2023-04-21 2. Testing Throughout Software Development Lifecycle – 130 minutes Keywords acceptance testing black-box testing component integration testing component testing confirmation testing functional testing integration testing maintenance testing non-functional testing regression testing shift-left system integration testing system testing test level test object test type white-box testing Learning Objectives for Chapter 2: 2.1 Testing Context of a Software Development Lifecycle FL-\n",
      "----------------------------------------\n",
      "Section Title: 2.1\n",
      "Section Content: 2 Recall good testing practices that apply to all software development lifecycles FL-\n",
      "----------------------------------------\n",
      "Section Title: 2.1\n",
      "Section Content: 4 Summarize how DevOps might have an impact on testing FL-\n",
      "----------------------------------------\n",
      "Section Title: 2.1\n",
      "Section Content: 6 Explain how retrospectives can be used as a mechanism for process improvement 2.2 Test Levels and Test Types FL-\n",
      "----------------------------------------\n",
      "Section Title: 2.2\n",
      "Section Content: 2 Distinguish different test types FL-\n",
      "----------------------------------------\n",
      "Section Title: 2.3\n",
      "Section Content: 1 Summarize maintenance testing and its triggers v4.0 Page of 74 2023-04-21\n",
      "----------------------------------------\n",
      "Section Title: 2.1.1\n",
      "Section Content: Impact of Software Development Lifecycle on Testing Testing must be adapted to SDLC to succeed. choice of SDLC impacts on the: Scope and timing of test activities Level of detail of test documentation Choice of test techniques and test approach Extent of test automation Role and responsibilities of a tester sequential development models initial phases testers typically participate requirement reviews test analysis and test design. executable code is usually created later phases so typically dynamic testing cannot be performed early SDLC. some iterative and incremental development models it is assumed that each iteration delivers a working prototype product increment. This implies that each iteration both static and dynamic testing may be performed at all test levels. Frequent delivery of increments requires fast feedback and extensive regression testing. Agile software development assumes that change may occur throughout project. Therefore lightweight work product documentation and extensive test automation to make regression testing easier are favored agile projects. Also most of manual testing tends to be done using experience-based test techniques that do not require extensive prior test analysis and design.\n",
      "----------------------------------------\n",
      "Section Title: 2.1.3\n",
      "Section Content: Testing as a Driver for Software Development TDD ATDD and BDD are similar development approaches where tests are defined as a means of directing development. Each of these approaches implements principle of early testing and follows a shift-left approach since tests are defined before code is written. They support an iterative development model. These approaches are characterized as follows: Test-Driven Development : Directs coding through test cases Tests are written first then code is written to satisfy tests and then tests and code are refactored Acceptance Test-Driven Development : Derives tests from acceptance criteria as part of system design process Tests are written before part of application is developed to satisfy tests Behavior-Driven Development : Expresses desired behavior of an application with test cases written a simple form of natural language which is easy to understand by stakeholders – usually using Given/When/Then format. Test cases are then automatically translated into executable tests For all above approaches tests may persist as automated tests to ensure code quality future adaptions / refactoring.\n",
      "----------------------------------------\n",
      "Section Title: 2.1.5\n",
      "Section Content: Shift-Left Approach principle of early testing is sometimes referred to as shift-left because it is an approach where testing is performed earlier SDLC. Shift-left normally suggests that testing should be done earlier but it does not mean that testing later SDLC should be neglected. There are some good practices that illustrate how to achieve a “shift-left” testing which include: Reviewing specification from perspective of testing. These review activities on specifications often find potential defects such as ambiguities incompleteness and inconsistencies Writing test cases before code is written and have code run a test harness during code implementation Using CI and even better CD as it comes with fast feedback and automated component tests to accompany source code when it is submitted to code repository Completing static analysis of source code prior to dynamic testing as part of an automated process Performing non-functional testing starting at component test level where possible. This is a form of shift-left as these non-functional test types tend to be performed later SDLC when a complete system and a representative test environment are available A shift-left approach might result extra training effort and/or costs earlier process but is expected to save efforts and/or costs later process. For shift-left approach it is important that stakeholders are convinced and bought into this concept.\n",
      "----------------------------------------\n",
      "Section Title: 2.2\n",
      "Section Content: Test Levels and Test Types Test levels are groups of test activities that are organized and managed together. Each test level is an instance of test process performed relation to software at a given stage of development from individual components to complete systems where applicable systems of systems. Test levels are related to other activities within SDLC. sequential SDLC models test levels are often defined such that exit criteria of one level are part of entry criteria for next level. some iterative models this may not apply. Development activities may span through multiple test levels. Test levels may overlap time. Test types are groups of test activities related to specific quality characteristics and most of those test activities can be performed at every test level.\n",
      "----------------------------------------\n",
      "Section Title: 2.2.2\n",
      "Section Content: Test Types A lot of test types exist and can be applied projects. this syllabus following four test types are addressed: Functional testing evaluates functions that a component system should perform. functions are “what” test object should do. main objective of functional testing is checking functional completeness functional correctness and functional appropriateness. Non-functional testing evaluates attributes other than functional characteristics of a component system. Non-functional testing is testing of “how well system behaves”. main objective of non- functional testing is checking non-functional software quality characteristics. ISO/IEC 25010 standard provides following classification of non-functional software quality characteristics: Performance efficiency Compatibility Usability Reliability Security Maintainability Portability It is sometimes appropriate for non-functional testing to start early life cycle . Many non-functional tests are derived from functional tests as v4.0 Page of 74 2023-04-21 they use same functional tests but check that while performing function a non-functional constraint is satisfied . late discovery of non-functional defects can pose a serious threat to success of a project. Non-functional testing sometimes needs a very specific test environment such as a usability lab for usability testing. Black-box testing is specification-based and derives tests from documentation external to test object. main objective of black-box testing is checking system's behavior against its specifications. White-box testing is structure-based and derives tests from system's implementation internal structure . main objective of white-box testing is to cover underlying structure by tests to acceptable level. All four above mentioned test types can be applied to all test levels although focus will be different at each level. Different test techniques can be used to derive test conditions and test cases for all mentioned test types.\n",
      "----------------------------------------\n",
      "Section Title: 2.3\n",
      "Section Content: Maintenance Testing There are different categories of maintenance it can be corrective adaptive to changes environment improve performance maintainability so maintenance can involve planned releases/deployments and unplanned releases/deployments . Impact analysis may be done before a change is made to help decide if change should be made based on potential consequences other areas of system. Testing changes to a system production includes both evaluating success of implementation of change and checking for possible regressions parts of system that remain unchanged . scope of maintenance testing typically depends on: degree of risk of change size of existing system size of change triggers for maintenance and maintenance testing can be classified as follows: Modifications such as planned enhancements corrective changes hot fixes. Upgrades migrations of operational environment such as from one platform to another which can require tests associated with new environment as well as of changed software tests of data conversion when data from another application is migrated into system being maintained. Retirement such as when an application reaches end of its life. When a system is retired this can require testing of data archiving if long data-retention periods are required. Testing of restore and retrieval procedures after archiving may also be needed event that certain data is required during archiving period. v4.0 Page of 74 2023-04-21 3. Static Testing – 80 minutes Keywords anomaly dynamic testing formal review informal review inspection review static analysis static testing technical review walkthrough Learning Objectives for Chapter 3: 3.1 Static Testing Basics FL-\n",
      "----------------------------------------\n",
      "Section Title: 3.1\n",
      "Section Content: 2 Explain value of static testing FL-\n",
      "----------------------------------------\n",
      "Section Title: 3.2\n",
      "Section Content: 1 Identify benefits of early and frequent stakeholder feedback FL-\n",
      "----------------------------------------\n",
      "Section Title: 3.2\n",
      "Section Content: 3 Recall which responsibilities are assigned to principal roles when performing reviews FL-\n",
      "----------------------------------------\n",
      "Section Title: 3.2\n",
      "Section Content: 5 Recall factors that contribute to a successful review v4.0 Page of 74 2023-04-21\n",
      "----------------------------------------\n",
      "Section Title: 3.1.1\n",
      "Section Content: Work Products Examinable by Static Testing Almost any work product can be examined using static testing. Examples include requirement specification documents source code test plans test cases product backlog items test charters project documentation contracts and models. Any work product that can be read and understood can be subject of a review. However for static analysis work products need a structure against which they can be checked . Work products that are not appropriate for static testing include those that are difficult to interpret by human beings and that should not be analyzed by tools .\n",
      "----------------------------------------\n",
      "Section Title: 3.1.3\n",
      "Section Content: Differences between Static Testing and Dynamic Testing Static testing and dynamic testing practices complement each other. They have similar objectives such as supporting detection of defects work products but there are also some differences such as: Static and dynamic testing can both lead to detection of defects however there are some defect types that can only be found by either static dynamic testing. Static testing finds defects directly while dynamic testing causes failures from which associated defects are determined through subsequent analysis Static testing may more easily detect defects that lay on paths through code that are rarely executed hard to reach using dynamic testing Static testing can be applied to non-executable work products while dynamic testing can only be applied to executable work products Static testing can be used to measure quality characteristics that are not dependent on executing code while dynamic testing can be used to measure quality characteristics that are dependent on executing code Typical defects that are easier and/or cheaper to find through static testing include: Defects requirements Design defects Certain types of coding defects Deviations from standards Incorrect interface specifications Specific types of security vulnerabilities Gaps inaccuracies test basis coverage\n",
      "----------------------------------------\n",
      "Section Title: 3.2.1\n",
      "Section Content: Benefits of Early and Frequent Stakeholder Feedback Early and frequent feedback allows for early communication of potential quality problems. If there is little stakeholder involvement during SDLC product being developed might not meet stakeholder’s original current vision. A failure to deliver what stakeholder wants can result costly rework missed deadlines blame games and might even lead to complete project failure. v4.0 Page of 74 2023-04-21 Frequent stakeholder feedback throughout SDLC can prevent misunderstandings about requirements and ensure that changes to requirements are understood and implemented earlier. This helps development team to improve their understanding of what they are building. It allows them to focus on those features that deliver most value to stakeholders and that have most positive impact on identified risks.\n",
      "----------------------------------------\n",
      "Section Title: 3.2.3\n",
      "Section Content: Roles and Responsibilities Reviews Reviews involve various stakeholders who may take on several roles. principal roles and their responsibilities are: Manager – decides what is to be reviewed and provides resources such as staff and time for review Author – creates and fixes work product under review v4.0 Page of 74 2023-04-21 Moderator – ensures effective running of review meetings including mediation time management and a safe review environment which everyone can speak freely Scribe – collates anomalies from reviewers and records review information such as decisions and new anomalies found during review meeting Reviewer – performs reviews. A reviewer may be someone working on project a subject matter expert any other stakeholder Review leader – takes overall responsibility for review such as deciding who will be involved and organizing when and where review will take place Other more detailed roles are possible as described ISO/IEC 20246 standard.\n",
      "----------------------------------------\n",
      "Section Title: 3.2.5\n",
      "Section Content: Success Factors for Reviews There are several factors that determine success of reviews which include: v4.0 Page of 74 2023-04-21 Defining clear objectives and measurable exit criteria. Evaluation of participants should never be an objective Choosing appropriate review type to achieve given objectives and to suit type of work product review participants project needs and context Conducting reviews on small chunks so that reviewers do not lose concentration during an individual review and/or review meeting Providing feedback from reviews to stakeholders and authors so they can improve product and their activities Providing adequate time to participants to prepare for review Support from management for review process Making reviews part of organization’s culture to promote learning and process improvement Providing adequate training for all participants so they know how to fulfil their role Facilitating meetings v4.0 Page of 74 2023-04-21 4. Test Analysis and Design – 390 minutes Keywords acceptance criteria acceptance test-driven development black-box test technique boundary value analysis branch coverage checklist-based testing collaboration-based test approach coverage coverage item decision table testing equivalence partitioning error guessing experience-based test technique exploratory testing state transition testing statement coverage test technique white-box test technique 4.1 Test Techniques Overview FL-\n",
      "----------------------------------------\n",
      "Section Title: 4.2\n",
      "Section Content: 1 Use equivalence partitioning to derive test cases FL-\n",
      "----------------------------------------\n",
      "Section Title: 4.2\n",
      "Section Content: 3 Use decision table testing to derive test cases FL-\n",
      "----------------------------------------\n",
      "Section Title: 4.3\n",
      "Section Content: 1 Explain statement testing FL-\n",
      "----------------------------------------\n",
      "Section Title: 4.3\n",
      "Section Content: 3 Explain value of white-box testing 4.4 Experience-based Test Techniques FL-\n",
      "----------------------------------------\n",
      "Section Title: 4.4\n",
      "Section Content: 2 Explain exploratory testing FL-\n",
      "----------------------------------------\n",
      "Section Title: 4.5\n",
      "Section Content: Collaboration-based Test Approaches FL-\n",
      "----------------------------------------\n",
      "Section Title: 4.5\n",
      "Section Content: 2 Classify different options for writing acceptance criteria FL-\n",
      "----------------------------------------\n",
      "Section Title: 4.1\n",
      "Section Content: Test Techniques Overview Test techniques support tester test analysis and test design . Test techniques help to develop a relatively small but sufficient set of test cases a systematic way. Test techniques also help tester to define test conditions identify coverage items and identify test data during test analysis and design. Further information on test techniques and their corresponding measures can be found ISO/IEC/IEEE 29119-4 standard and . this syllabus test techniques are classified as black-box white-box and experience-based. Black-box test techniques are based on an analysis of specified behavior of test object without reference to its internal structure. Therefore test cases are independent of how software is implemented. Consequently if implementation changes but required behavior stays same then test cases are still useful. White-box test techniques are based on an analysis of test object’s internal structure and processing. As test cases are dependent on how software is designed they can only be created after design implementation of test object. Experience-based test techniques effectively use knowledge and experience of testers for design and implementation of test cases. effectiveness of these techniques depends heavily on tester’s skills. Experience-based test techniques can detect defects that may be missed using black- box and white-box test techniques. Hence experience-based test techniques are complementary to black-box and white-box test techniques.\n",
      "----------------------------------------\n",
      "Section Title: 4.2.1\n",
      "Section Content: Equivalence Partitioning Equivalence Partitioning divides data into partitions based on expectation that all elements of a given partition are to be processed same way by test object. theory behind this technique is that if a test case that tests one value from an equivalence partition detects a defect this defect should also be detected by test cases that test any other value from same partition. Therefore one test for each partition is sufficient. Equivalence partitions can be identified for any data element related to test object including inputs outputs configuration items internal values time-related values and interface parameters. partitions may be continuous discrete ordered unordered finite infinite. partitions must not overlap and must be non-empty sets. For simple test objects EP can be easy but practice understanding how test object will treat different values is often complicated. Therefore partitioning should be done with care. v4.0 Page of 74 2023-04-21 A partition containing valid values is called a valid partition. A partition containing invalid values is called an invalid partition. definitions of valid and invalid values may vary among teams and organizations. For example valid values may be interpreted as those that should be processed by test object as those for which specification defines their processing. Invalid values may be interpreted as those that should be ignored rejected by test object as those for which no processing is defined test object specification. EP coverage items are equivalence partitions. To achieve 100% coverage with this technique test cases must exercise all identified partitions by covering each partition at least once. Coverage is measured as number of partitions exercised by at least one test case divided by total number of identified partitions and is expressed as a percentage. Many test objects include multiple sets of partitions which means that a test case will cover partitions from different sets of partitions. simplest coverage criterion case of multiple sets of partitions is called Each Choice coverage . Each Choice coverage requires test cases to exercise each partition from each set of partitions at least once. Each Choice coverage does not take into account combinations of partitions.\n",
      "----------------------------------------\n",
      "Section Title: 4.2.3\n",
      "Section Content: Decision Table Testing Decision tables are used for testing implementation of system requirements that specify how different combinations of conditions result different outcomes. Decision tables are an effective way of recording complex logic such as business rules. When creating decision tables conditions and resulting actions of system are defined. These form rows of table. Each column corresponds to a decision rule that defines a unique combination of conditions along with associated actions. limited-entry decision tables all values of conditions and actions are shown as Boolean values . Alternatively extended-entry decision tables some all conditions and actions may also take on multiple values . notation for conditions is as follows: “T” means that condition is satisfied. “F” means that condition is not satisfied. “–” means that value of condition is irrelevant for action outcome. “N/A” means that condition is infeasible for a given rule. For actions: “X” means that action should occur. Blank means that action should not occur. Other notations may also be used. A full decision table has enough columns to cover every combination of conditions. table can be simplified by deleting columns containing infeasible combinations of conditions. table can also be minimized by merging columns which some conditions do not affect outcome into a single column. Decision table minimization algorithms are out of scope of this syllabus. decision table testing coverage items are columns containing feasible combinations of conditions. To achieve 100% coverage with this technique test cases must exercise all these columns. Coverage is measured as number of exercised columns divided by total number of feasible columns and is expressed as a percentage. strength of decision table testing is that it provides a systematic approach to identify all combinations of conditions some of which might otherwise be overlooked. It also helps to find any gaps contradictions requirements. If there are many conditions exercising all decision rules may be time consuming since number of rules grows exponentially with number of conditions. such a case to reduce number of rules that need to be exercised a minimized decision table a risk- based approach may be used.\n",
      "----------------------------------------\n",
      "Section Title: 4.3\n",
      "Section Content: White-Box Test Techniques Because of their popularity and simplicity this section focuses on two code-related white-box test techniques: Statement testing Branch testing There are more rigorous techniques that are used some safety-critical mission-critical high-integrity environments to achieve more thorough code coverage. There are also white-box test techniques used higher test levels using coverage not related to code . These techniques are not discussed this syllabus.\n",
      "----------------------------------------\n",
      "Section Title: 4.3.2\n",
      "Section Content: Branch Testing and Branch Coverage A branch is a transfer of control between two nodes control flow graph which shows possible sequences which source code statements are executed test object. Each transfer of control can be either unconditional conditional . branch testing coverage items are branches and aim is to design test cases to exercise branches code until an acceptable level of coverage is achieved. Coverage is measured as number of branches exercised by test cases divided by total number of branches and is expressed as a percentage. When 100% branch coverage is achieved all branches code unconditional and conditional are exercised by test cases. Conditional branches typically correspond to a true false outcome from an “if...then” decision an outcome from a switch/case statement a decision to exit continue a loop. However exercising a branch with a test case will not detect defects all cases. For example it may not detect defects requiring execution of a specific path a code. Branch coverage subsumes statement coverage. This means that any set of test cases achieving 100% branch coverage also achieves 100% statement coverage .\n",
      "----------------------------------------\n",
      "Section Title: 4.4\n",
      "Section Content: Experience-based Test Techniques Commonly used experience-based test techniques discussed following sections are: Error guessing Exploratory testing Checklist-based testing\n",
      "----------------------------------------\n",
      "Section Title: 4.4.2\n",
      "Section Content: Exploratory Testing exploratory testing tests are simultaneously designed executed and evaluated while tester learns about test object. testing is used to learn more about test object to explore it more deeply with focused tests and to create tests for untested areas. Exploratory testing is sometimes conducted using session-based testing to structure testing. a session-based approach exploratory testing is conducted within a defined time-box. tester uses a test charter containing test objectives to guide testing. test session is usually followed by a debriefing that involves a discussion between tester and stakeholders interested test results of test session. this approach test objectives may be treated as high-level test conditions. Coverage items are identified and exercised during test session. tester may use test session sheets to document steps followed and discoveries made. Exploratory testing is useful when there are few inadequate specifications there is significant time pressure on testing. Exploratory testing is also useful to complement other more formal test techniques. Exploratory testing will be more effective if tester is experienced has domain knowledge and has a high degree of essential skills like analytical skills curiosity and creativeness . Exploratory testing can incorporate use of other test techniques . More information about exploratory testing can be found .\n",
      "----------------------------------------\n",
      "Section Title: 4.5\n",
      "Section Content: Collaboration-based Test Approaches Each of above-mentioned techniques has a particular objective with respect to defect detection. Collaboration-based approaches on other hand focus also on defect avoidance by collaboration and communication.\n",
      "----------------------------------------\n",
      "Section Title: 4.5.2\n",
      "Section Content: Acceptance Criteria Acceptance criteria for a user story are conditions that an implementation of user story must meet to be accepted by stakeholders. From this perspective acceptance criteria may be viewed as test conditions that should be exercised by tests. Acceptance criteria are usually a result of Conversation . Acceptance criteria are used to: Define scope of user story Reach consensus among stakeholders Describe both positive and negative scenarios Serve as a basis for user story acceptance testing v4.0 Page of 74 2023-04-21 Allow accurate planning and estimation There are several ways to write acceptance criteria for a user story. two most common formats are: Scenario-oriented Rule-oriented Most acceptance criteria can be documented one of these two formats. However team may use another custom format as long as acceptance criteria are well-defined and unambiguous.\n",
      "----------------------------------------\n",
      "Section Title: 5.1\n",
      "Section Content: 1 Exemplify purpose and content of a test plan FL-\n",
      "----------------------------------------\n",
      "Section Title: 5.1\n",
      "Section Content: 3 Compare and contrast entry criteria and exit criteria FL-\n",
      "----------------------------------------\n",
      "Section Title: 5.1\n",
      "Section Content: 5 Apply test case prioritization FL-\n",
      "----------------------------------------\n",
      "Section Title: 5.1\n",
      "Section Content: 7 Summarize testing quadrants and their relationships with test levels and test types 5.2 Risk Management FL-\n",
      "----------------------------------------\n",
      "Section Title: 5.2\n",
      "Section Content: 2 Distinguish between project risks and product risks FL-\n",
      "----------------------------------------\n",
      "Section Title: 5.2\n",
      "Section Content: 4 Explain what measures can be taken response to analyzed product risks 5.3 Test Monitoring Test Control and Test Completion FL-\n",
      "----------------------------------------\n",
      "Section Title: 5.3\n",
      "Section Content: 2 Summarize purposes content and audiences for test reports FL-\n",
      "----------------------------------------\n",
      "Section Title: 5.4\n",
      "Section Content: 1 Summarize how configuration management supports testing 5.5 Defect Management FL-\n",
      "----------------------------------------\n",
      "Section Title: 5.1\n",
      "Section Content: Test Planning\n",
      "----------------------------------------\n",
      "Section Title: 5.1.2\n",
      "Section Content: Tester's Contribution to Iteration and Release Planning iterative SDLCs typically two kinds of planning occur: release planning and iteration planning. Release planning looks ahead to release of a product defines and re-defines product backlog and may involve refining larger user stories into a set of smaller user stories. It also serves as basis for test approach and test plan across all iterations. Testers involved release planning participate writing testable user stories and acceptance criteria participate project and quality risk analyses estimate test effort associated with user stories determine test approach and plan testing for release. Iteration planning looks ahead to end of a single iteration and is concerned with iteration backlog. Testers involved iteration planning participate detailed risk analysis of user stories determine testability of user stories break down user stories into tasks estimate test effort for all testing tasks and identify and refine functional and non-functional aspects of test object. v4.0 Page of 74 2023-04-21\n",
      "----------------------------------------\n",
      "Section Title: 5.1.4\n",
      "Section Content: Estimation Techniques Test effort estimation involves predicting amount of test-related work needed to meet objectives of a test project. It is important to make it clear to stakeholders that estimate is based on a number of assumptions and is always subject to estimation error. Estimation for small tasks is usually more accurate than for large ones. Therefore when estimating a large task it can be decomposed into a set of smaller tasks which then turn can be estimated. this syllabus following four estimation techniques are described. Estimation based on ratios. this metrics-based technique figures are collected from previous projects within organization which makes it possible to derive “standard” ratios for similar projects. ratios of an organization’s own projects are generally best source to use estimation process. These standard ratios can then be used to estimate test effort for new project. For example if previous project development-to-test effort ratio was 3:2 and current project development effort is expected to be 600 person-days test effort can be estimated to be 400 person-days. Extrapolation. this metrics-based technique measurements are made as early as possible current project to gather data. Having enough observations effort required for remaining work can be approximated by extrapolating this data . This method is very suitable iterative SDLCs. For example team may extrapolate test effort forthcoming iteration as averaged effort from last three iterations. Wideband Delphi. this iterative expert-based technique experts make experience-based estimations. Each expert isolation estimates effort. results are collected and if there are deviations that are out of range of agreed upon boundaries experts discuss their current estimates. Each expert is then asked to make a new estimation based on that feedback again isolation. This process is repeated until a consensus is reached. Planning Poker is a variant of Wideband Delphi commonly used Agile v4.0 Page of 74 2023-04-21 software development. Planning Poker estimates are usually made using cards with numbers that represent effort size. Three-point estimation. this expert-based technique three estimations are made by experts: most optimistic estimation most likely estimation and most pessimistic estimation . final estimate is their weighted arithmetic mean. most popular version of this technique estimate is calculated as E = / 6. advantage of this technique is that it allows experts to calculate measurement error: SD = / 6. For example if estimates are: a=6 m=9 and b=18 then final estimation is 10±2 person-hours because E = / 6 = 10 and SD = / 6 = 2. See for these and many other test estimation techniques.\n",
      "----------------------------------------\n",
      "Section Title: 5.1.6\n",
      "Section Content: Test Pyramid test pyramid is a model showing that different tests may have different granularity. test pyramid model supports team test automation and test effort allocation by showing that different goals are supported by different levels of test automation. pyramid layers represent groups of tests. higher layer lower test granularity test isolation and test execution time. Tests bottom layer are small isolated fast and check a small piece of functionality so usually a lot of them are needed to achieve a reasonable coverage. top layer represents complex high-level end-to-end tests. These high-level tests are generally slower than tests from lower layers and they typically check a large piece of functionality so usually just a few of them are needed to achieve a reasonable coverage. number and naming of layers may differ. For example original test pyramid model defines three layers: “unit tests” “service tests” and “UI tests”. Another popular model defines unit v4.0 Page of 74 2023-04-21 tests integration tests and end-to-end tests. Other test levels can also be used.\n",
      "----------------------------------------\n",
      "Section Title: 5.2\n",
      "Section Content: Risk Management Organizations face many internal and external factors that make it uncertain whether and when they will achieve their objectives . Risk management allows organizations to increase likelihood of achieving objectives improve quality of their products and increase stakeholders’ confidence and trust. main risk management activities are: Risk analysis Risk control test approach which test activities are selected prioritized and managed based on risk analysis and risk control is called risk-based testing.\n",
      "----------------------------------------\n",
      "Section Title: 5.2.2\n",
      "Section Content: Project Risks and Product Risks software testing one is generally concerned with two types of risks: project risks and product risks. Project risks are related to management and control of project. Project risks include: Organizational issues People issues Technical issues Supplier issues Project risks when they occur may have an impact on project schedule budget scope which affects project's ability to achieve its objectives. Product risks are related to product quality characteristics . Examples of product risks include: missing wrong functionality incorrect calculations runtime errors poor architecture inefficient algorithms inadequate response time poor user experience security vulnerabilities. Product risks when they occur may result various negative consequences including: User dissatisfaction Loss of revenue trust reputation Damage to third parties High maintenance costs overload of helpdesk Criminal penalties extreme cases physical damage injuries even death\n",
      "----------------------------------------\n",
      "Section Title: 5.2.4\n",
      "Section Content: Product Risk Control Product risk control comprises all measures that are taken response to identified and assessed product risks. Product risk control consists of risk mitigation and risk monitoring. Risk mitigation involves implementing actions proposed risk assessment to reduce risk level. aim of risk monitoring is to ensure that mitigation actions are effective to obtain further information to improve risk assessment and to identify emerging risks. With respect to product risk control once a risk has been analyzed several response options to risk are possible e.g. risk mitigation by testing risk acceptance risk transfer contingency plan . Actions that can be taken to mitigate product risks by testing are as follows: Select testers with right level of experience and skills suitable for a given risk type Apply an appropriate level of independence of testing Conduct reviews and perform static analysis Apply appropriate test techniques and coverage levels Apply appropriate test types addressing affected quality characteristics Perform dynamic testing including regression testing\n",
      "----------------------------------------\n",
      "Section Title: 5.3.1\n",
      "Section Content: Metrics used Testing Test metrics are gathered to show progress against planned schedule and budget current quality of test object and effectiveness of test activities with respect to objectives an iteration goal. Test monitoring gathers a variety of metrics to support test control and test completion. Common test metrics include: Project progress metrics Test progress metrics Product quality metrics Defect metrics Risk metrics Coverage metrics Cost metrics\n",
      "----------------------------------------\n",
      "Section Title: 5.3.3\n",
      "Section Content: Communicating Status of Testing best means of communicating test status varies depending on test management concerns organizational test strategies regulatory standards case of self-organizing teams on team itself. options include: Verbal communication with team members and other stakeholders Dashboards Electronic communication channels Online documentation Formal test reports One more of these options can be used. More formal communication may be more appropriate for distributed teams where direct face-to-face communication is not always possible due to geographical distance time differences. Typically different stakeholders are interested different types of information so communication should be tailored accordingly.\n",
      "----------------------------------------\n",
      "Section Title: 5.5\n",
      "Section Content: Defect Management Since one of major test objectives is to find defects an established defect management process is essential. Although we refer to \"defects\" here reported anomalies may turn out to be real defects something else - this is resolved during process of dealing with defect reports. Anomalies may be reported during any phase of SDLC and form depends on SDLC. At a minimum defect management process includes a workflow for handling individual anomalies from their discovery to their closure and rules for their classification. workflow typically comprises activities to log reported anomalies analyze and classify them decide on a suitable response such as to fix keep it as it is and finally to close defect report. process must be followed by all involved stakeholders. It is advisable to handle defects from static testing a similar way. Typical defect reports have following objectives: Provide those responsible for handling and resolving reported defects with sufficient information to resolve issue Provide a means of tracking quality of work product Provide ideas for improvement of development and test process A defect report logged during dynamic testing typically includes: Unique identifier Title with a short summary of anomaly being reported Date when anomaly was observed issuing organization and author including their role Identification of test object and test environment Context of defect v4.0 Page 55 of 74 2023-04-21 Description of failure to enable reproduction and resolution including steps that detected anomaly and any relevant test logs database dumps screenshots recordings Expected results and actual results Severity of defect on interests of stakeholders requirements Priority to fix Status of defect References Some of this data may be automatically included when using defect management tools . Document templates for a defect report and example defect reports can be found ISO/IEC/IEEE 29119-3 standard which refers to defect reports as incident reports. v4.0\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Define a regular expression pattern to match section titles\n",
    "#section_pattern = r'\\d+\\.\\d+\\.\\d+\\.'\n",
    "\n",
    "# using combined reg. exp to extract 1.1.1. and 1.2.\n",
    "section_pattern_3d = r'\\d+\\.\\d+\\.\\d+\\.'  # Pattern for \"1.1.1.\"\n",
    "section_pattern_2d = r'\\d+\\.\\d+\\.'    # Pattern for \"1.2.\"\n",
    "combined_pattern = f\"({section_pattern_3d}|{section_pattern_2d})\"\n",
    "\n",
    "# Using re.finditer to find all section titles and their starting positions\n",
    "section_matches = re.finditer(combined_pattern, extracted_text)\n",
    "\n",
    "# Create lists to store sections\n",
    "sections = []\n",
    "\n",
    "# Iterate through section matches\n",
    "for match in section_matches:\n",
    "    start_pos = match.start()\n",
    "    end_pos = (\n",
    "        match.end()\n",
    "        if match.end() < len(extracted_text)\n",
    "        else len(extracted_text)\n",
    "    )\n",
    "    section_title = match.group().strip()\n",
    "    \n",
    "    # Remove the last dot from the section title\n",
    "    section_title = section_title[:-1]  # Remove the last dot\n",
    "    \n",
    "    try:\n",
    "        # Find the corresponding section content based on section title position\n",
    "        next_match = next(section_matches)\n",
    "        content_start = end_pos\n",
    "        content_end = (\n",
    "            next_match.start()\n",
    "            if content_start < len(extracted_text)\n",
    "            else len(extracted_text)\n",
    "        )\n",
    "        section_content = extracted_text[content_start:content_end].strip()\n",
    "    except StopIteration:\n",
    "        # Handle the case when there are no more matches\n",
    "        section_content = extracted_text[end_pos:].strip()\n",
    "    \n",
    "    sections.append((section_title, section_content))\n",
    "\n",
    "# Print the extracted sections\n",
    "if sections:\n",
    "    for section in sections:\n",
    "        print(\"Section Title:\", section[0])\n",
    "        print(\"Section Content:\", section[1])\n",
    "        print(\"-\" * 40)\n",
    "else:\n",
    "    print(\"No sections found in the text, you did something wrong check once more\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding name?\n",
    "\n",
    "#text = \"1.1.2. Testing and Debugging\\nTesting and debugging are separate activities. Testing can trigger failures that are caused by defects in the software (dynamic testing) or can directly find defects in the test object (static testing). When dynamic testing (see chapter 4) triggers a failure, debugging is concerned with finding causes of this failure (defects), analyzing these causes, and eliminating the\"\n",
    "\n",
    "# Define a regular expression pattern to match section titles and names\n",
    "# section_pattern = r'(\\d+\\.\\d+\\.\\d+\\.\\s[^\\n]+)'\n",
    "# \n",
    "#Using re.finditer to find all section titles and their starting positions\n",
    "# section_matches = re.finditer(section_pattern, extracted_text)\n",
    "# \n",
    "#Create lists to store sections\n",
    "# sections = []\n",
    "# \n",
    "#Iterate through section matches\n",
    "# for match in section_matches:\n",
    "    # section_info = match.group(1).strip()\n",
    "# \n",
    " #   Find the corresponding section content based on section title position\n",
    "    # start_pos = match.end()\n",
    "    # end_pos = (\n",
    "        # next(section_matches, None)\n",
    "        # if start_pos < len(text)\n",
    "        # else None\n",
    "    # )\n",
    "    # \n",
    "    # if end_pos:\n",
    "        # section_content = text[start_pos:end_pos.start()].strip()\n",
    "    # else:\n",
    "        # section_content = text[start_pos:].strip()\n",
    "# \n",
    "    # sections.append((section_info, section_content))\n",
    "# \n",
    "#Print the extracted sections\n",
    "# if sections:\n",
    "    # for section in sections:\n",
    "        # print(\"Section Info:\", section[0])\n",
    "        # print(\"Section Content:\", section[1])\n",
    "        # print(\"-\" * 40)\n",
    "# else:\n",
    "    # print(\"No sections found in the text.\")\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DRAFT\n",
    "\n",
    "# Sample text (replace this with your actual text)\n",
    "\n",
    "# Define a regular expression pattern to match section titles\n",
    "# section_pattern = r'\\b\\d+\\.\\d+(?:\\.\\d+)?(?=\\s)'\n",
    "# \n",
    "#Using re.finditer to find all section titles and their starting positions\n",
    "# section_matches = re.finditer(section_pattern, extracted_text)\n",
    "# \n",
    "#Create lists to store sections\n",
    "# sections = []\n",
    "# \n",
    "#Iterate through section matches\n",
    "# for match in section_matches:\n",
    "    # start_pos = match.start()\n",
    "    # end_pos = (\n",
    "        # match.end()\n",
    "        # if match.end() < len(extracted_text)\n",
    "        # else len(extracted_text)\n",
    "    # )\n",
    "    # section_title = match.group().strip()\n",
    "    # \n",
    "#   Remove the last dot from the section title\n",
    "    # section_title = section_title[:-1]  # Remove the last dot\n",
    "    # \n",
    "#    Find the corresponding section content based on section title position\n",
    "    # content_start = end_pos\n",
    "    # content_end = (\n",
    "        # next(section_matches).start()\n",
    "        # if content_start < len(extracted_text)\n",
    "        # else len(extracted_text)\n",
    "    # )\n",
    "    # section_content = extracted_text[content_start:content_end].strip()\n",
    "    # \n",
    "    # sections.append((section_title, section_content))\n",
    "# \n",
    "#Print the extracted sections\n",
    "# if sections:\n",
    "    # for section in sections:\n",
    "        # print(\"Section Title:\", section[0])\n",
    "        # print(\"Section Content:\", section[1])\n",
    "        # print(\"-\" * 40)\n",
    "# else:\n",
    "    # print(\"No sections found in the text.\")\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!Base Modeling!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary for Section 1 - Title: 1.1\n",
      "Summary: software testing is a set of activities to discover defects and evaluate quality of software artifacts. a common misconception about testing is that it only consists of executing tests. software testing is not only a technical activity. it also needs to be properly planned managed estimated monitored and controlled.\n",
      "----------------------------------------\n",
      "Summary for Section 2 - Title: 1.1.2\n",
      "Summary: testing can trigger failures that are caused by defects software can directly find defects test object. debugging is concerned with finding causes and eliminating them. when static testing identifies a defect debugging is concerned with removing it.\n",
      "----------------------------------------\n",
      "Summary for Section 3 - Title: 1.2.1\n",
      "Summary: testing provides a cost-effective means of detecting defects. defects can be removed so testing indirectly contributes to higher quality test objects. testing provides users with indirect representation on development project.\n",
      "----------------------------------------\n",
      "Summary for Section 4 - Title: 1.2.3\n",
      "Summary: human beings make errors which produce defects which turn may result failures. errors can be found documentation such as a requirements specification a test script source code a supporting artifact such as a build file. root causes are identified through root cause analysis which is typically performed when a failure occurs a defect is identified.\n",
      "----------------------------------------\n",
      "Summary for Section 5 - Title: 1.4\n",
      "Summary: tests are context dependent but at a high level there are common sets of test activities without which testing is less likely to achieve test objectives. test process can be tailored to a given situation based on various factors. which test activities are included this test process how they are implemented and when they occur is normally decided as part of test planning for specific situation.\n",
      "----------------------------------------\n",
      "Summary for Section 6 - Title: 5.1\n",
      "Summary: test monitoring involves ongoing checking of all test activities and comparison of actual progress against plan. test control involves taking actions necessary to meet objectives of testing.\n",
      "----------------------------------------\n",
      "Summary for Section 7 - Title: 1.4.2\n",
      "Summary: testing is not performed isolation. test activities are integral part of development processes carried out within an organization. way testing is carried out will depend on a number of contextual factors including: Stakeholders Team members Business domain technical factors project constraints organizational factors tools.\n",
      "----------------------------------------\n",
      "Summary for Section 8 - Title: 1.4.1\n",
      "Summary: test planning work products include: test plan test schedule risk register and entry and exit criteria. test execution work products include: test logs and defect reports. test completion work products include: action items for improvement of subsequent projects.\n",
      "----------------------------------------\n",
      "Summary for Section 9 - Title: 1.4.5\n",
      "Summary: activities and tasks assigned to these two roles depend on factors such as project and product context skills of people roles and organization. test management role takes overall responsibility for test process test team and leadership of test activities. testing role is mainly focused on activities of test analysis test design test implementation and test execution.\n",
      "----------------------------------------\n",
      "Summary for Section 10 - Title: 1.5.1\n",
      "Summary: Generic skills are particularly relevant for testers. testers are often bearers of bad news. it is a common human trait to blame bearer of bad news. communicating test results may be perceived as criticism of product and its author.\n",
      "----------------------------------------\n",
      "Summary for Section 11 - Title: 1.5.3\n",
      "Summary: a certain degree of independence makes tester more effective at finding defects due to differences between author’s and tester’s cognitive biases. independent testers are likely to recognize different kinds of failures and defects compared to developers. independent testers may be isolated from development team which may lead to a lack of collaboration communication problems.\n",
      "----------------------------------------\n",
      "Summary for Section 12 - Title: 2.1\n",
      "Summary: good testing practices that apply to all software development lifecycles. fl-testing focuses on identifying and identifying bugs early in the software development lifecycle.\n",
      "----------------------------------------\n",
      "Summary for Section 13 - Title: 2.1\n",
      "Summary: Devops might have an impact on testing FL- 4 summarize how devops might have an impact on testing FL- 5 summarize how devops might have an impact on testing FL- 6 summarize how devops might have an impact on testing FL- 7 summarize how devops might have an impact on testing FL- 8 summarize how devops might have an impact on testing FL- 8 summarize how devops might have an impact on testing FL- 8 summarize how de\n",
      "----------------------------------------\n",
      "Summary for Section 14 - Title: 2.1\n",
      "Summary: retrospectives can be used as a mechanism for process improvement. retrospectives can be used to identify areas for process improvement. retrospectives can be used to identify areas for process improvement.\n",
      "----------------------------------------\n",
      "Summary for Section 15 - Title: 2.2\n",
      "Summary: 2 distinguish different test types fl-fl-fl-fl-fl-fl-fl-fl-fl-fl-fl.\n",
      "----------------------------------------\n",
      "Summary for Section 16 - Title: 2.3\n",
      "Summary: summarize maintenance testing and its triggers v4.0 page of 74 2023-04-21 2023-04-21 2023-04-21 2023-04-21.\n",
      "----------------------------------------\n",
      "Summary for Section 17 - Title: 2.1.1\n",
      "Summary: choice of SDLC impacts on the: Scope and timing of test activities Level of detail of test documentation Choice of test techniques and test approach Extent of test automation Role and responsibilities of a tester.\n",
      "----------------------------------------\n",
      "Summary for Section 18 - Title: 2.1.3\n",
      "Summary: TDD ATDD and BDD are similar development approaches where tests are defined as a means of directing development. each approach implements principle of early testing and follows a shift-left approach since tests are defined before code is written. tests may persist as automated tests to ensure code quality future adaptions / refactoring.\n",
      "----------------------------------------\n",
      "Summary for Section 19 - Title: 2.1.5\n",
      "Summary: the principle of early testing is sometimes referred to as shift-left. shift-left suggests that testing should be done earlier but it does not mean that testing later SDLC should be neglected. it is important that stakeholders are convinced and bought into this concept.\n",
      "----------------------------------------\n",
      "Summary for Section 20 - Title: 2.2\n",
      "Summary: each test level is an instance of test process performed relation to software at a given stage of development from individual components to complete systems where applicable systems of systems. test levels are often defined such that exit criteria of one level are part of entry criteria for next level. test levels may overlap time. test types are groups of test activities related to specific quality characteristics.\n",
      "----------------------------------------\n",
      "Summary for Section 21 - Title: 2.2.2\n",
      "Summary: functional testing evaluates functions that a component system should perform. non-functional testing evaluates attributes other than functional characteristics of a component system. black-box testing is specification-based and derives tests from documentation external to test object. white-box testing is structure-based and derives tests from system's implementation internal structure.\n",
      "----------------------------------------\n",
      "Summary for Section 22 - Title: 2.3\n",
      "Summary: maintenance testing can be corrective adaptive to changes environment improve performance maintainability so maintenance can involve planned releases/deployments and unplanned releases/deployments. impact analysis may be done before a change is made to help decide if change should be made. testing changes to a system production includes both evaluating success of implementation of change and checking for possible regressions.\n",
      "----------------------------------------\n",
      "Summary for Section 23 - Title: 3.1\n",
      "Summary: explain value of static testing. explain value of static testing. explain value of static testing. explain value of static testing. explain value of dynamic testing.\n",
      "----------------------------------------\n",
      "Summary for Section 24 - Title: 3.2\n",
      "Summary: identifying benefits of early and frequent stakeholder feedback. identifying benefits of early and frequent stakeholder feedback. identifying benefits of frequent stakeholder feedback.\n",
      "----------------------------------------\n",
      "Summary for Section 25 - Title: 3.2\n",
      "Summary: responsibilities are assigned to principal roles when performing reviews. reviewers should be aware of which responsibilities are assigned to principal roles when performing reviews. reviewers should be aware of which responsibilities are assigned to principal roles when performing reviews.\n",
      "----------------------------------------\n",
      "Summary for Section 26 - Title: 3.2\n",
      "Summary: 5 recall factors that contribute to a successful review v4.0 Page of 74 2023-04-21 2023-04-21 2023-04-21 2023-04-21 2023-04-21 2023-04-21.\n",
      "----------------------------------------\n",
      "Summary for Section 27 - Title: 3.1.1\n",
      "Summary: Almost any work product that can be read and understood can be subject of a review. work products that are not appropriate for static testing include those that are difficult to interpret by human beings.\n",
      "----------------------------------------\n",
      "Summary for Section 28 - Title: 3.1.3\n",
      "Summary: static and dynamic testing can both lead to detection of defects work products. however there are some defect types that can only be found by either static dynamic testing. static testing may more easily detect defects that lay on paths through code that are rarely executed hard to reach using dynamic testing.\n",
      "----------------------------------------\n",
      "Summary for Section 29 - Title: 3.2.1\n",
      "Summary: early and frequent stakeholder feedback allows for early communication of potential quality problems. a failure to deliver what stakeholder wants can result costly rework missed deadlines blame games and might even lead to complete project failure.\n",
      "----------------------------------------\n",
      "Summary for Section 30 - Title: 3.2.3\n",
      "Summary: principal roles and their responsibilities are: Manager – decides what is to be reviewed and provides resources such as staff and time for review. a reviewer may be someone working on project a subject matter expert any other stakeholder.\n",
      "----------------------------------------\n",
      "Summary for Section 31 - Title: 3.2.5\n",
      "Summary: success factors for reviews include: Defining clear objectives and measurable exit criteria. Choosing appropriate review type to achieve given objectives and to suit type of work product review participants project needs and context. Providing feedback from reviews to stakeholders and authors so they can improve product and their activities. Providing adequate time to participants to prepare for review. providing adequate training for all participants so they know how to fulfil their role.\n",
      "----------------------------------------\n",
      "Summary for Section 32 - Title: 4.2\n",
      "Summary: 1 Use equivalence partitioning to derive test cases. 1 use equivalence partitioning to derive test cases.\n",
      "----------------------------------------\n",
      "Summary for Section 33 - Title: 4.2\n",
      "Summary: 3 use decision table testing to derive test cases. use decision table testing to derive test cases. use decision table testing to derive test cases. use decision table testing to derive test cases.\n",
      "----------------------------------------\n",
      "Summary for Section 34 - Title: 4.3\n",
      "Summary: explains statement testing. explains statement testing. explains statement testing. explains statement testing. explains statement testing. explains statement testing.\n",
      "----------------------------------------\n",
      "Summary for Section 35 - Title: 4.3\n",
      "Summary: explain value of white-box testing 4.4 experience-based test techniques 4.4 white-box testing. explain value of white-box testing. explain value of white-box testing.\n",
      "----------------------------------------\n",
      "Summary for Section 36 - Title: 4.4\n",
      "Summary: exploratory testing is a form of exploratory testing. it involves a series of tests to find out if a hypothesis is true or false. the results are compared with those of a control group.\n",
      "----------------------------------------\n",
      "Summary for Section 37 - Title: 4.5\n",
      "Summary: collaboration-based test approaches - cbtas. collaboration-based test approaches - cbtas. cbtas.\n",
      "----------------------------------------\n",
      "Summary for Section 38 - Title: 4.5\n",
      "Summary: acceptance criteria can be written in a number of different ways. acceptance criteria can be written in a number of different ways. acceptance criteria can be written in a number of different ways. acceptance criteria can be written in a number of different ways.\n",
      "----------------------------------------\n",
      "Summary for Section 39 - Title: 4.1\n",
      "Summary: test techniques help to develop a relatively small but sufficient set of test cases a systematic way. test techniques also help tester to define test conditions identify coverage items and identify test data. test techniques are classified as black-box white-box and experience-based.\n",
      "----------------------------------------\n",
      "Summary for Section 40 - Title: 4.2.1\n",
      "Summary: equivalence partitioning divides data into partitions based on expectation that all elements of a given partition are to be processed same way by test object. if a test case that tests one value from an equivalence partition detects a defect this defect should also be detected by test cases that test any other value from same partition. therefore one test for each partition is sufficient.\n",
      "----------------------------------------\n",
      "Summary for Section 41 - Title: 4.2.3\n",
      "Summary: decision tables are an effective way of recording complex logic such as business rules. a full decision table has enough columns to cover every combination of conditions. table can be simplified by deleting columns containing infeasible combinations.\n",
      "----------------------------------------\n",
      "Summary for Section 42 - Title: 4.3\n",
      "Summary: this section focuses on two code-related white-box test techniques: statement testing and branch testing. there are also more rigorous techniques that are used some safety-critical mission-critical high-integrity environments. these techniques are not discussed this syllabus.\n",
      "----------------------------------------\n",
      "Summary for Section 43 - Title: 4.3.2\n",
      "Summary: branch testing and branch coverage aim is to design test cases to exercise branches code until an acceptable level of coverage is achieved. coverage measured as number of branches exercised by test cases divided by total number of branches. when 100% branch coverage is achieved all branches code unconditional and conditional are exercised by test cases.\n",
      "----------------------------------------\n",
      "Summary for Section 44 - Title: 4.4\n",
      "Summary: Error guessing Exploratory testing Checklist-based testing Checklist-based testing Checklist-based testing Error guessing Exploratory testing Checklist-based testing.\n",
      "----------------------------------------\n",
      "Summary for Section 45 - Title: 4.4.2\n",
      "Summary: testing is used to learn more about test object to explore it more deeply with focused tests and to create tests for untested areas. exploratory testing is sometimes conducted using session-based testing to structure testing. tests are executed and evaluated while tester learns about test object.\n",
      "----------------------------------------\n",
      "Summary for Section 46 - Title: 4.5\n",
      "Summary: each of above-mentioned techniques has a particular objective with respect to defect detection. collaboration-based approaches focus also on defect avoidance by collaboration and communication.\n",
      "----------------------------------------\n",
      "Summary for Section 47 - Title: 4.5.2\n",
      "Summary: acceptance criteria are conditions that an implementation of user story must meet to be accepted by stakeholders. acceptance criteria may be viewed as test conditions that should be exercised by tests. acceptance criteria are used to: Define scope of user story Reach consensus among stakeholders Describe both positive and negative scenarios Serve as a basis for user story acceptance testing\n",
      "----------------------------------------\n",
      "Summary for Section 48 - Title: 5.1\n",
      "Summary: 1 Exemplify purpose and content of a test plan FL-fl-fl-fl-fl-fl-fl-fl-fl-fl-fl-fl-fl-fl-fl-fl-fl-fl-fl-fl-fl-fl-fl-fl-fl-fl-fl-fl-fl-fl-fl-fl-fl-f\n",
      "----------------------------------------\n",
      "Summary for Section 49 - Title: 5.1\n",
      "Summary: compare and contrast entry criteria and exit criteria. compare and contrast entry criteria and exit criteria. compare and contrast entry criteria and exit criteria. compare and contrast entry criteria and exit criteria.\n",
      "----------------------------------------\n",
      "Summary for Section 50 - Title: 5.1\n",
      "Summary: 5 apply test case prioritization. FL- 5 apply test case prioritization. FL- 5 apply test case prioritization.\n",
      "----------------------------------------\n",
      "Summary for Section 51 - Title: 5.1\n",
      "Summary: testing quadrants and their relationships with test levels and test types. risk management is a key part of a test manager's job. a test manager's job is to manage the risk of an incident.\n",
      "----------------------------------------\n",
      "Summary for Section 52 - Title: 5.2\n",
      "Summary: 2 distinguish between project risks and product risks. product risks are more likely to be a problem than a project risk. product risks more likely to be a problem than a project risk.\n",
      "----------------------------------------\n",
      "Summary for Section 53 - Title: 5.2\n",
      "Summary: measures taken response to analyzed product risks 5.3 test monitoring test control and test completion 5.3 test monitoring test control and test completion 5.3 test monitoring test control and test completion 5.4 test monitoring test control and test completion 5.3 test monitoring test control and test completion 5.4 test monitoring test control and test completion 5.3 test monitoring test control and test completion 5.4 test monitoring test control and test completion\n",
      "----------------------------------------\n",
      "Summary for Section 54 - Title: 5.3\n",
      "Summary: 2 summarize purposes content and audiences for test reports. test reports should be able to summarise test results. test reports should also be able to summarise test results.\n",
      "----------------------------------------\n",
      "Summary for Section 55 - Title: 5.4\n",
      "Summary: configuration management supports testing 5.5 defect management 5.5 configuration management supports testing 5.5 defect management 5.5 configuration management supports defect management 5.5 defect management 5.5 configuration management supports testing.\n",
      "----------------------------------------\n",
      "Summary for Section 56 - Title: 5.1\n",
      "Summary: test planning.................\n",
      "----------------------------------------\n",
      "Summary for Section 57 - Title: 5.1.2\n",
      "Summary: release planning looks ahead to release of a product defines and re-defines product backlog. it also serves as basis for test approach and test plan across all iterations. iteration planning looks ahead to end of a single iteration and is concerned with iteration backlog.\n",
      "----------------------------------------\n",
      "Summary for Section 58 - Title: 5.1.4\n",
      "Summary: test effort estimation involves predicting amount of test-related work needed to meet objectives of a test project. it is important to make it clear to stakeholders that estimate is based on a number of assumptions and is always subject to estimation error. this syllabus following four estimation techniques are described.\n",
      "----------------------------------------\n",
      "Summary for Section 59 - Title: 5.1.6\n",
      "Summary: test pyramid model supports team test automation and test effort allocation by showing that different goals are supported by different levels of test automation. pyramid layers represent groups of tests. higher layer lower test granularity test isolation and test execution time. top layer represents complex high-level end-to-end tests which are generally slower than tests from lower layers.\n",
      "----------------------------------------\n",
      "Summary for Section 60 - Title: 5.2\n",
      "Summary: risk management allows organizations to increase likelihood of achieving objectives improve quality of their products and increase stakeholders’ confidence and trust. main risk management activities are: risk analysis Risk control test approach which test activities are selected prioritized and managed based on risk analysis and risk control.\n",
      "----------------------------------------\n",
      "Summary for Section 61 - Title: 5.2.2\n",
      "Summary: project risks are related to management and control of project. product risks are related to product quality characteristics. product risks when they occur may result various negative consequences including: user dissatisfaction Loss of revenue trust reputation Damage to third parties High maintenance costs overload of helpdesk.\n",
      "----------------------------------------\n",
      "Summary for Section 62 - Title: 5.2.4\n",
      "Summary: product risk control consists of risk mitigation and risk monitoring. aim of risk monitoring is to ensure that mitigation actions are effective. several response options to risk are possible e.g. risk mitigation by testing.\n",
      "----------------------------------------\n",
      "Summary for Section 63 - Title: 5.3.1\n",
      "Summary: test monitoring gathers a variety of metrics to support test control and test completion. common test metrics include: project progress metrics Test progress metrics Product quality metrics Defect metrics Risk metrics Coverage metrics cost metrics. test metrics are gathered to show progress against planned schedule and budget.\n",
      "----------------------------------------\n",
      "Summary for Section 64 - Title: 5.3.3\n",
      "Summary: best means of communicating test status varies depending on test management concerns organizational test strategies regulatory standards case of self-organizing teams on team itself. verbal communication with team members and other stakeholders dashboards electronic communication channels online documentation formal test reports one more these options can be used. more formal communication may be more appropriate for distributed teams where direct face-to-face communication is not always possible due to geographical distance time differences.\n",
      "----------------------------------------\n",
      "Summary for Section 65 - Title: 5.5\n",
      "Summary: defect management includes a workflow for handling individual anomalies from their discovery to their closure and rules for their classification. a defect report logged during dynamic testing typically includes: Unique identifier Title with a short summary of anomaly being reported. a defect report logged during static testing typically includes: Unique identifier Title with a short summary of anomaly being reported.\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Check if there are sections available before accessing them\n",
    "if len(sections) >= 2:\n",
    "    # Load the pre-trained T5 model and tokenizer\n",
    "    model_name = \"t5-large\"\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "    tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "    for section_index, (section_title, section_content) in enumerate(sections):\n",
    "        # Tokenize the input section\n",
    "        input_ids = tokenizer.encode(\"summarize: \" + section_content, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "\n",
    "        # Generate the summary\n",
    "        summary_ids = model.generate(input_ids, max_length=150, min_length=30, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "        summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "        # Print the summary for each section\n",
    "        print(f\"Summary for Section {section_index + 1} - Title: {section_title}\")\n",
    "        print(\"Summary:\", summary)\n",
    "        print(\"-\" * 40)\n",
    "else:\n",
    "    print(\"Not enough sections found in the list.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Question: <pad> What is the most common cause of a defect?</s>\n"
     ]
    }
   ],
   "source": [
    "# questions generation\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Load the pre-trained model and tokenizer for question generation\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"PrimeQA/mt5-base-tydi-question-generator\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"PrimeQA/mt5-base-tydi-question-generator\")\n",
    "\n",
    "# Function to generate a question for a given summary\n",
    "def generate_question(summary, max_length=64):\n",
    "    # Tokenize the input text and generate the question\n",
    "    features = tokenizer([summary], return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "    output = model.generate(input_ids=features['input_ids'], \n",
    "                            attention_mask=features['attention_mask'],\n",
    "                            max_length=max_length,\n",
    "                            num_return_sequences=1)\n",
    "    \n",
    "    return tokenizer.decode(output[0])\n",
    "\n",
    "# Example usage:\n",
    "summary = \"testing can trigger failures that are caused by defects software can directly find defects test object. debugging is concerned with finding causes and eliminating them. when static testing identifies a defect debugging is concerned with removing it.\"\n",
    "question = generate_question(summary)\n",
    "print(\"Generated Question:\", question)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct Answer: requirements ambiguity, inadequate testing, poor quality control\n",
      "Wrong Answers: ['This is a wrong answer.', 'This is a wrong answer.']\n"
     ]
    }
   ],
   "source": [
    "# answer extraction\n",
    "\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "import random\n",
    "\n",
    "# Load the pre-trained T5 model and tokenizer\n",
    "model_name = \"t5-small\"  # You can choose a different model size if needed\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Function to generate a list of wrong answers for a given question and context\n",
    "def generate_wrong_answers(question, context, num_wrong_answers=3, max_length=64):\n",
    "    wrong_answers = []\n",
    "    \n",
    "    # Generate wrong answers by modifying the context or providing plausible but incorrect information\n",
    "    for _ in range(num_wrong_answers):\n",
    "        # Modify the context or generate a random incorrect response\n",
    "        incorrect_response = \"This is a wrong answer.\"\n",
    "        \n",
    "        # Append the wrong answer to the list\n",
    "        wrong_answers.append(incorrect_response)\n",
    "    \n",
    "    return wrong_answers\n",
    "\n",
    "# Function to generate an answer for a given question and context\n",
    "def generate_answer(question, context, max_length=64):\n",
    "    # Prepare the input text by combining the question and context\n",
    "    input_text = f\"question: {question} context: {context}\"\n",
    "    \n",
    "    # Tokenize the input text\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    \n",
    "    # Generate the answer\n",
    "    answer_ids = model.generate(input_ids, max_length=max_length, num_beams=4, early_stopping=True, num_return_sequences=1)\n",
    "    \n",
    "    # Decode the generated answer\n",
    "    answer = tokenizer.decode(answer_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    return answer\n",
    "\n",
    "# Example usage:\n",
    "question = \"What is the most common cause of a defect?\"\n",
    "context = \"Defects can be caused by a variety of factors, such as human errors, environmental issues, technical problems, or communication gaps. Common causes of defects include requirements ambiguity, inadequate testing, poor quality control, and lack of skills or knowledge.\"\n",
    "\n",
    "# Generate the correct answer\n",
    "correct_answer = generate_answer(question, context)\n",
    "print(\"Correct Answer:\", correct_answer)\n",
    "\n",
    "# Generate wrong answers\n",
    "wrong_answers = generate_wrong_answers(question, context, num_wrong_answers=2)\n",
    "print(\"Wrong Answers:\", wrong_answers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keywords extraction - missing so far, leads to HTTPSConnectionPool(host='cdn-lfs.huggingface.co', port=443): Read timed out. A lot of hours are required. \n",
    "# Talk to teachers\n",
    "\n",
    "# Load the pre-trained KeyBERT model\n",
    "#model = KeyBERT(\"distilbert-base-nli-mean-tokens\")\n",
    "\n",
    "# Input text (use sections[0][1] as the content of the first section)\n",
    "#section_content = sections[0][1]\n",
    "\n",
    "# Extract keywords\n",
    "#try:\n",
    "#    keywords = model.extract_keywords(section_content, keyphrase_ngram_range=(1, 2), stop_words='english', use_mmr=True, top_n=10, resume_download=True)\n",
    "    \n",
    "    # Print the extracted keywords\n",
    "#    for keyword in keywords:\n",
    "#        print(keyword)\n",
    "#except Exception as e:\n",
    "#    print(\"An error occurred:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Greedy methods without beam search do not support `num_return_sequences` different than 1 (got 20).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Elena\\github\\Quiz_Generator_Markov_Chain\\extraction.ipynb Cell 35\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Elena/github/Quiz_Generator_Markov_Chain/extraction.ipynb#X56sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m sentence\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Elena/github/Quiz_Generator_Markov_Chain/extraction.ipynb#X56sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m \u001b[39m# Generate 20 questions from the sentences using the T5 model\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Elena/github/Quiz_Generator_Markov_Chain/extraction.ipynb#X56sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m questions \u001b[39m=\u001b[39m generate_questions_t5(sentences, num_questions\u001b[39m=\u001b[39m\u001b[39m20\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Elena/github/Quiz_Generator_Markov_Chain/extraction.ipynb#X56sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m \u001b[39m# Print the generated questions\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Elena/github/Quiz_Generator_Markov_Chain/extraction.ipynb#X56sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, question \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(questions, start\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n",
      "\u001b[1;32mc:\\Users\\Elena\\github\\Quiz_Generator_Markov_Chain\\extraction.ipynb Cell 35\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Elena/github/Quiz_Generator_Markov_Chain/extraction.ipynb#X56sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m input_ids \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mencode(input_text, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m, max_length\u001b[39m=\u001b[39m\u001b[39m512\u001b[39m, truncation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Elena/github/Quiz_Generator_Markov_Chain/extraction.ipynb#X56sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m# Generate the questions\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Elena/github/Quiz_Generator_Markov_Chain/extraction.ipynb#X56sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m question_ids \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mgenerate(input_ids, max_length\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m, num_return_sequences\u001b[39m=\u001b[39mnum_questions)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Elena/github/Quiz_Generator_Markov_Chain/extraction.ipynb#X56sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m generated_questions \u001b[39m=\u001b[39m [tokenizer\u001b[39m.\u001b[39mdecode(q, skip_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m) \u001b[39mfor\u001b[39;00m q \u001b[39min\u001b[39;00m question_ids]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Elena/github/Quiz_Generator_Markov_Chain/extraction.ipynb#X56sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m questions\u001b[39m.\u001b[39mextend(generated_questions)\n",
      "File \u001b[1;32mc:\\Users\\Elena\\anaconda3\\envs\\qgmc\\Lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Elena\\anaconda3\\envs\\qgmc\\Lib\\site-packages\\transformers\\generation\\utils.py:1428\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[0;32m   1426\u001b[0m generation_config \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mdeepcopy(generation_config)\n\u001b[0;32m   1427\u001b[0m model_kwargs \u001b[39m=\u001b[39m generation_config\u001b[39m.\u001b[39mupdate(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# All unused kwargs must be model kwargs\u001b[39;00m\n\u001b[1;32m-> 1428\u001b[0m generation_config\u001b[39m.\u001b[39mvalidate()\n\u001b[0;32m   1429\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_model_kwargs(model_kwargs\u001b[39m.\u001b[39mcopy())\n\u001b[0;32m   1431\u001b[0m \u001b[39m# 2. Set generation parameters if not already defined\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Elena\\anaconda3\\envs\\qgmc\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:467\u001b[0m, in \u001b[0;36mGenerationConfig.validate\u001b[1;34m(self, is_init)\u001b[0m\n\u001b[0;32m    465\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_beams \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    466\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdo_sample \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[1;32m--> 467\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    468\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mGreedy methods without beam search do not support `num_return_sequences` different than 1 \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    469\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(got \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_return_sequences\u001b[39m}\u001b[39;00m\u001b[39m).\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    470\u001b[0m         )\n\u001b[0;32m    471\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_return_sequences \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_beams:\n\u001b[0;32m    472\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    473\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m`num_return_sequences` (\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_return_sequences\u001b[39m}\u001b[39;00m\u001b[39m) has to be smaller or equal to `num_beams` \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    474\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_beams\u001b[39m}\u001b[39;00m\u001b[39m).\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    475\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Greedy methods without beam search do not support `num_return_sequences` different than 1 (got 20)."
     ]
    }
   ],
   "source": [
    "# Attempt of questions generation\n",
    "# Load the pre-trained T5 model and tokenizer\n",
    "model_name = \"t5-large\"\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Tokenize the summary into sentences\n",
    "sentences = sent_tokenize(summary)  # section_content = sections[0][1]\n",
    "\n",
    "# Function to generate questions from sentences using the T5 model\n",
    "def generate_questions_t5(text, num_questions=20):\n",
    "    questions = []\n",
    "    for sentence in text:\n",
    "        # Preprocess the sentence (e.g., remove special characters)\n",
    "        preprocessed_sentence = preprocess_sentence(sentence)\n",
    "        \n",
    "        # Generate questions using the T5 model\n",
    "        input_text = f\"generate questions: {preprocessed_sentence}\"\n",
    "        input_ids = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "        \n",
    "        # Generate the questions\n",
    "        question_ids = model.generate(input_ids, max_length=100, num_return_sequences=num_questions)\n",
    "        generated_questions = [tokenizer.decode(q, skip_special_tokens=True) for q in question_ids]\n",
    "        \n",
    "        questions.extend(generated_questions)\n",
    "\n",
    "        # Stop generating questions if we reach the desired number\n",
    "        if len(questions) >= num_questions:\n",
    "            return questions\n",
    "\n",
    "    return questions[:num_questions]  # Return only the specified number of questions\n",
    "\n",
    "# Function to preprocess a sentence (customize as needed)\n",
    "def preprocess_sentence(sentence):\n",
    "    # Implement your preprocessing logic here\n",
    "    # For example, you can remove special characters, lower-case the text, etc.\n",
    "    return sentence\n",
    "\n",
    "# Generate 20 questions from the sentences using the T5 model\n",
    "questions = generate_questions_t5(sentences, num_questions=20)\n",
    "\n",
    "# Print the generated questions\n",
    "for i, question in enumerate(questions, start=1):\n",
    "    print(f\"Question {i}: {question}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1: What is that are caused?\n",
      "Question 2: What can that are caused?\n",
      "Question 3: What is caused by defects?\n",
      "Question 4: What can caused by defects?\n",
      "Question 5: What is by defects software?\n",
      "Question 6: What can by defects software?\n",
      "Question 7: What is directly find defects?\n",
      "Question 8: What can directly find defects?\n",
      "Question 9: What is find defects test?\n",
      "Question 10: What can find defects test?\n",
      "Question 11: What is defects test object?\n",
      "Question 12: What can defects test object?\n",
      "Question 13: What is test object .?\n",
      "Question 14: What can test object .?\n",
      "Question 15: What is that are caused?\n",
      "Question 16: What can that are caused?\n",
      "Question 17: What is caused by defects?\n",
      "Question 18: What can caused by defects?\n",
      "Question 19: What is by defects software?\n",
      "Question 20: What can by defects software?\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sentences = sent_tokenize(summary) \n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Function to generate a fixed number of questions from sentences using trigrams\n",
    "def generate_questions(text, num_questions=20):\n",
    "    questions = []\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Function to generate a fixed number of questions from sentences using trigrams\n",
    "def generate_questions(text, num_questions=20):\n",
    "    questions = []\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # Define question templates with different prefixes\n",
    "    question_templates = [\"What is\", \"What can\"]\n",
    "    \n",
    "    for sentence in text:\n",
    "        # Tokenize each sentence into words\n",
    "        words = nltk.word_tokenize(sentence)\n",
    "        # Generate n-grams (trigrams) from the words\n",
    "        n_grams = list(ngrams(words, 3))\n",
    "\n",
    "        # Construct questions using the trigrams and different question templates\n",
    "        for n_gram in n_grams:\n",
    "            if (\n",
    "                n_gram[-1].lower() not in stop_words \n",
    "                and n_gram[-1].lower() != n_gram[-2].lower()\n",
    "                and \"can\" not in n_gram\n",
    "            ):\n",
    "                for template in question_templates:\n",
    "                    question = f\"{template} {n_gram[0]} {n_gram[1]} {n_gram[2]}?\"\n",
    "                    questions.append(question)\n",
    "\n",
    "            # Stop generating questions if we reach the desired number\n",
    "            if len(questions) >= num_questions:\n",
    "                return questions\n",
    "\n",
    "    return questions[:num_questions]  # Return only the specified number of questions\n",
    "\n",
    "# Generate 20 questions from the sentences using trigrams and different prefixes\n",
    "questions = generate_questions(sentences, num_questions=20)\n",
    "\n",
    "# Print the generated questions\n",
    "for i, question in enumerate(questions, start=1):\n",
    "    print(f\"Question {i}: {question}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !!! possible training of T5 !!!\n",
    "\n",
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, T5Config\n",
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load your custom dataset using the datasets library\n",
    "dataset = load_dataset('your_custom_dataset')\n",
    "\n",
    "# Initialize the tokenizer and model\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "config = T5Config.from_pretrained(\"t5-small\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\", config=config)\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['input_text'], examples['target_text'], padding='max_length', truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    per_device_train_batch_size=4,\n",
    "    output_dir=\"./t5-fine-tuned\",\n",
    "    num_train_epochs=3,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_steps=10_000,\n",
    "    eval_steps=10_000,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "# Initialize the trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=tokenized_datasets.data_collator,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save_pretrained(\"t5-fine-tuned\")\n",
    "tokenizer.save_pretrained(\"t5-fine-tuned\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here comes gradio + manual selection of correct questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained model and tokenizer\n",
    "#model_name = \"bert-large-uncased-whole-word-masking-finetuned-squad\"\n",
    "#tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "#model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "\n",
    "# Provide a passage and a question\n",
    "#passage = extracted_text\n",
    "#question = \"Which of the following statements describe a valid test objective?\"\n",
    "\n",
    "#Which of the following statements describe a valid test objective?\n",
    "#What does not work as expected?\n",
    "\n",
    "# Tokenize the passage and question\n",
    "#inputs = tokenizer(question, passage, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "# Get the answer from the model\n",
    "#start_scores, end_scores = model(**inputs, return_dict = False)\n",
    "#start_idx = torch.argmax(start_scores)\n",
    "#end_idx = torch.argmax(end_scores)\n",
    "\n",
    "# Decode the answer from the tokenized output\n",
    "#answer_tokens = inputs[\"input_ids\"][0][start_idx:end_idx + 1]\n",
    "#answer = tokenizer.decode(answer_tokens)\n",
    "\n",
    "#print(\"Answer:\", answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To process text and generate questions with answers, you can consider using pre-trained language models, such as GPT-3, GPT-4, BERT, T5, or similar models. Each of these models has its strengths and can be used for different aspects of question generation and answering:\n",
    "\n",
    "T5 (Text-to-Text Transfer Transformer): T5 is a versatile language model that can be fine-tuned for various natural language processing tasks, including question generation. You can fine-tune a pre-trained T5 model on your specific dataset to generate high-quality questions.\n",
    "\n",
    "GPT-3: OpenAI's GPT-3 is a powerful language model known for its natural language generation capabilities. You can prompt GPT-3 to generate questions based on your input text. It can produce contextually relevant questions, but it may require careful instruction and filtering of the generated output.\n",
    "\n",
    "BART (Bidirectional and Auto-Regressive Transformers): BART is another transformer-based model that can be fine-tuned for question generation tasks. It excels in text generation tasks and can produce coherent and meaningful questions.\n",
    "\n",
    "XLNet: XLNet is a transformer model that has achieved strong performance in various NLP tasks. It can be fine-tuned for question generation, and its bidirectional context modeling can lead to better question generation.\n",
    "\n",
    "BERT (Bidirectional Encoder Representations from Transformers): BERT can also be used for question generation by fine-tuning. While it was originally designed for understanding context, it can be adapted for question generation with appropriate training data.\n",
    "\n",
    "UniLM: UniLM is a model that can be used for various text generation tasks, including question generation. It combines unidirectional, bidirectional, and sequence-to-sequence learning, making it versatile for NLP tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "# from io import StringIO\n",
    "\n",
    "# # Load the pre-trained model and tokenizer\n",
    "# model_name = \"bert-large-uncased-whole-word-masking-finetuned-squad\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "\n",
    "# # Create a StringIO object with your text\n",
    "# text_io = StringIO()\n",
    "# text_io.write(\"Your text goes here.\")\n",
    "# text_io.seek(0)  # Reset the StringIO object to the beginning\n",
    "\n",
    "# # Read the text from the StringIO object and convert it to a regular string\n",
    "# text = text_io.read()\n",
    "\n",
    "# # Provide a question\n",
    "# question = \"What is the answer to my question?\"\n",
    "\n",
    "# # Tokenize the text and question\n",
    "# inputs = tokenizer(question, text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "# # Get the answer from the model\n",
    "# start_scores, end_scores = model(**inputs)\n",
    "# start_idx = torch.argmax(start_scores)\n",
    "# end_idx = torch.argmax(end_scores)\n",
    "\n",
    "# # Decode the answer from the tokenized output\n",
    "# answer_tokens = inputs[\"input_ids\"][0][start_idx:end_idx + 1]\n",
    "# answer = tokenizer.decode(answer_tokens)\n",
    "\n",
    "# print(\"Answer:\", answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Question Generation using Seq2Seq (T5)\n",
    "\n",
    "# Load the pre-trained Seq2Seq model for question generation\n",
    "# question_generation_model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "# question_generation_tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "# \n",
    "#ISTQB document (replace with your actual content)\n",
    "# istqb_document = \"\"\"\n",
    "# 1.1. What is Testing? \n",
    "# \n",
    "# Software systems are an integral part of our daily life. Most people have had experience with software \n",
    "# that did not work as expected. Software that does not work correctly can lead to many problems, \n",
    "# including loss of money, time or business reputation, and, in extreme cases, even injury or death. \n",
    "# Software testing assesses software quality and helps reducing the risk of software failure in operation. \n",
    "# \n",
    "# Software testing is a set of activities to discover defects and evaluate the quality of software artifacts. \n",
    "# These artifacts, when being tested, are known as test objects. A common misconception about testing is \n",
    "# that it only consists of executing tests (i.e., running the software and checking the test results). However, \n",
    "# software testing also includes other activities and must be aligned with the software development lifecycle \n",
    "# (see chapter 2). \n",
    "# \n",
    "# Another common misconception about testing is that testing focuses entirely on verifying the test object. \n",
    "# Whilst testing involves verification, i.e., checking whether the system meets specified requirements, it also \n",
    "# involves validation, which means checking whether the system meets users’ and other stakeholders’ \n",
    "# needs in its operational environment. \n",
    "# \"\"\"\n",
    "# \n",
    "#Generate questions from the ISTQB document\n",
    "# def generate_questions(document, max_length=64, num_questions=1):\n",
    "    # inputs = question_generation_tokenizer.encode(\"generate questions: \" + document, return_tensors=\"pt\", max_length=max_length, truncation=True)\n",
    "    # questions = question_generation_model.generate(inputs, max_length=max_length, num_return_sequences=num_questions)\n",
    "    # return [question_generation_tokenizer.decode(question, skip_special_tokens=True) for question in questions]\n",
    "# \n",
    "# generated_questions = generate_questions(istqb_document)\n",
    "# \n",
    "#Print generated questions\n",
    "# for question in generated_questions:\n",
    "    # print(\"Question:\", question)\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is template for Quize layout, needs to be re-worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Elena\\AppData\\Local\\Temp\\ipykernel_34488\\715082763.py:26: GradioDeprecationWarning: Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components\n",
      "  inputs=gr.inputs.Dropdown(section),\n",
      "C:\\Users\\Elena\\AppData\\Local\\Temp\\ipykernel_34488\\715082763.py:26: GradioDeprecationWarning: `optional` parameter is deprecated, and it has no effect\n",
      "  inputs=gr.inputs.Dropdown(section),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7894\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7894/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "# Replace this with your actual list of section titles and content\n",
    "#sections = {\n",
    "#    (\"1.1.1.\", \"Content of Section 1.1.1...\"),\n",
    "#    (\"1.2.\", \"Content of Section 1.2...\"),\n",
    "    # Add more sections as needed\n",
    "#}\n",
    "\n",
    "def display_sections(section):\n",
    "    # Find the selected section in the list based on the section title\n",
    "    selected_section = next((s for s in sections if s[0] == section), None)\n",
    "\n",
    "    if selected_section:\n",
    "        section_title, section_content = selected_section\n",
    "        return f\"Section Title: {section_title}\\nSection Content: {section_content}\"\n",
    "    else:\n",
    "        return \"Section not found\"\n",
    "\n",
    "# Get a list of section titles from the sections list\n",
    "section_titles = [section[0] for section in sections]\n",
    "\n",
    "# Create a Gradio interface with a dropdown list of section titles\n",
    "iface = gr.Interface(\n",
    "    fn=display_sections,\n",
    "    inputs=gr.inputs.Dropdown(section),\n",
    "    outputs=\"text\",\n",
    "    css=\".gradio-container {background-color: lightblue}\"\n",
    ")\n",
    "\n",
    "iface.launch(share=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JSON converter from CSVlogger() to JSON, this is when the previous step w questions selection is finished via Flag button"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV data has been converted to JSON and saved as flagged/questions_answers.json. The file contains final result of Quiz generator. \n"
     ]
    }
   ],
   "source": [
    "# set path to both files\n",
    "csv_file = 'flagged/log.csv'\n",
    "json_file = 'flagged/questions_answers.json'\n",
    "\n",
    "# empty list to store the JSON data\n",
    "json_data = []\n",
    "\n",
    "# Open the CSV file for reading\n",
    "with open(csv_file, 'r') as csvfile:\n",
    "    # Create a CSV reader object\n",
    "    csvreader = csv.DictReader(csvfile)\n",
    "    \n",
    "    # Iterate through each row in the CSV file\n",
    "    for row in csvreader:\n",
    "        # Append each row as a dictionary to the JSON data list\n",
    "        json_data.append(row)\n",
    "\n",
    "# Open the JSON file for writing and save the JSON data\n",
    "with open(json_file, 'w') as jsonfile:\n",
    "    json.dump(json_data, jsonfile, indent=4)\n",
    "\n",
    "print(f\"CSV data has been converted to JSON and saved as {json_file}. The file contains final result of Quiz generator. \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draft metrics\n",
    "\n",
    "import nltk\n",
    "\n",
    "# Reference questions (human-generated)\n",
    "reference_questions = [\n",
    "    \"What is the test objective?\",\n",
    "    \"How do objectives vary?\",\n",
    "    \"What does the context include?\",\n",
    "    # Add more reference questions here\n",
    "]\n",
    "\n",
    "# Automatically generated questions\n",
    "generated_questions = [\n",
    "    \"What is test objectives?\",\n",
    "    \"What is objectives vary?\",\n",
    "    \"How do objectives depend?\",\n",
    "    # Add more generated questions here\n",
    "]\n",
    "\n",
    "# Initialize the NLTK BLEU scorer\n",
    "bleu_scorer = nltk.translate.bleu_score.SmoothingFunction()\n",
    "\n",
    "# Calculate BLEU score (a measure of similarity)\n",
    "bleu_scores = [nltk.translate.bleu_score.sentence_bleu([r.split()], g.split(), smoothing_function=bleu_scorer.method1) for r, g in zip(reference_questions, generated_questions)]\n",
    "\n",
    "# Calculate accuracy rate (percentage of questions that match reference questions)\n",
    "accuracy_rate = sum(score == 1.0 for score in bleu_scores) / len(bleu_scores) * 100\n",
    "\n",
    "print(\"Accuracy Rate: {:.2f}%\".format(accuracy_rate))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq2SeqModelOutput(last_hidden_state=tensor([[[ 2.5253e-01,  1.5952e-01, -1.9853e-01,  ...,  1.0274e-01,\n",
      "          -3.6560e-04, -8.1298e-03],\n",
      "         [ 1.8470e-01,  1.0938e-01, -1.7418e-01,  ...,  3.2739e-02,\n",
      "          -5.3408e-04, -5.0218e-02],\n",
      "         [ 2.8847e-01,  2.3717e-01, -7.3225e-02,  ...,  5.7853e-02,\n",
      "          -3.4904e-04, -9.3356e-02],\n",
      "         [ 4.6172e-02,  4.3064e-01, -7.4659e-02,  ...,  5.6104e-02,\n",
      "          -3.3899e-04, -1.2440e-01]]], grad_fn=<MulBackward0>), past_key_values=((tensor([[[[ 1.3420,  1.0303, -1.1059,  ...,  0.4331, -0.6219, -0.5857],\n",
      "          [ 2.6920, -0.3059,  0.6619,  ...,  0.3362,  2.2622, -2.3633],\n",
      "          [ 2.3041,  1.6750, -1.5346,  ..., -0.7961, -0.8684, -0.7488],\n",
      "          [ 0.8822,  0.3299, -1.1180,  ..., -0.4515, -0.1813, -1.0115]],\n",
      "\n",
      "         [[ 2.1050,  0.3594, -0.1215,  ..., -0.6568,  0.0301, -1.4927],\n",
      "          [ 1.8789, -0.3770,  0.0770,  ..., -0.0882, -0.0668,  0.3396],\n",
      "          [ 0.6981, -1.4311,  0.0749,  ...,  0.9702,  1.5700, -0.2472],\n",
      "          [-0.0718, -1.8361, -0.6670,  ..., -0.6290,  0.0926,  0.7815]],\n",
      "\n",
      "         [[-1.9993, -0.5563,  0.4198,  ..., -1.2902,  1.8640,  0.2006],\n",
      "          [ 0.8220,  0.1163, -1.0906,  ..., -1.0260,  0.2069,  0.1699],\n",
      "          [ 0.5395, -0.8147,  0.0076,  ..., -0.3199, -0.3299, -0.7068],\n",
      "          [ 1.0025,  0.1487, -0.5932,  ...,  1.4527,  1.2500,  0.1555]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.5321, -1.3110, -0.1119,  ...,  0.8827,  2.4946,  0.6760],\n",
      "          [-0.0612, -0.2981, -1.7833,  ..., -0.9530,  0.2336, -0.3942],\n",
      "          [-0.0536, -0.9715,  0.6109,  ...,  0.5674,  0.0988, -0.5013],\n",
      "          [ 0.2959, -2.4671, -1.4441,  ..., -0.3756,  1.3632, -0.3540]],\n",
      "\n",
      "         [[-2.0003, -1.4009, -1.1702,  ..., -1.1139, -1.1071, -2.0598],\n",
      "          [-1.2808,  0.6910, -2.1441,  ...,  0.5858,  0.0604,  0.2579],\n",
      "          [-1.8379, -0.2792, -2.8481,  ...,  0.3211,  2.9190,  0.1615],\n",
      "          [-0.7958, -0.5181, -1.6686,  ..., -0.1749,  0.6551,  0.6886]],\n",
      "\n",
      "         [[ 0.8150, -0.8560, -0.1998,  ..., -0.8926,  1.3495,  0.2403],\n",
      "          [ 0.0270, -0.5540, -0.6196,  ..., -0.6943,  0.5491,  0.9193],\n",
      "          [ 0.5297, -0.0251,  0.4972,  ...,  0.0768,  0.8497,  0.2250],\n",
      "          [ 1.2619, -0.4575,  1.5478,  ...,  1.1051,  0.6808, -1.2374]]]],\n",
      "       grad_fn=<TransposeBackward0>), tensor([[[[-1.2030, -0.0800, -0.8353,  ...,  0.1296,  0.4528,  2.5505],\n",
      "          [ 0.3424,  0.1389,  0.9328,  ..., -1.0254,  0.6240, -0.0830],\n",
      "          [ 0.2136, -0.1222,  0.0262,  ...,  0.1563,  0.2917,  1.1970],\n",
      "          [ 0.0833,  1.1035, -1.4480,  ..., -0.4279,  0.3702,  0.1257]],\n",
      "\n",
      "         [[ 0.2748,  0.5525, -0.5894,  ...,  0.1461,  0.9067, -0.9725],\n",
      "          [ 0.6591,  0.4361, -0.7001,  ...,  0.2979, -0.6403,  0.0997],\n",
      "          [ 0.2334, -0.3465,  0.3773,  ...,  1.0575, -0.7464, -0.0489],\n",
      "          [ 2.0778, -1.2436, -0.2788,  ...,  0.1330,  1.6115, -1.6080]],\n",
      "\n",
      "         [[-1.5747, -0.4144,  0.9654,  ...,  0.5599,  0.4875,  0.1638],\n",
      "          [-0.0749, -1.2657, -0.6202,  ...,  1.0615,  0.0835,  0.0663],\n",
      "          [-0.1422, -1.1654, -1.4651,  ...,  0.2468, -0.8250,  1.7838],\n",
      "          [-0.7428,  0.3852, -0.7413,  ...,  0.0935, -0.9202,  0.7779]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.3740, -0.4282, -0.9548,  ..., -1.6374, -1.4010, -0.2158],\n",
      "          [-1.7587, -0.6535, -0.2353,  ..., -0.6829, -0.7054, -0.8861],\n",
      "          [-0.7676,  0.4078,  0.1272,  ..., -0.5771, -1.0287, -0.8181],\n",
      "          [ 0.4231, -0.5454,  0.5188,  ...,  0.5438, -0.6774, -1.2099]],\n",
      "\n",
      "         [[-1.6324,  0.4654, -1.0860,  ...,  1.1180, -0.8837,  0.5495],\n",
      "          [-0.5849,  0.0052, -0.6306,  ...,  0.3195,  0.1717,  0.0112],\n",
      "          [-1.0274, -1.8104,  1.2701,  ...,  1.6422, -0.7127, -1.7600],\n",
      "          [-0.1201, -0.8176, -0.1246,  ...,  0.1747, -0.3611, -1.1205]],\n",
      "\n",
      "         [[ 1.0567, -1.7580,  0.9833,  ..., -0.0836,  0.4462,  0.0769],\n",
      "          [-1.9568, -2.1042,  0.2593,  ..., -1.4886,  2.7088, -0.9147],\n",
      "          [ 1.4026, -1.4261, -1.2642,  ..., -1.0275, -0.7669,  0.8463],\n",
      "          [-0.0849, -0.5589,  1.0846,  ..., -1.2484,  1.1162,  1.3824]]]],\n",
      "       grad_fn=<TransposeBackward0>), tensor([[[[ 1.3582e+00, -1.3688e+00, -2.6136e+00,  ...,  1.9532e-01,\n",
      "            3.2367e+00,  1.6584e+00],\n",
      "          [ 4.8816e-01,  1.4711e+00, -2.9769e+00,  ...,  6.5964e-01,\n",
      "            9.5685e-01, -9.3507e-01],\n",
      "          [-3.4820e-02, -2.3346e-01, -6.5759e-01,  ..., -5.5872e-01,\n",
      "            4.6330e-01,  1.0037e-01],\n",
      "          ...,\n",
      "          [-3.6909e-01, -8.3469e-01,  1.5953e+00,  ...,  1.4140e+00,\n",
      "            1.2306e+00, -1.6929e+00],\n",
      "          [ 4.7114e-02,  1.4860e+00,  4.0879e+00,  ...,  6.8159e-01,\n",
      "            2.3244e+00, -1.2511e+00],\n",
      "          [-1.2366e+00, -7.5191e-01, -4.4003e-01,  ...,  5.7451e-01,\n",
      "           -4.3301e-01, -1.2648e+00]],\n",
      "\n",
      "         [[-2.6116e+00, -2.4803e+00, -2.3537e+00,  ..., -1.7845e+00,\n",
      "            1.5359e+00,  4.8305e+00],\n",
      "          [ 9.7185e-01,  2.4708e+00, -1.3890e+00,  ..., -4.5792e+00,\n",
      "           -1.0198e+00,  2.7318e+00],\n",
      "          [ 5.0031e-02,  5.6632e-02,  9.3576e-02,  ...,  2.8549e+00,\n",
      "           -1.2723e-01,  4.2514e-02],\n",
      "          ...,\n",
      "          [ 2.1954e+00, -2.7077e+00, -1.3504e+00,  ..., -7.1096e+00,\n",
      "            1.1295e+00, -2.8789e-01],\n",
      "          [ 8.7581e-01, -2.6985e+00, -2.9836e-01,  ...,  1.6780e-01,\n",
      "           -3.1454e-01,  1.3295e+00],\n",
      "          [-2.3622e-01,  6.2014e-02,  1.9327e-01,  ...,  2.8795e+00,\n",
      "            5.8935e-01, -9.6473e-02]],\n",
      "\n",
      "         [[-3.0544e-01,  3.2596e+00, -1.8789e+00,  ..., -1.1050e+00,\n",
      "           -1.7493e+00, -1.2237e+00],\n",
      "          [-5.6868e-01,  1.2991e+00, -2.3912e+00,  ..., -5.4283e-01,\n",
      "           -9.5873e-02,  5.3671e-01],\n",
      "          [-1.3148e-01,  2.0969e-01, -2.9007e-01,  ...,  8.0299e-02,\n",
      "           -5.6172e-01,  3.2945e-01],\n",
      "          ...,\n",
      "          [ 9.4580e-01, -2.0194e+00,  2.9060e-01,  ...,  1.2288e+00,\n",
      "           -7.0897e-01, -2.6891e-01],\n",
      "          [-6.4119e-01, -7.8504e-01,  2.4164e+00,  ...,  1.6812e+00,\n",
      "           -1.7735e+00, -1.2230e+00],\n",
      "          [ 4.3735e-01,  4.6253e-01, -3.4447e-01,  ..., -3.6690e-02,\n",
      "            1.2426e+00,  1.1533e+00]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.0189e+00, -1.2081e+00,  1.3681e-01,  ...,  3.1688e+00,\n",
      "           -7.1118e-01, -2.6837e+00],\n",
      "          [ 2.5179e+00,  7.5541e-01, -6.4915e-01,  ...,  5.1308e-01,\n",
      "           -9.6867e-01, -1.2965e-01],\n",
      "          [-5.5224e-01, -6.3197e-01,  4.8524e-03,  ..., -2.4184e-01,\n",
      "           -5.4100e-01, -4.2154e-01],\n",
      "          ...,\n",
      "          [-1.9766e+00,  9.1354e-01, -3.7749e-01,  ..., -5.8034e-01,\n",
      "           -7.8516e-01,  2.8732e-01],\n",
      "          [ 1.4863e+00,  1.2788e+00, -1.5303e-01,  ..., -3.4331e-01,\n",
      "            6.5494e-01, -5.3088e-01],\n",
      "          [-3.0763e-01,  1.6252e+00, -1.1707e+00,  ..., -2.0234e-01,\n",
      "           -2.3431e+00,  8.2781e-01]],\n",
      "\n",
      "         [[-2.8769e+00, -2.5823e+00, -1.3678e+00,  ...,  2.0135e+00,\n",
      "           -5.5713e-01, -6.4559e-01],\n",
      "          [-4.4844e-01, -1.9905e+00,  7.4537e-02,  ...,  1.1615e+00,\n",
      "           -4.5633e-01, -6.9846e-01],\n",
      "          [-4.5656e-01, -1.1165e+00,  1.0864e-01,  ..., -2.3394e+00,\n",
      "            5.7440e-01,  2.5909e-01],\n",
      "          ...,\n",
      "          [-5.6753e-02, -1.0833e+00, -3.3745e+00,  ...,  1.6238e+00,\n",
      "            2.7182e-01,  1.1715e+00],\n",
      "          [ 4.6885e-01, -1.1852e+00, -2.7269e+00,  ...,  1.3615e+00,\n",
      "            2.6680e-01, -5.3005e-01],\n",
      "          [ 1.1900e+00, -1.3030e+00, -7.5851e-01,  ..., -1.0790e+00,\n",
      "            8.9144e-01, -9.4167e-01]],\n",
      "\n",
      "         [[ 1.5283e+00, -1.0090e+00,  6.4318e-01,  ..., -4.9611e-01,\n",
      "           -1.2749e-01,  2.0487e+00],\n",
      "          [ 2.2963e-01, -5.9705e-01,  2.7318e+00,  ...,  8.1679e-01,\n",
      "           -2.0888e+00, -9.6459e-01],\n",
      "          [ 5.2670e-01, -2.7995e-01,  7.8918e-01,  ...,  2.6660e-01,\n",
      "           -1.2957e+00, -7.1846e-01],\n",
      "          ...,\n",
      "          [ 1.0377e+00, -1.6158e+00,  4.7984e-01,  ...,  5.0230e-01,\n",
      "           -1.1166e+00, -1.3858e+00],\n",
      "          [-9.8411e-01, -8.0852e-01, -1.1132e+00,  ...,  9.8022e-01,\n",
      "           -5.2367e-01, -3.7916e+00],\n",
      "          [ 1.5539e-01,  2.6377e-01, -1.0498e+00,  ..., -8.2589e-01,\n",
      "            1.4876e+00,  1.2063e+00]]]], grad_fn=<TransposeBackward0>), tensor([[[[ 3.5710e+00,  3.8561e-01,  5.1172e+00,  ...,  1.1487e+00,\n",
      "            6.3301e-01, -4.1174e+00],\n",
      "          [ 9.3708e-01,  1.7596e+00,  4.2972e+00,  ...,  1.9408e+00,\n",
      "           -1.7799e+00, -3.4230e+00],\n",
      "          [-2.2007e-01, -2.9037e-01,  1.1522e+00,  ..., -5.0980e-03,\n",
      "            3.0630e-01,  5.7117e-02],\n",
      "          ...,\n",
      "          [-1.1080e+00, -4.1206e-01, -9.7637e-01,  ...,  1.2982e-01,\n",
      "           -4.1427e-01, -1.6462e+00],\n",
      "          [-2.6057e+00,  1.3516e+00, -2.3685e+00,  ..., -1.3207e+00,\n",
      "            6.0655e-01, -2.5884e+00],\n",
      "          [ 4.5016e-01,  5.8991e-01,  4.3866e-01,  ..., -6.6454e-01,\n",
      "           -6.2119e-01, -8.7223e-01]],\n",
      "\n",
      "         [[-7.3915e-01,  3.6722e+00,  1.1131e+00,  ..., -1.1000e+00,\n",
      "            2.1930e-01,  5.1473e+00],\n",
      "          [ 4.2034e-01,  1.8519e+00,  3.2707e+00,  ..., -1.5454e+00,\n",
      "            8.2073e-01,  2.1752e+00],\n",
      "          [-3.5924e-01, -2.7475e-01,  1.7883e-01,  ..., -7.5437e-02,\n",
      "            6.7707e-02,  5.8684e-01],\n",
      "          ...,\n",
      "          [ 4.0669e-01,  4.7740e-01,  1.8417e+00,  ..., -1.7772e+00,\n",
      "           -1.5528e+00,  2.6745e+00],\n",
      "          [-1.8376e-01,  4.1884e+00,  6.9301e-01,  ...,  1.6765e+00,\n",
      "           -2.8483e+00,  6.1767e-03],\n",
      "          [ 3.5209e-01, -1.7663e+00,  1.6919e-01,  ..., -1.6355e-01,\n",
      "            2.3668e-01,  5.3860e-01]],\n",
      "\n",
      "         [[ 2.4060e-02, -4.5685e+00,  4.8092e+00,  ..., -9.0269e-01,\n",
      "           -1.8915e+00,  1.8847e+00],\n",
      "          [-1.8748e+00, -5.2107e+00,  6.2888e+00,  ...,  2.1601e+00,\n",
      "           -7.1312e-01,  2.8423e+00],\n",
      "          [-1.5528e+00, -4.5150e-01,  1.8584e+00,  ..., -1.9937e-01,\n",
      "            4.7913e-01, -4.2713e-01],\n",
      "          ...,\n",
      "          [ 3.7675e+00,  8.4909e-01,  5.3550e+00,  ...,  2.1126e+00,\n",
      "            2.7623e+00, -5.7876e-01],\n",
      "          [ 3.8056e+00,  5.5072e+00,  5.4231e+00,  ...,  7.3559e-01,\n",
      "            9.2802e-02, -1.5557e+00],\n",
      "          [-1.6923e+00, -4.6872e-01,  5.9505e+00,  ...,  7.2208e-01,\n",
      "            8.8573e-01, -1.7663e+00]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-2.0819e+00, -3.6415e+00,  4.5109e+00,  ...,  1.9469e+00,\n",
      "            8.6288e-01, -1.3432e+00],\n",
      "          [ 2.7320e-01, -1.9323e+00,  3.4639e+00,  ...,  6.8585e-01,\n",
      "           -1.7927e+00,  4.2697e-01],\n",
      "          [ 1.2343e-02, -2.3933e-01, -4.2496e-01,  ...,  9.5064e-01,\n",
      "            1.7476e-01,  7.5352e-01],\n",
      "          ...,\n",
      "          [ 4.8429e-01, -1.3660e-01,  2.0481e+00,  ...,  5.5355e-01,\n",
      "            6.4133e-01, -6.6990e-02],\n",
      "          [ 3.1791e+00,  1.7775e+00,  4.1049e-01,  ...,  6.8242e-01,\n",
      "            1.4528e+00, -2.3535e+00],\n",
      "          [ 6.8695e-01, -4.6403e-01, -1.1944e+00,  ...,  1.1717e+00,\n",
      "            1.8048e+00,  2.3457e+00]],\n",
      "\n",
      "         [[-1.0380e+00,  4.1629e-01,  2.0590e+00,  ...,  4.0927e+00,\n",
      "            7.9436e-01,  1.2380e+00],\n",
      "          [-3.0686e-01, -5.8389e-01, -3.6276e+00,  ...,  1.0985e+00,\n",
      "            1.3471e+00, -1.2751e+00],\n",
      "          [ 9.0208e-02,  1.4635e-01,  2.5163e-01,  ...,  2.7183e-01,\n",
      "            3.1209e-01, -1.2217e-01],\n",
      "          ...,\n",
      "          [-3.2481e+00, -3.3724e-01,  2.5831e+00,  ...,  1.9319e+00,\n",
      "            1.8146e+00, -1.1522e+00],\n",
      "          [-1.0646e+00, -1.8841e+00,  8.3781e-01,  ...,  1.0728e+00,\n",
      "            1.1899e+00,  2.4533e-01],\n",
      "          [-1.3955e+00,  3.5425e-01,  1.6330e+00,  ...,  3.0674e-01,\n",
      "            2.1723e+00,  7.6671e-01]],\n",
      "\n",
      "         [[ 2.6274e+00, -4.5307e+00,  1.1794e+00,  ..., -5.4835e+00,\n",
      "            1.8375e+00,  1.3591e+00],\n",
      "          [-3.2785e-01, -9.4960e-01, -3.4357e-01,  ..., -3.4552e+00,\n",
      "           -5.7104e-01,  1.7616e+00],\n",
      "          [ 1.8553e-01,  1.7673e-01, -8.4285e-01,  ...,  2.8079e-01,\n",
      "           -2.5793e-01,  2.1979e-01],\n",
      "          ...,\n",
      "          [ 4.6624e-02,  7.9646e-01,  7.4668e-01,  ..., -2.5657e-01,\n",
      "            8.0600e-01, -3.6572e+00],\n",
      "          [-3.9932e+00, -1.1632e+00,  2.0212e+00,  ...,  3.2867e+00,\n",
      "           -4.2012e+00, -2.2540e+00],\n",
      "          [ 1.2721e+00,  2.0365e-01,  5.0135e-01,  ..., -7.0575e-01,\n",
      "           -1.4886e+00, -1.2259e+00]]]], grad_fn=<TransposeBackward0>)), (tensor([[[[-0.4363,  1.0918,  1.4477,  ...,  0.8033, -0.6240, -1.8955],\n",
      "          [-1.0561,  1.1053,  1.2752,  ...,  0.2941, -0.7104, -0.6171],\n",
      "          [ 0.6138, -0.5102,  0.8021,  ...,  0.3960, -0.0387, -1.3379],\n",
      "          [-0.8896, -0.4561,  0.5241,  ...,  0.9593, -0.1200, -0.9566]],\n",
      "\n",
      "         [[ 0.1736, -0.3554, -0.5005,  ...,  1.9365, -0.0613, -0.4486],\n",
      "          [ 0.8229, -0.0708, -0.7474,  ...,  0.9631, -0.7395,  0.2248],\n",
      "          [ 0.8478, -0.7734, -2.3346,  ...,  1.4238, -1.6361,  0.3999],\n",
      "          [ 0.1428, -0.3654, -2.8293,  ...,  0.6981, -0.2803,  1.8600]],\n",
      "\n",
      "         [[ 1.9905, -0.6627, -0.5344,  ..., -1.3702, -0.6738, -0.2995],\n",
      "          [ 2.5830, -1.1139, -0.2099,  ..., -1.5733,  0.2958, -0.9174],\n",
      "          [ 1.3014, -0.4578, -0.1984,  ..., -1.0696, -0.6753, -0.9137],\n",
      "          [-1.3670, -1.1617,  1.1661,  ..., -1.4248, -0.1825, -1.6608]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.4761,  0.3877, -1.2247,  ..., -0.9188, -0.6090, -0.0795],\n",
      "          [ 1.1380, -0.6294, -0.3371,  ..., -0.5192, -1.6053, -0.2479],\n",
      "          [ 1.2428, -0.7770, -0.6877,  ..., -0.0420, -0.7623, -1.0145],\n",
      "          [-0.2015,  0.9301,  0.3864,  ...,  0.8078, -0.2655,  0.0325]],\n",
      "\n",
      "         [[ 0.4212,  1.0433,  0.2407,  ..., -0.7641,  0.1378, -0.1918],\n",
      "          [-0.8305,  0.6519, -0.4354,  ...,  0.8905, -0.0146, -0.4993],\n",
      "          [-1.4067,  0.4045,  0.1988,  ..., -0.6510, -0.4723, -0.4506],\n",
      "          [ 0.1851,  1.6294, -1.2341,  ..., -1.1424, -1.2400, -1.7801]],\n",
      "\n",
      "         [[-0.3418, -0.5109, -0.8390,  ...,  0.3074,  0.4754,  0.1895],\n",
      "          [-0.7059,  0.7263,  0.0472,  ...,  0.1853,  0.5666, -0.0748],\n",
      "          [-0.8113,  1.9149, -0.3756,  ...,  0.5584,  1.1605, -1.1034],\n",
      "          [-1.5177, -0.3292, -1.1729,  ...,  0.9802,  0.4994,  0.0309]]]],\n",
      "       grad_fn=<TransposeBackward0>), tensor([[[[ 1.2061, -0.4029,  0.6083,  ...,  0.7924, -1.8236, -0.0871],\n",
      "          [ 1.1776,  1.3924,  0.1268,  ...,  1.1760, -0.0665,  1.5477],\n",
      "          [ 0.1030,  0.6997, -1.9195,  ...,  2.3335, -1.3549,  1.4725],\n",
      "          [-1.2101,  0.9592, -1.5674,  ...,  1.9627,  0.9170,  0.0963]],\n",
      "\n",
      "         [[-0.3783, -0.1432, -1.9981,  ..., -2.0912,  1.0230, -1.2474],\n",
      "          [ 0.5350, -0.8984, -1.8779,  ...,  0.2479,  1.3941, -1.6714],\n",
      "          [ 0.2187, -2.1216, -0.9977,  ...,  2.2255, -0.4184, -1.8839],\n",
      "          [ 0.2221,  2.6789, -1.3578,  ...,  1.0752,  0.5896,  0.4052]],\n",
      "\n",
      "         [[-0.7246,  2.1487, -0.2429,  ..., -0.0373,  0.9440, -1.9476],\n",
      "          [ 1.5238,  2.5744, -2.5605,  ...,  0.5666,  0.3010, -1.4145],\n",
      "          [ 1.2516,  2.6488, -0.8583,  ..., -0.7608,  0.5069, -0.4277],\n",
      "          [-0.4032,  1.9411,  0.0210,  ...,  1.0765, -0.9739,  0.0674]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.2609,  2.6259, -4.6480,  ..., -1.5807,  0.2452,  0.5202],\n",
      "          [-1.0581,  0.6838, -3.3325,  ...,  0.8319,  0.7132,  0.6538],\n",
      "          [-1.8102, -1.1115, -0.3631,  ...,  3.0410,  1.0721,  1.2249],\n",
      "          [-0.2678,  0.7724, -0.3490,  ..., -0.6826,  2.1185,  2.2124]],\n",
      "\n",
      "         [[ 1.1881, -3.5383, -1.5894,  ..., -0.0371, -0.4441, -1.8830],\n",
      "          [ 1.2438,  2.5171, -1.0108,  ..., -3.4931, -0.3558, -1.9771],\n",
      "          [ 1.7324,  1.6227, -1.9681,  ..., -5.0333, -1.1564, -0.2942],\n",
      "          [ 0.5531, -2.0263,  1.9917,  ...,  1.5757,  1.1426,  0.7687]],\n",
      "\n",
      "         [[ 1.6031,  1.3517,  2.3583,  ..., -2.7124,  2.0689, -0.7339],\n",
      "          [ 0.9127,  3.6490,  0.7658,  ..., -4.3904,  1.2229,  2.1874],\n",
      "          [ 3.5986,  1.4265, -3.0828,  ..., -6.5202, -2.3722,  3.1632],\n",
      "          [-0.1642,  4.5532, -0.7463,  ..., -0.3784, -0.3643,  0.1064]]]],\n",
      "       grad_fn=<TransposeBackward0>), tensor([[[[-1.6071,  0.5500, -1.2510,  ..., -0.0385,  2.1545, -0.0903],\n",
      "          [-0.9909,  2.1020,  1.1848,  ...,  0.1079,  0.8082,  0.3634],\n",
      "          [-0.4647,  0.0858,  0.0770,  ..., -0.1240,  0.0717,  0.9304],\n",
      "          ...,\n",
      "          [-0.6266, -2.0039,  0.0391,  ..., -0.8990,  0.6631,  1.5553],\n",
      "          [ 0.6517, -2.0777, -1.1311,  ..., -0.7303, -0.7973,  2.1332],\n",
      "          [-0.5112, -0.2389, -0.4533,  ...,  0.7295, -0.8987,  1.7291]],\n",
      "\n",
      "         [[-0.2481, -2.4122,  1.7999,  ...,  0.6093, -2.7153, -1.3227],\n",
      "          [-3.4674, -1.4140,  0.1012,  ..., -0.6814, -0.2287, -0.0370],\n",
      "          [ 0.8702, -0.2469, -0.0346,  ...,  0.8214,  0.8954, -0.6228],\n",
      "          ...,\n",
      "          [-0.8223,  0.1143,  1.4228,  ..., -0.4280,  0.6913,  1.6923],\n",
      "          [ 0.9414, -1.7165,  1.3394,  ..., -0.0743,  1.9548,  1.1288],\n",
      "          [-0.8629,  0.4033,  0.3434,  ...,  0.1789, -0.1536,  0.2127]],\n",
      "\n",
      "         [[ 0.9217, -0.5519,  0.4453,  ..., -0.3215, -2.0011, -0.0383],\n",
      "          [ 0.5482, -0.7816,  0.2427,  ..., -0.7338, -0.1328, -1.5858],\n",
      "          [ 1.9084, -1.3163,  0.3410,  ..., -0.3818,  0.2067, -1.2600],\n",
      "          ...,\n",
      "          [-0.0429,  0.7482, -0.9789,  ..., -1.0557, -0.6108, -1.2207],\n",
      "          [ 0.3928,  0.4663, -1.0583,  ...,  0.2694, -1.3627,  0.1235],\n",
      "          [ 0.1975,  0.4706, -1.8880,  ..., -1.9834, -0.8534,  0.6398]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.3225, -0.4520,  2.0798,  ..., -0.7042, -2.6784, -0.8591],\n",
      "          [-1.3369, -0.4154, -0.7754,  ..., -2.5097, -3.0909,  0.6210],\n",
      "          [ 0.3669, -0.5437,  1.2631,  ...,  0.2682,  0.7823,  1.0195],\n",
      "          ...,\n",
      "          [-2.2676,  1.8897, -0.5618,  ..., -1.4596,  0.3708, -1.1898],\n",
      "          [-0.8175,  3.0012, -0.3117,  ..., -1.6775, -0.0903, -0.8405],\n",
      "          [ 0.7307, -0.3564,  1.7502,  ..., -0.3326,  0.5064,  0.6556]],\n",
      "\n",
      "         [[ 3.1470,  0.6271,  0.5830,  ..., -0.7236, -0.6388,  1.3216],\n",
      "          [ 1.6181,  2.1614, -2.1468,  ..., -0.1922, -1.0269,  1.1239],\n",
      "          [ 0.2806,  0.0711,  1.8818,  ..., -0.6550,  0.4110,  0.7345],\n",
      "          ...,\n",
      "          [ 0.5397,  2.3510, -6.4039,  ...,  2.1021, -1.3653, -0.1811],\n",
      "          [ 0.8332,  1.2864, -1.7758,  ...,  0.9541,  0.1388,  1.8867],\n",
      "          [ 0.9729,  0.1497,  0.8953,  ..., -1.0670, -0.2214,  1.2109]],\n",
      "\n",
      "         [[ 0.5349, -0.8809,  2.6725,  ..., -0.0336, -0.7706,  5.0493],\n",
      "          [ 0.2889, -0.6605,  1.3428,  ..., -0.1244,  1.1702,  2.6305],\n",
      "          [ 3.6297,  0.0334, -2.7400,  ..., -0.9739, -0.5968, -0.0387],\n",
      "          ...,\n",
      "          [-0.0786,  1.1098,  0.7905,  ...,  0.5794, -2.3771,  5.6924],\n",
      "          [ 0.0447,  0.3744,  0.9823,  ..., -0.1066, -1.5556,  7.6279],\n",
      "          [ 2.2738, -0.5241, -0.4428,  ..., -1.5857, -0.9958,  3.8957]]]],\n",
      "       grad_fn=<TransposeBackward0>), tensor([[[[-0.9959,  3.0473,  4.0455,  ..., -1.0980, -0.9562,  2.0744],\n",
      "          [-3.1130,  1.0316,  3.8335,  ...,  1.5999,  0.6902, -2.1380],\n",
      "          [-0.0865, -0.1721, -0.7766,  ..., -0.8762, -0.5358, -0.1325],\n",
      "          ...,\n",
      "          [-1.1317,  1.3338, -1.4618,  ..., -0.5842, -3.1221, -0.4266],\n",
      "          [-1.6500,  1.7861, -1.3124,  ...,  0.6770, -2.8419,  0.3075],\n",
      "          [-0.8756,  1.5071, -0.2722,  ..., -0.7785, -0.2589,  1.6870]],\n",
      "\n",
      "         [[-1.8681,  0.2943,  1.0223,  ...,  5.2542,  3.8577,  0.4940],\n",
      "          [-1.6827,  3.6897,  1.2236,  ...,  3.4877, -0.4370,  4.0403],\n",
      "          [-0.2418, -0.5476, -0.3137,  ..., -0.7359,  0.1284, -0.6308],\n",
      "          ...,\n",
      "          [ 1.2216,  0.7777, -1.3095,  ...,  1.7714,  0.5900, -0.8178],\n",
      "          [ 0.9659,  1.3949, -0.0393,  ...,  3.1896,  2.4503, -0.3980],\n",
      "          [-0.1349, -1.9151, -0.7575,  ...,  0.9530, -1.7817,  5.1909]],\n",
      "\n",
      "         [[-3.9804, -1.1690, -4.3660,  ..., -0.7566,  3.5411, -0.4159],\n",
      "          [-0.7905, -1.7110, -2.7582,  ..., -3.7613,  3.9255,  5.6483],\n",
      "          [-0.1728,  0.2874,  0.5130,  ...,  0.1292,  0.1954, -0.7126],\n",
      "          ...,\n",
      "          [-2.1550, -1.4152, -2.0261,  ..., -1.3716, -2.8505, -2.5977],\n",
      "          [-1.8670, -2.7485, -0.5017,  ...,  1.2944,  0.3980, -4.6700],\n",
      "          [-0.6007, -4.0581,  1.1122,  ...,  1.0227,  0.6704,  0.2870]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 4.1189,  0.0996, -2.4223,  ...,  0.1331, -0.9797,  1.8390],\n",
      "          [ 0.8040, -3.5256,  0.6423,  ..., -3.7997,  0.7137, -0.1472],\n",
      "          [-0.0140,  0.1726,  0.5829,  ...,  0.2781, -0.0238,  0.4926],\n",
      "          ...,\n",
      "          [-0.0419, -0.6777, -2.4989,  ...,  1.0146,  0.7628, -0.0607],\n",
      "          [ 1.7777,  0.1033, -2.1157,  ..., -2.4493,  1.9264,  2.2615],\n",
      "          [-0.4446,  0.6451, -1.4833,  ...,  0.3589, -2.7630, -0.4835]],\n",
      "\n",
      "         [[ 0.1578, -4.9430,  4.7985,  ...,  0.0217,  2.7576,  0.8828],\n",
      "          [ 1.1182, -1.9553, -0.4896,  ..., -0.4731, -3.1219,  0.9936],\n",
      "          [-0.1130,  0.6541,  0.0304,  ..., -0.1712,  0.8114,  0.5467],\n",
      "          ...,\n",
      "          [-3.3759,  2.1051, -0.8625,  ...,  0.3785, -1.6885, -1.0282],\n",
      "          [-2.1127,  2.2200, -4.9847,  ...,  3.0749,  0.0463,  0.3722],\n",
      "          [ 2.1780,  3.8733, -0.4272,  ...,  0.5676,  1.9259, -1.8011]],\n",
      "\n",
      "         [[ 0.4663,  2.9045, -2.2109,  ..., -2.0853,  1.0649, -0.8903],\n",
      "          [-0.6063, -0.2197, -0.1358,  ..., -2.8000, -1.1183, -1.7987],\n",
      "          [-0.5647,  0.1976, -1.4597,  ...,  0.4219, -0.9870, -0.1626],\n",
      "          ...,\n",
      "          [-1.6884,  1.3824, -0.4812,  ..., -1.2269, -2.7542,  5.0504],\n",
      "          [-0.8248,  5.3453,  2.0814,  ...,  0.6048,  2.0195,  3.6034],\n",
      "          [-0.1307, -1.7332,  0.7972,  ..., -1.0543, -0.1341,  0.8845]]]],\n",
      "       grad_fn=<TransposeBackward0>)), (tensor([[[[-3.3295, -0.3773,  1.9745,  ...,  0.0921,  0.4266, -0.3503],\n",
      "          [-2.6196, -0.5800,  0.6615,  ..., -1.0723, -1.4349, -0.2007],\n",
      "          [-1.3012, -0.4186,  0.2299,  ..., -0.3258, -2.3158, -1.0601],\n",
      "          [-2.7483,  0.7362,  0.2466,  ...,  0.1108, -1.1857, -0.3763]],\n",
      "\n",
      "         [[-0.6591, -0.8539, -1.8497,  ...,  1.9679, -0.4271,  0.1611],\n",
      "          [-0.4435, -0.2142, -2.3121,  ...,  1.0715, -1.3506, -0.7771],\n",
      "          [-1.3249, -0.6601, -1.4563,  ...,  1.3352, -0.9148,  0.6294],\n",
      "          [-0.8372, -1.6850, -3.4187,  ...,  1.0918, -1.7197, -0.2062]],\n",
      "\n",
      "         [[-1.0027,  1.8910,  1.7379,  ...,  0.4384,  0.5961,  2.0784],\n",
      "          [-0.4728,  2.7105,  1.3279,  ...,  0.1394, -0.1687,  0.1038],\n",
      "          [-0.9961,  1.7393,  0.7270,  ...,  0.9319,  0.0065, -0.5046],\n",
      "          [-1.7607,  2.2302,  1.4481,  ...,  1.6580,  1.3084,  0.5539]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0985, -1.6257, -1.6154,  ...,  0.2847, -0.1788,  2.1265],\n",
      "          [ 0.0942, -2.4905, -1.1734,  ..., -0.1812,  0.7448,  0.4356],\n",
      "          [ 1.2284, -0.8107, -1.2007,  ..., -0.8700,  0.3241,  0.4606],\n",
      "          [ 1.7044, -1.5906,  0.9041,  ...,  0.7933,  0.1645,  0.1744]],\n",
      "\n",
      "         [[ 1.1709, -0.5258,  0.9128,  ..., -2.5419,  1.1253, -0.2605],\n",
      "          [ 1.8264, -0.0608,  0.8476,  ..., -2.9328,  1.7123,  0.3235],\n",
      "          [ 0.5568, -0.0912,  0.4087,  ..., -3.8542,  2.0172, -1.7351],\n",
      "          [-2.9257,  2.1459,  0.7746,  ..., -2.6555,  1.2543, -2.8574]],\n",
      "\n",
      "         [[-0.4022,  1.3242, -2.2177,  ...,  0.7617,  1.7890, -1.1437],\n",
      "          [-0.0268,  1.1522, -0.6246,  ...,  1.6409,  2.3459, -2.4197],\n",
      "          [-0.3250,  1.4115, -0.5663,  ...,  1.3773,  1.0758, -2.4867],\n",
      "          [-0.6647,  2.2805, -1.8732,  ...,  0.7455,  1.6797, -2.8458]]]],\n",
      "       grad_fn=<TransposeBackward0>), tensor([[[[ 6.4732, -1.9733,  0.8014,  ..., -0.6202,  1.4488, -1.3383],\n",
      "          [ 1.3061, -0.2055, -2.6317,  ..., -1.0974, -0.1319, -0.0818],\n",
      "          [ 2.3426,  0.9713,  0.8730,  ..., -0.4344,  2.2630, -1.4108],\n",
      "          [ 2.2644,  1.4431,  2.4263,  ...,  0.2113, -1.2211, -2.3213]],\n",
      "\n",
      "         [[-0.3617, -2.9242, -2.8533,  ...,  1.5130,  0.5739,  1.3307],\n",
      "          [-5.5687,  0.3644, -2.1265,  ...,  0.0130, -0.1007,  2.0223],\n",
      "          [-3.7150,  1.3147, -2.2505,  ..., -4.8728,  1.3062,  2.4316],\n",
      "          [-2.0341, -1.8859,  1.5484,  ..., -3.9737, -1.2701,  5.5611]],\n",
      "\n",
      "         [[-0.6575, -1.4235,  1.0631,  ..., -3.3649, -5.7881, -0.6469],\n",
      "          [-0.8119,  1.1446,  0.3213,  ..., -3.9769, -0.6877,  0.3651],\n",
      "          [ 0.7841,  0.6177, -1.6416,  ..., -1.0065,  1.0410,  3.4824],\n",
      "          [-1.9690, -2.2176, -1.8229,  ...,  1.2718,  2.2549,  1.8313]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.3793, -1.0354,  0.5836,  ..., -1.7748, -5.2791, -1.9538],\n",
      "          [ 1.7855,  2.0846,  1.9223,  ..., -1.6252, -3.8616, -0.4922],\n",
      "          [ 1.7096,  3.4552, -1.1427,  ..., -1.4221, -5.2002,  0.7383],\n",
      "          [-0.2546,  0.1568, -0.5557,  ..., -0.3723, -0.7616, -0.8587]],\n",
      "\n",
      "         [[ 0.1223, -3.5038, -0.9999,  ...,  1.2864,  1.4365,  0.6822],\n",
      "          [ 2.1501, -1.2502,  0.3565,  ...,  0.9472,  1.1609,  1.8189],\n",
      "          [ 3.8600, -1.2747, -0.9856,  ..., -2.3227, -0.8792, -0.3846],\n",
      "          [-0.4886, -0.5673, -0.4328,  ...,  1.2418,  0.6276, -1.3371]],\n",
      "\n",
      "         [[-0.3847,  4.4138,  1.3879,  ..., -0.7284,  0.4929,  0.3771],\n",
      "          [-2.3784,  3.0967,  2.4110,  ..., -0.3655, -0.6468,  0.0582],\n",
      "          [ 0.9568,  4.0513,  1.8337,  ...,  0.3024, -1.3134, -0.2439],\n",
      "          [ 1.4179,  5.3091, -1.8307,  ..., -0.0158,  1.4479, -4.3587]]]],\n",
      "       grad_fn=<TransposeBackward0>), tensor([[[[-0.7769, -1.3523, -1.7775,  ...,  0.3853, -0.3741, -0.2185],\n",
      "          [-1.1861, -0.6588,  0.9135,  ..., -1.2941,  0.8858,  0.7222],\n",
      "          [-1.0017, -0.7642, -0.2530,  ..., -1.7758, -0.8242, -0.0506],\n",
      "          ...,\n",
      "          [-2.7410, -0.0767, -1.1998,  ...,  1.4844, -0.9316,  0.9096],\n",
      "          [-1.7433, -1.1915, -0.3084,  ..., -0.5376, -2.4246, -0.0187],\n",
      "          [-1.8069,  1.1095, -0.9962,  ..., -0.0750, -1.0722, -0.3851]],\n",
      "\n",
      "         [[ 0.0740, -0.7344,  2.0153,  ...,  0.3410, -1.1899, -1.4911],\n",
      "          [-2.3678, -0.8682, -0.5056,  ..., -1.4527, -2.0222,  1.4493],\n",
      "          [ 3.3371,  0.0762,  0.1428,  ...,  0.2279,  0.3488,  0.6221],\n",
      "          ...,\n",
      "          [-2.5149,  0.9956, -0.5374,  ..., -0.5273, -1.5411,  1.7195],\n",
      "          [-3.6614, -0.5116, -0.3225,  ...,  0.8464, -0.1562, -0.9086],\n",
      "          [ 1.8915,  0.1400, -1.2716,  ..., -0.2179, -0.9942, -0.5467]],\n",
      "\n",
      "         [[ 1.5075, -0.6466,  2.4210,  ...,  2.5605,  1.0468, -2.2957],\n",
      "          [ 2.4213,  0.9115,  1.1737,  ...,  0.7452, -0.8122, -3.3524],\n",
      "          [-1.0395, -0.0442,  1.9090,  ..., -2.0069, -0.5107, -0.6489],\n",
      "          ...,\n",
      "          [-2.9142, -0.3992, -0.2662,  ...,  5.0430, -0.6043,  0.8248],\n",
      "          [-0.3434, -0.7619, -0.9610,  ...,  4.6707,  1.2051,  0.4701],\n",
      "          [-0.1148,  1.8205,  0.6907,  ...,  0.4717,  0.0167, -1.9060]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.4090,  0.4297,  1.5569,  ..., -0.3503, -1.7638,  3.0802],\n",
      "          [-0.3346, -3.3027, -0.7588,  ...,  0.2347, -1.5581,  4.6325],\n",
      "          [-2.0047,  0.4970, -0.5991,  ..., -0.4688,  1.2997, -2.3578],\n",
      "          ...,\n",
      "          [-1.0421,  0.9772,  1.8932,  ..., -1.5095,  0.3064,  0.9337],\n",
      "          [-3.3642,  1.9048,  0.8416,  ..., -0.6059, -0.0795,  2.2556],\n",
      "          [-2.3122,  0.3266,  0.4885,  ...,  0.6734,  0.2014,  0.0993]],\n",
      "\n",
      "         [[-0.4006,  0.6956,  2.4664,  ...,  2.8957,  1.4931, -2.0086],\n",
      "          [ 2.8464,  0.0640, -0.0763,  ..., -2.5234, -2.5562, -1.6313],\n",
      "          [ 0.5780, -0.1370, -0.6583,  ...,  0.0216,  0.3049, -0.9197],\n",
      "          ...,\n",
      "          [-0.5186,  1.8561, -2.8003,  ..., -0.5944,  0.3839, -5.2694],\n",
      "          [-2.4768,  0.7067, -0.8348,  ..., -1.5834,  1.9664, -4.1861],\n",
      "          [-2.3643,  1.5880,  0.5904,  ..., -0.4716, -0.4175, -1.4148]],\n",
      "\n",
      "         [[-1.4688, -0.6635,  0.7674,  ..., -1.2810, -0.5651,  1.6107],\n",
      "          [-0.5536, -2.7953, -2.2454,  ..., -0.0063,  1.3967,  1.5423],\n",
      "          [ 0.2135,  1.4813,  1.2553,  ...,  1.2630, -0.7095,  1.3434],\n",
      "          ...,\n",
      "          [ 1.4942,  0.5926, -0.2276,  ...,  0.0230,  2.0664,  0.3148],\n",
      "          [-0.6305,  0.5198, -0.3938,  ...,  0.8946,  1.8107,  1.7140],\n",
      "          [-0.3087,  0.0982,  0.0703,  ...,  1.8969,  1.5546,  1.5847]]]],\n",
      "       grad_fn=<TransposeBackward0>), tensor([[[[ -0.3308,   7.1236,  -6.4550,  ...,   1.3874,  -3.6430,  -1.1290],\n",
      "          [  2.0542,   6.8345,  -4.7966,  ...,   3.1711,  -1.5129,   8.6704],\n",
      "          [  0.0103,   0.0951,  -0.1445,  ...,  -0.2760,  -0.0108,  -0.0460],\n",
      "          ...,\n",
      "          [ -1.1823,  -1.8889,  -0.7717,  ...,  -1.1111,   3.4052,  -3.9204],\n",
      "          [ -8.6769,  -5.8491,  -4.9544,  ...,   4.5190,   5.4169,  -4.5175],\n",
      "          [  0.7557,  -0.7964,   3.1881,  ...,   2.3185,  -2.5781,  -0.0532]],\n",
      "\n",
      "         [[ -1.5801,   4.1592,   0.6258,  ...,   2.0278,   1.1306,   4.7229],\n",
      "          [  0.5791,  -0.4088,  -5.9749,  ...,   1.4200,   3.2491,   2.0889],\n",
      "          [  0.3316,  -1.9952,  -2.0550,  ...,   1.8983,   0.6483,   1.2560],\n",
      "          ...,\n",
      "          [ -1.1038,  -3.5967,  -0.2134,  ...,  -1.2562,   1.5417,  -1.8506],\n",
      "          [  0.0725,  -2.2227,  -3.5574,  ...,   0.5644,   6.3971,   1.4925],\n",
      "          [ -1.3786,   5.2608,  -2.1270,  ...,   0.1095,   1.1946,   0.6019]],\n",
      "\n",
      "         [[ -3.2376,  -0.4373,   1.8551,  ...,   5.0997,  -2.8373, -10.1589],\n",
      "          [ -5.3160,  -2.5299,   2.0656,  ...,  -0.6194,  -1.0752,  -6.7195],\n",
      "          [ -2.4616,  -0.7693,   0.7865,  ...,  -0.9309,   0.3701,  -1.1425],\n",
      "          ...,\n",
      "          [ -7.5219,   1.4975,   0.5780,  ...,   1.8058,   3.2108,  -7.3100],\n",
      "          [ -4.8203,  -0.4463,  -7.3837,  ...,   4.9406,   5.8339,  -3.9415],\n",
      "          [  0.9589,  -0.0532,   2.0620,  ...,   1.2679,  -0.2493,  -0.7819]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ -1.6164,  -1.0195,  -1.6041,  ...,  -2.8041,  -0.1516,  -3.1198],\n",
      "          [ -1.3794,  -2.5710,  -0.1825,  ...,   0.9306,  -0.7878,  -1.5535],\n",
      "          [ -0.1377,  -1.0417,   1.5847,  ...,   2.6136,  -1.1606,   0.1274],\n",
      "          ...,\n",
      "          [  1.0944,  -1.4220,  -3.9167,  ...,   0.3369,  -0.1412,   2.8926],\n",
      "          [ -1.3301,  -0.1614,  -0.2958,  ...,   1.6606,   0.9231,   2.5451],\n",
      "          [  1.1554,   0.0515,   0.8233,  ...,   0.2923,  -0.4361,   0.1482]],\n",
      "\n",
      "         [[  0.7228,   1.8049,  -1.1817,  ...,   1.4701,  -0.3938,   0.2583],\n",
      "          [ -4.4880,   4.7270,   3.1042,  ...,   1.5942,   2.5450,   2.9911],\n",
      "          [ -0.3009,   0.8634,  -0.3249,  ...,   0.6372,  -0.1277,   0.1090],\n",
      "          ...,\n",
      "          [ -3.3453,   1.3525,   2.7793,  ...,   0.1443,   3.9333,  -0.1183],\n",
      "          [ -2.5254,   0.4594,  -0.3849,  ...,  -1.4652,   0.5900,  -0.6529],\n",
      "          [  2.3883,  -0.1794,   0.6363,  ...,  -0.1878,   0.7829,   0.5886]],\n",
      "\n",
      "         [[ -0.3110,   4.2779,   0.6189,  ...,  -2.2981,   0.9650,   2.2223],\n",
      "          [  0.9665,  -0.0879,   0.6853,  ...,  -1.0233,  -4.0211,  -0.0356],\n",
      "          [ -1.4368,  -2.5819,   1.8707,  ...,   0.6954,   0.2481,  -0.3285],\n",
      "          ...,\n",
      "          [ -0.3038,  -1.7907,   1.1414,  ...,  -3.8293,   0.7724,  -1.6661],\n",
      "          [  0.5677,   0.8118,   1.5814,  ...,  -1.2767,   0.5271,  -2.0143],\n",
      "          [  0.2359,  -2.8712,   0.4489,  ...,  -0.3266,  -0.5791,   1.4694]]]],\n",
      "       grad_fn=<TransposeBackward0>)), (tensor([[[[-0.8884,  4.9651, -2.5303,  ...,  4.9061, -2.4603,  3.4798],\n",
      "          [-0.5766,  6.3290, -2.2567,  ...,  3.5401, -1.1888,  4.0417],\n",
      "          [ 1.5402,  5.0714, -1.3438,  ...,  3.3535, -1.4175,  5.4112],\n",
      "          [ 0.1326,  4.8992, -0.3869,  ...,  4.0774, -1.2409,  2.4831]],\n",
      "\n",
      "         [[ 1.4144,  3.2558, -1.8906,  ...,  1.4908, -0.0553,  1.5947],\n",
      "          [ 1.9504,  3.0511, -2.1841,  ...,  0.7687,  0.1816,  1.1886],\n",
      "          [ 1.3681,  2.8614, -2.0022,  ...,  0.9980, -0.3515,  0.7470],\n",
      "          [-0.2877,  2.3901, -0.8966,  ...,  1.7784, -0.9070,  1.1199]],\n",
      "\n",
      "         [[-0.1367,  0.2957, -0.4974,  ...,  0.9146,  2.0662,  0.5773],\n",
      "          [ 0.2777, -0.2932,  0.6760,  ...,  1.2645,  1.4664,  3.0007],\n",
      "          [ 1.6018, -0.6260, -0.8026,  ...,  2.5392,  0.6393,  3.5463],\n",
      "          [ 4.0028, -1.3923, -4.0000,  ...,  2.3981, -0.6510,  1.4019]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.5702, -0.3170, -0.1266,  ..., -3.2060,  2.2947, -1.9029],\n",
      "          [-0.0852,  0.5336, -0.2005,  ..., -3.2339,  2.5211, -2.4858],\n",
      "          [-0.0920,  1.0619,  0.0117,  ..., -3.6997,  0.6506, -2.9303],\n",
      "          [-0.0697, -0.5492,  3.2180,  ..., -4.7113,  0.8440, -0.5663]],\n",
      "\n",
      "         [[-1.0794,  0.0134,  0.5490,  ..., -1.1544,  0.6369,  1.2916],\n",
      "          [-1.4371,  0.3342,  1.1592,  ...,  0.0618,  2.1562,  0.4166],\n",
      "          [-0.5869,  0.1248, -0.0413,  ...,  0.5738,  3.0313,  0.6933],\n",
      "          [ 0.0241, -1.8307, -0.5027,  ..., -2.1316,  0.6167,  1.9280]],\n",
      "\n",
      "         [[ 1.8798,  0.6647, -3.3464,  ...,  2.1776,  0.7782,  2.5780],\n",
      "          [ 1.2427,  0.8241, -3.8565,  ...,  2.7757,  0.7660,  2.0167],\n",
      "          [-0.1506, -0.6996, -3.7205,  ...,  0.8978, -0.3177,  1.1949],\n",
      "          [-0.4152, -4.2514, -2.1147,  ...,  1.7374, -0.2516,  1.6180]]]],\n",
      "       grad_fn=<TransposeBackward0>), tensor([[[[-2.8212e-01,  2.5784e+00,  2.9140e+00,  ..., -6.3795e-01,\n",
      "            8.1064e-01,  1.6807e-01],\n",
      "          [ 1.2928e+00,  2.9030e+00,  1.2846e+00,  ..., -4.0048e-01,\n",
      "           -3.3051e-01, -2.1643e-01],\n",
      "          [ 1.2845e+00,  1.8095e+00,  6.1341e-01,  ..., -2.0129e+00,\n",
      "            2.3847e+00, -1.4936e+00],\n",
      "          [ 1.7623e+00,  3.7038e-01,  6.4336e-01,  ..., -4.0533e+00,\n",
      "           -1.3972e+00,  1.4088e+00]],\n",
      "\n",
      "         [[ 1.1171e+00,  3.8745e+00, -4.4888e+00,  ...,  2.4163e+00,\n",
      "           -1.5827e+00, -5.1328e+00],\n",
      "          [ 1.8923e+00,  1.9148e+00, -3.3310e+00,  ...,  3.1432e+00,\n",
      "           -3.0501e+00, -1.9803e+00],\n",
      "          [ 4.2393e+00,  5.1978e-01, -1.4991e+00,  ...,  3.2399e+00,\n",
      "           -9.4889e-01, -8.5407e-01],\n",
      "          [ 1.5418e+00, -9.6552e-01, -3.3035e+00,  ..., -9.6442e-01,\n",
      "            4.5642e-01, -3.4126e+00]],\n",
      "\n",
      "         [[-2.0502e+00,  5.2499e+00, -3.3853e+00,  ...,  2.0490e-02,\n",
      "            7.0111e+00,  2.2098e-01],\n",
      "          [-8.4258e-01,  2.2101e+00, -3.2453e+00,  ...,  4.3562e+00,\n",
      "            5.6018e+00, -5.5249e-01],\n",
      "          [ 1.4003e-01,  1.8274e+00, -2.0655e+00,  ...,  5.6165e+00,\n",
      "            3.2099e+00,  5.6419e-02],\n",
      "          [ 1.7410e-03,  2.7788e+00,  2.4322e+00,  ..., -4.6634e-01,\n",
      "            1.0157e+00,  3.7079e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 7.2300e-02,  1.9165e+00,  3.3189e+00,  ...,  3.4282e+00,\n",
      "            2.0486e+00, -8.0033e-01],\n",
      "          [ 1.2153e+00,  2.5770e+00,  2.6554e+00,  ...,  3.6081e+00,\n",
      "            1.3568e+00,  4.2607e-01],\n",
      "          [ 1.0398e+00,  2.2332e+00,  4.4960e-03,  ...,  3.7196e+00,\n",
      "            4.5853e-01,  1.2093e+00],\n",
      "          [ 9.2930e-01,  2.2235e+00, -8.2854e-01,  ..., -3.5499e-02,\n",
      "            3.2670e+00,  3.6662e-01]],\n",
      "\n",
      "         [[ 2.8770e-01, -1.0335e+00,  4.0339e+00,  ..., -5.9426e+00,\n",
      "            4.2827e+00, -2.6227e-01],\n",
      "          [ 1.3168e+00, -3.9990e+00,  2.9793e+00,  ..., -5.0623e+00,\n",
      "            7.2445e-01, -3.7453e+00],\n",
      "          [ 1.7937e+00, -6.6343e-01,  1.1436e+00,  ..., -2.1840e+00,\n",
      "            2.4806e+00, -4.0104e+00],\n",
      "          [-2.7892e+00,  1.8830e-01,  7.8171e-01,  ..., -6.4825e-01,\n",
      "            2.8140e+00, -2.6399e+00]],\n",
      "\n",
      "         [[ 5.5526e+00, -1.2649e+00, -2.9190e+00,  ..., -2.7488e-02,\n",
      "            1.7499e+00,  6.5415e+00],\n",
      "          [ 3.5050e+00,  1.3307e+00, -8.4194e-01,  ..., -1.9072e+00,\n",
      "            1.2771e+00,  4.9048e+00],\n",
      "          [ 1.9049e+00,  3.9941e+00, -1.0512e+00,  ..., -1.2621e+00,\n",
      "            1.2040e+00,  2.0808e+00],\n",
      "          [ 2.6116e-01, -1.0801e+00, -2.0584e+00,  ..., -4.8499e-02,\n",
      "            9.6925e-01,  8.2986e-01]]]], grad_fn=<TransposeBackward0>), tensor([[[[ 0.5462, -1.9763,  0.4255,  ..., -1.6577, -0.7166,  1.1525],\n",
      "          [-1.4028, -4.3010,  0.8699,  ..., -0.0727, -1.8762, -1.9168],\n",
      "          [-0.0128, -0.1510, -2.5612,  ..., -1.3385, -0.5983,  0.9963],\n",
      "          ...,\n",
      "          [ 0.4890,  2.6240, -0.3105,  ..., -0.0551,  0.7924,  0.8577],\n",
      "          [-1.4217,  4.4175, -2.6666,  ..., -0.4262,  1.5550, -1.2794],\n",
      "          [ 0.8537, -0.2650, -1.6648,  ..., -1.5381, -0.6998,  0.0290]],\n",
      "\n",
      "         [[ 2.0748,  1.3561, -1.0687,  ..., -2.0329, -1.6837,  1.9794],\n",
      "          [ 1.4311, -2.1370, -2.5124,  ...,  0.7666, -5.7090,  3.4585],\n",
      "          [ 0.3729, -0.4760, -0.8116,  ...,  0.4693,  0.0729,  0.9704],\n",
      "          ...,\n",
      "          [ 0.6858,  3.0563,  1.6011,  ..., -3.8339, -0.5708,  1.6107],\n",
      "          [ 2.0561,  4.4683,  2.0669,  ..., -1.8788, -2.5393,  3.3179],\n",
      "          [ 0.9631, -0.2290, -0.5671,  ..., -1.8279, -0.0743,  0.9926]],\n",
      "\n",
      "         [[-2.9083,  0.1242,  0.1174,  ...,  1.2620,  1.0834,  1.2239],\n",
      "          [ 0.0722,  1.0657, -0.8418,  ...,  1.6393,  0.1071,  1.0846],\n",
      "          [-0.8525, -1.6662, -0.9740,  ...,  1.3102,  0.4264, -4.9326],\n",
      "          ...,\n",
      "          [ 1.5898,  2.4491,  0.2538,  ...,  0.5899,  2.8667,  1.5824],\n",
      "          [ 0.8862,  0.9238, -1.0703,  ...,  0.3375,  2.9983,  0.9842],\n",
      "          [-0.3760,  1.8746,  0.9889,  ...,  0.0115,  0.9555, -1.9268]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0131, -0.1328,  0.0774,  ..., -0.5097, -0.0107,  2.5490],\n",
      "          [-0.6992, -0.0559,  0.2627,  ..., -0.5877,  1.5954,  2.7849],\n",
      "          [ 0.2575, -0.4328, -0.8356,  ..., -0.1961,  0.4244, -0.0927],\n",
      "          ...,\n",
      "          [ 0.2737,  2.9588,  1.3226,  ...,  0.0080, -0.9127,  0.8781],\n",
      "          [-1.2583,  2.1097,  0.7874,  ...,  2.4464, -0.7755,  1.1366],\n",
      "          [-0.4287, -1.8581,  2.0512,  ...,  2.5591, -1.0754,  0.5933]],\n",
      "\n",
      "         [[-5.2916, -1.7814,  0.7496,  ...,  0.6317,  2.8854,  3.2619],\n",
      "          [-1.3567, -0.1392, -0.0697,  ...,  1.7099, -1.2542, -0.1797],\n",
      "          [-1.1456,  0.8819, -0.4005,  ...,  0.2428,  3.4013,  0.4334],\n",
      "          ...,\n",
      "          [-2.6630, -3.3043, -0.8541,  ..., -0.6644, -0.0387,  3.3192],\n",
      "          [-2.3560, -2.2166, -1.4728,  ...,  1.2757,  4.9478,  2.1402],\n",
      "          [-1.7869,  0.9778,  1.4076,  ...,  0.4093,  2.0052, -0.5593]],\n",
      "\n",
      "         [[-1.8490, -0.3874, -0.9446,  ...,  2.3476, -3.0121,  2.4920],\n",
      "          [ 2.9379,  1.7111,  1.7625,  ...,  2.2150,  0.5839,  0.7472],\n",
      "          [-0.1276,  0.1905,  0.6986,  ..., -0.4423,  0.3265,  0.7010],\n",
      "          ...,\n",
      "          [ 0.4757,  0.4505, -1.4018,  ...,  1.1962,  1.5427,  0.9080],\n",
      "          [-0.9808,  2.2343, -1.7635,  ..., -1.4650, -1.4826,  1.4226],\n",
      "          [-0.8261,  0.3593, -0.6254,  ...,  0.3827,  1.8363, -0.1227]]]],\n",
      "       grad_fn=<TransposeBackward0>), tensor([[[[ 2.4420e+00, -4.9030e+00,  5.2133e+00,  ..., -7.6661e+00,\n",
      "           -2.1222e-01,  3.1054e-01],\n",
      "          [-1.0513e+00,  1.6306e+00,  3.6093e+00,  ..., -2.6072e+00,\n",
      "            1.6967e+00, -2.3914e+00],\n",
      "          [-2.1709e-01, -4.8259e-03, -3.2648e-01,  ...,  2.4754e-01,\n",
      "            3.3183e-01,  5.0883e-01],\n",
      "          ...,\n",
      "          [ 3.7014e+00,  1.2838e+00,  5.0387e+00,  ..., -1.3093e+00,\n",
      "            6.0919e+00, -3.0741e+00],\n",
      "          [ 4.7823e+00, -1.1978e+00, -1.9980e+00,  ..., -1.2514e+00,\n",
      "            5.3175e+00,  3.0200e+00],\n",
      "          [ 7.5059e-01,  4.7466e+00, -3.8687e+00,  ..., -2.9935e-01,\n",
      "            6.2797e-01, -5.4152e+00]],\n",
      "\n",
      "         [[ 2.9055e+00, -4.4900e-01, -5.8076e+00,  ...,  8.3300e-01,\n",
      "            5.7939e+00,  6.5135e+00],\n",
      "          [ 4.4676e+00,  4.0655e-01, -2.3553e+00,  ...,  1.0648e-01,\n",
      "            1.3586e-01,  8.2962e-01],\n",
      "          [-5.3215e-01,  3.3759e-01,  5.8217e-02,  ..., -1.4039e-01,\n",
      "            5.8365e-01,  4.0729e-01],\n",
      "          ...,\n",
      "          [-4.9630e-01,  9.8916e-01, -7.4450e+00,  ..., -1.3226e-01,\n",
      "           -4.0771e+00, -2.5461e+00],\n",
      "          [ 3.8338e+00,  4.1970e+00, -1.3410e+00,  ...,  2.3714e+00,\n",
      "           -5.0729e+00, -9.9868e+00],\n",
      "          [ 5.5957e-01,  2.8877e+00,  2.1527e+00,  ..., -7.9046e-01,\n",
      "           -3.3160e-01, -1.9102e+00]],\n",
      "\n",
      "         [[-3.6512e+00,  2.1412e+00,  9.6765e-01,  ..., -3.6211e+00,\n",
      "            8.4107e-01,  5.0650e+00],\n",
      "          [-2.2670e+00,  1.6196e+00,  4.2912e+00,  ..., -6.2730e-01,\n",
      "           -3.1533e-01,  7.0655e+00],\n",
      "          [ 9.4803e-01, -1.6483e-01,  6.5963e-01,  ..., -6.6577e-01,\n",
      "            4.1703e-01, -1.9131e-01],\n",
      "          ...,\n",
      "          [ 1.4877e+00,  3.1062e+00, -4.4830e+00,  ...,  4.0090e+00,\n",
      "            1.2549e+00,  2.3936e+00],\n",
      "          [ 1.1179e+00, -1.2372e+00, -5.2707e+00,  ...,  8.1585e+00,\n",
      "            9.7325e-01, -2.6636e+00],\n",
      "          [-2.3173e+00, -5.5374e+00, -6.4919e-01,  ...,  3.0147e+00,\n",
      "           -6.2798e-01,  2.8335e+00]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-2.8960e-01,  4.4619e+00, -2.4747e+00,  ...,  5.6623e-01,\n",
      "           -6.8934e+00,  3.1781e+00],\n",
      "          [ 2.7928e+00,  5.8488e+00,  1.5879e+00,  ...,  2.2446e+00,\n",
      "           -9.1008e-01,  2.8062e+00],\n",
      "          [ 1.8241e-01,  3.6487e+00,  1.1273e+00,  ..., -4.8624e-01,\n",
      "            1.6753e-02, -8.7276e-01],\n",
      "          ...,\n",
      "          [ 9.6285e-01,  2.6193e+00, -4.8712e-01,  ...,  4.0119e+00,\n",
      "           -1.1255e+00, -1.7901e+00],\n",
      "          [ 5.0658e+00,  2.3892e+00, -2.2215e+00,  ...,  4.0034e+00,\n",
      "            9.3432e-01,  1.6539e+00],\n",
      "          [-2.5583e-01,  1.4574e-01,  2.7176e+00,  ..., -9.8312e-01,\n",
      "           -2.8334e+00,  7.7452e-01]],\n",
      "\n",
      "         [[ 3.4257e+00,  3.5864e+00, -1.8392e+00,  ..., -7.2564e+00,\n",
      "           -2.5499e+00,  5.2206e+00],\n",
      "          [-4.3803e+00,  4.5318e+00, -6.5010e+00,  ..., -9.2125e-01,\n",
      "           -3.6950e+00, -2.1390e-01],\n",
      "          [-7.9392e-02, -5.8839e-01, -8.1917e-01,  ..., -1.8881e+00,\n",
      "            2.3689e-01, -7.8798e-01],\n",
      "          ...,\n",
      "          [ 2.9819e+00,  3.8592e+00, -3.3362e+00,  ...,  4.4442e+00,\n",
      "           -4.6992e+00,  2.1667e+00],\n",
      "          [ 2.2885e+00, -7.8613e-01, -7.1831e+00,  ...,  2.2208e+00,\n",
      "            1.6966e-01,  2.7582e+00],\n",
      "          [ 1.2821e+00, -2.2534e+00,  2.2228e+00,  ..., -1.8412e+00,\n",
      "            5.1440e+00, -9.1846e-01]],\n",
      "\n",
      "         [[ 2.5425e+00, -7.3580e+00,  1.6965e+00,  ..., -3.5152e+00,\n",
      "           -3.2033e-01, -8.8333e+00],\n",
      "          [-3.2462e+00,  6.2747e-01, -1.7756e+00,  ...,  1.9484e+00,\n",
      "           -7.4091e-01, -1.0158e+01],\n",
      "          [-5.1845e-02, -5.8756e-01, -8.3996e-02,  ..., -7.9183e-01,\n",
      "            1.1271e+00, -4.0777e-01],\n",
      "          ...,\n",
      "          [-4.8275e+00, -8.0895e+00,  7.8108e+00,  ...,  3.6022e+00,\n",
      "           -4.1794e+00, -3.1953e+00],\n",
      "          [-6.6352e+00, -3.7195e+00,  1.7291e+00,  ...,  1.9323e+00,\n",
      "           -2.7363e+00,  1.3834e+00],\n",
      "          [-2.7697e+00, -1.8964e+00, -8.7689e-01,  ..., -4.2422e+00,\n",
      "           -3.8368e-01, -1.6220e+00]]]], grad_fn=<TransposeBackward0>)), (tensor([[[[ 0.7429,  3.0414, -2.4119,  ...,  0.1532, -1.6467, -1.0699],\n",
      "          [ 0.0909,  2.9292, -2.8196,  ...,  0.6638, -1.6208, -1.0197],\n",
      "          [ 0.1495,  2.5132, -4.5709,  ..., -1.3515, -1.3311,  0.1594],\n",
      "          [ 1.3678,  2.6025, -3.3165,  ..., -1.2040, -1.6915,  1.9887]],\n",
      "\n",
      "         [[ 0.4016, -1.3904,  1.3123,  ..., -2.5990,  5.2178, -0.2157],\n",
      "          [ 2.0570, -2.1490,  0.4662,  ..., -2.4901,  3.2975,  0.1034],\n",
      "          [ 1.7125, -2.1250,  0.6955,  ..., -2.0222,  4.6295,  1.2841],\n",
      "          [ 0.9617, -1.5509,  0.9989,  ..., -0.7776,  3.4111,  0.8339]],\n",
      "\n",
      "         [[ 1.2103,  0.5536,  0.1161,  ..., -2.5434,  0.6419,  0.6253],\n",
      "          [ 1.1880,  1.0605, -0.5515,  ..., -1.6217, -0.0822,  0.7018],\n",
      "          [ 1.6508,  1.5667, -1.1663,  ..., -1.5012,  1.4042, -0.0925],\n",
      "          [ 1.3892,  0.6563, -0.3461,  ..., -0.8515,  2.1410,  1.0617]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.9593,  1.6909, -2.4657,  ..., -1.6151,  1.4620, -1.4243],\n",
      "          [ 2.3713,  2.4199, -1.9706,  ..., -0.6457,  0.6342, -1.5910],\n",
      "          [ 2.4013,  2.8757, -2.4076,  ...,  0.5566, -0.2520, -2.3073],\n",
      "          [ 2.7788,  1.6148, -1.6671,  ..., -0.8906, -0.0788, -3.5847]],\n",
      "\n",
      "         [[ 0.7037, -1.0349,  2.5634,  ...,  1.8895,  0.4653, -0.1877],\n",
      "          [ 0.0658, -0.7962,  1.9702,  ...,  1.7883,  0.4442,  0.3585],\n",
      "          [-0.2079,  0.4504,  1.8626,  ...,  0.7438,  0.1389,  1.2514],\n",
      "          [ 3.0930, -0.8200,  1.8250,  ...,  1.4946, -0.8626,  0.5811]],\n",
      "\n",
      "         [[-1.2876,  1.6074,  0.4030,  ..., -2.3302,  0.2762,  0.5921],\n",
      "          [-2.1137,  1.7340,  0.5537,  ..., -1.8465,  0.8504,  0.7360],\n",
      "          [-1.2731,  1.8521,  1.0105,  ..., -0.2798,  1.6098,  2.2622],\n",
      "          [-0.0165,  2.7727, -0.4976,  ..., -0.3040,  1.4618,  4.4393]]]],\n",
      "       grad_fn=<TransposeBackward0>), tensor([[[[-1.5065,  2.1640,  1.7806,  ...,  0.8260, -4.2053,  6.0910],\n",
      "          [-1.4710,  2.4145, -0.5374,  ...,  1.4217, -6.0249,  7.5580],\n",
      "          [-2.2527,  1.5926,  0.0283,  ..., -1.5401, -3.6152,  4.6380],\n",
      "          [-0.1856, -3.1377, -1.9276,  ..., -0.4659, -3.8046,  2.0738]],\n",
      "\n",
      "         [[ 1.2190, -0.6535,  0.2249,  ...,  3.2910,  2.2470,  4.2355],\n",
      "          [ 2.7236, -0.4277, -1.0501,  ...,  4.3432,  2.2577,  6.3188],\n",
      "          [ 1.4916,  0.2454, -2.0240,  ...,  3.0084,  1.9383,  8.8146],\n",
      "          [ 0.6237, -4.3997,  1.8038,  ...,  4.0847, -0.0595,  2.8199]],\n",
      "\n",
      "         [[ 2.0388, -8.4899, -3.9195,  ...,  0.3510, -0.7469,  0.9450],\n",
      "          [ 2.1978, -5.6883, -3.5427,  ...,  2.8416, -1.7737,  3.9728],\n",
      "          [ 0.0893, -5.3971, -3.3513,  ...,  4.0087, -1.0975,  3.6938],\n",
      "          [-0.7646, -5.3654,  1.1415,  ..., -1.8831,  0.0333, -1.1797]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.8357,  2.8817,  0.2631,  ...,  2.5059, -0.0799,  0.2834],\n",
      "          [-1.0195,  2.5113, -1.0007,  ...,  2.5882, -1.7396,  0.5882],\n",
      "          [-1.4809,  1.1146,  0.3845,  ...,  1.8699, -0.2567,  0.4090],\n",
      "          [-3.5907,  4.4776,  0.4787,  ...,  2.0855,  1.4613,  1.2507]],\n",
      "\n",
      "         [[-3.6242,  1.7475, -1.2965,  ..., -4.7132,  1.7918, -0.1357],\n",
      "          [-2.7146, -0.9120, -1.5702,  ..., -4.9820,  0.6654,  0.4895],\n",
      "          [-3.4310, -3.9716, -0.6625,  ..., -3.0268,  0.0261,  2.9254],\n",
      "          [-3.6785, -3.0488,  1.6343,  ..., -0.8748,  4.6504,  3.0812]],\n",
      "\n",
      "         [[-1.3934, -1.1053, -0.8284,  ..., -1.3921, -7.4009,  2.7342],\n",
      "          [ 0.1137,  0.0246, -1.1764,  ...,  0.1015, -4.9772, -0.0853],\n",
      "          [ 2.5977, -0.2787, -0.3432,  ..., -1.0503, -4.4261,  0.8505],\n",
      "          [ 1.1201, -0.3927, -1.1270,  ..., -1.3790, -2.2058,  4.5922]]]],\n",
      "       grad_fn=<TransposeBackward0>), tensor([[[[-0.1978, -2.0160, -2.5517,  ..., -6.4089,  0.8669,  1.7787],\n",
      "          [-0.8427, -1.1314, -6.7071,  ..., -4.2063, -0.7938,  1.0884],\n",
      "          [ 5.6735, -0.6802,  7.5651,  ..., -2.0939, -1.6998,  1.1249],\n",
      "          ...,\n",
      "          [-1.6030,  1.6286, -0.4238,  ..., -2.1525, -1.9987,  2.0734],\n",
      "          [ 1.4963,  0.5908,  1.2477,  ..., -4.6781, -3.9862, -3.2327],\n",
      "          [ 4.6794, -0.9067,  4.3026,  ..., -1.2234, -0.4606,  4.9470]],\n",
      "\n",
      "         [[-0.5951, -2.5598, -2.1892,  ...,  0.3186, -2.6370,  2.3262],\n",
      "          [ 1.7685, -4.3984,  4.9615,  ...,  0.9649, -3.7130, -3.6987],\n",
      "          [-0.7744,  3.8883, -3.8542,  ...,  1.6269, -0.7678, -0.5953],\n",
      "          ...,\n",
      "          [-2.8764, -5.1753,  4.1356,  ...,  0.5260, -2.6902, -1.4465],\n",
      "          [-3.1517, -3.5170,  1.5418,  ..., -0.0436, -2.8092,  0.9854],\n",
      "          [-0.7016, -1.3175,  0.1560,  ...,  1.9269, -2.0491,  1.4638]],\n",
      "\n",
      "         [[-1.1186,  2.1424, -2.0981,  ...,  0.5070, -0.5184,  2.2116],\n",
      "          [ 2.2378, -0.3342, -0.2788,  ...,  2.6386,  2.4390, -0.9555],\n",
      "          [ 1.1611,  1.2083, -1.4542,  ..., -0.1781, -4.3610, -1.4097],\n",
      "          ...,\n",
      "          [ 0.2215, -0.4283,  3.2749,  ..., -2.4862,  0.9791,  1.5806],\n",
      "          [ 2.0568,  1.2756,  4.8946,  ..., -2.8202, -0.4262,  2.3738],\n",
      "          [ 3.2045,  1.1775,  0.4329,  ...,  0.2164, -3.3871,  1.5394]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-2.1305,  1.1741, -4.1120,  ..., -1.7004, -0.7322, -1.5946],\n",
      "          [-2.4007, -1.4045, -1.0059,  ...,  1.7356, -4.1407,  2.6154],\n",
      "          [ 0.3682,  1.2062, -3.0651,  ..., -2.0118,  0.3958, -3.0021],\n",
      "          ...,\n",
      "          [-5.3553, -0.0841, -0.8712,  ...,  3.5023, -5.9868,  3.1267],\n",
      "          [-4.8970, -2.0411, -0.5689,  ...,  5.1091, -3.8774,  2.1465],\n",
      "          [-2.5820,  1.1749, -1.5082,  ..., -2.9467,  0.3309, -1.7285]],\n",
      "\n",
      "         [[-0.0698,  1.0911,  1.9105,  ...,  2.2298,  2.8335,  3.1897],\n",
      "          [ 0.1877,  0.8464,  0.5761,  ...,  0.5805,  3.7662,  1.2535],\n",
      "          [ 1.0670,  0.5955, -0.1031,  ...,  1.4593,  0.6664,  1.4259],\n",
      "          ...,\n",
      "          [ 0.4620, -0.3593, -1.4714,  ...,  1.9965,  0.4349, -0.5525],\n",
      "          [ 1.1023, -0.4577, -2.6884,  ...,  3.3095, -3.1174, -1.1579],\n",
      "          [ 0.8036,  0.3362,  0.1619,  ...,  0.8315,  1.5778,  1.5960]],\n",
      "\n",
      "         [[-0.6485,  0.7875, -0.5932,  ...,  0.9295,  0.9223,  1.3184],\n",
      "          [ 2.9342, -1.5894, -0.7561,  ..., -1.5689,  0.4482,  2.8244],\n",
      "          [ 0.9934, -0.4921, -0.2053,  ..., -0.4333, -0.5382, -0.2027],\n",
      "          ...,\n",
      "          [-0.6232, -0.1738,  0.6354,  ..., -3.1392, -2.3286, -3.1441],\n",
      "          [-0.0811,  0.6280, -3.7784,  ..., -0.0835, -2.4269, -0.3428],\n",
      "          [-0.6210, -0.5291,  0.1961,  ..., -1.1475,  0.0906, -0.9664]]]],\n",
      "       grad_fn=<TransposeBackward0>), tensor([[[[-3.4783e+00, -1.0724e+00,  5.6519e+00,  ..., -6.3273e+00,\n",
      "           -6.0959e+00,  7.9338e-01],\n",
      "          [ 2.7009e+00, -1.7775e+00,  2.3895e+00,  ..., -4.5210e+00,\n",
      "            4.6654e+00,  6.9150e+00],\n",
      "          [-2.8048e-01, -3.0333e+00, -1.9940e-01,  ...,  6.4628e-01,\n",
      "            7.7229e-01,  1.9457e-01],\n",
      "          ...,\n",
      "          [-3.3291e-01,  4.7815e-01, -2.8142e+00,  ..., -6.1143e-01,\n",
      "           -2.0642e+00, -3.9407e+00],\n",
      "          [-3.1385e+00, -1.3338e+00, -2.6883e-01,  ...,  4.8876e-01,\n",
      "           -1.8235e+00,  1.1122e+00],\n",
      "          [ 7.2572e-01,  5.6075e+00, -1.7880e+00,  ...,  1.0774e+00,\n",
      "            9.1291e-01, -2.2878e-01]],\n",
      "\n",
      "         [[ 1.2770e+01,  1.6969e+00,  2.9190e+00,  ...,  4.0333e-01,\n",
      "            2.6106e+00,  4.1634e-01],\n",
      "          [ 2.1025e+00,  5.1484e-01, -5.6310e+00,  ...,  3.3793e+00,\n",
      "            2.3513e+00,  2.4812e+00],\n",
      "          [ 1.0473e+00, -8.9336e-01, -4.8980e-01,  ...,  8.9309e-01,\n",
      "            1.4728e+00,  1.1002e+00],\n",
      "          ...,\n",
      "          [ 2.3052e+00,  1.4798e-01, -2.2600e+00,  ...,  2.7257e+00,\n",
      "           -4.1255e+00, -1.8786e+00],\n",
      "          [ 3.1605e+00,  1.0681e+01, -3.3324e+00,  ..., -5.7836e+00,\n",
      "           -2.2534e+00, -7.9600e+00],\n",
      "          [ 1.6708e+00,  6.3428e-01, -2.7983e+00,  ...,  9.2152e-01,\n",
      "           -1.3170e+00,  7.0408e-01]],\n",
      "\n",
      "         [[ 2.1213e+00,  9.8668e-01,  3.9688e-01,  ..., -2.9326e+00,\n",
      "           -8.8599e+00, -6.9582e+00],\n",
      "          [ 1.5124e+00, -3.6436e+00, -1.8007e-01,  ..., -1.9102e+00,\n",
      "           -7.7106e+00,  1.4671e+00],\n",
      "          [ 4.3668e-01, -1.7684e+00, -1.8339e-02,  ..., -1.2284e-02,\n",
      "           -5.4990e-01, -9.1058e-01],\n",
      "          ...,\n",
      "          [ 5.2108e+00, -2.7625e+00,  1.8798e+00,  ...,  1.6864e+00,\n",
      "           -3.9869e+00,  1.1613e+00],\n",
      "          [ 3.0233e+00, -8.6669e+00,  3.1888e+00,  ..., -4.2014e+00,\n",
      "           -4.8024e+00,  2.1917e+00],\n",
      "          [-2.1497e+00,  7.3934e-01,  9.9075e-01,  ..., -3.7577e+00,\n",
      "           -3.8520e+00,  4.0637e+00]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-6.4446e+00,  3.0062e+00,  6.5919e+00,  ...,  3.9654e+00,\n",
      "            8.9222e+00,  6.4978e-01],\n",
      "          [-8.5636e-01, -3.7452e+00,  3.8945e+00,  ...,  2.7968e+00,\n",
      "            2.8631e+00,  1.8107e-02],\n",
      "          [ 5.4037e-01,  1.7090e-01,  1.1895e+00,  ...,  1.7401e+00,\n",
      "            2.8651e-01, -6.2062e-01],\n",
      "          ...,\n",
      "          [ 1.8248e+00, -7.0100e+00, -7.1624e-01,  ...,  2.7940e+00,\n",
      "            3.9485e+00, -4.7148e+00],\n",
      "          [-3.8822e+00,  4.7561e-01,  2.5519e+00,  ...,  1.0751e-01,\n",
      "            9.6249e-02, -4.3962e+00],\n",
      "          [ 8.5756e-01,  3.9888e+00,  5.1941e+00,  ..., -1.4043e+00,\n",
      "            2.7294e+00,  1.8085e+00]],\n",
      "\n",
      "         [[-1.0764e+01,  6.0395e+00, -1.0160e+01,  ..., -1.3960e+01,\n",
      "            9.3263e+00,  2.2205e+00],\n",
      "          [-4.4433e+00, -1.9299e+00, -5.3417e+00,  ..., -2.5360e+00,\n",
      "           -8.1392e-01,  2.7904e-01],\n",
      "          [ 3.4764e-02,  1.4251e-01,  6.9806e-02,  ...,  6.7691e-02,\n",
      "            4.7227e-01,  1.1642e-01],\n",
      "          ...,\n",
      "          [-7.2031e-01,  6.3862e+00, -2.3909e+00,  ...,  3.6506e+00,\n",
      "           -6.7789e+00, -2.8544e+00],\n",
      "          [-3.8935e+00,  7.2183e+00,  3.4454e+00,  ...,  6.6383e+00,\n",
      "            1.6102e+00,  2.5430e+00],\n",
      "          [-2.2856e-01,  5.7064e-01, -1.2528e+00,  ...,  4.2099e-01,\n",
      "            1.2001e+00, -5.5583e-01]],\n",
      "\n",
      "         [[-2.6620e+00, -1.7855e+00, -6.6906e-01,  ..., -2.8764e+00,\n",
      "            1.1704e+00, -7.7289e-01],\n",
      "          [ 2.4452e+00,  4.7877e+00,  1.1213e+00,  ...,  2.7913e+00,\n",
      "            3.0529e+00,  2.6948e+00],\n",
      "          [ 1.1216e+00, -7.8478e-01, -1.6682e+00,  ..., -4.4215e-01,\n",
      "            1.7322e+00,  1.0056e+00],\n",
      "          ...,\n",
      "          [-3.0866e+00, -4.9732e+00,  8.1529e+00,  ...,  7.4221e-01,\n",
      "            1.9069e+00, -5.5409e+00],\n",
      "          [ 2.2508e+00,  4.5084e+00, -2.9519e-01,  ...,  2.9968e+00,\n",
      "           -2.0603e-01, -3.1867e+00],\n",
      "          [-2.2778e+00, -1.5936e+00,  2.0212e-01,  ..., -1.3750e-01,\n",
      "           -1.4834e+00, -9.3010e-01]]]], grad_fn=<TransposeBackward0>)), (tensor([[[[-0.7006, -0.1449,  2.1173,  ...,  0.0530,  0.5172, -2.0087],\n",
      "          [-0.9901,  0.2164,  2.5997,  ..., -0.2419, -0.3085, -0.5320],\n",
      "          [-1.6688,  1.0260,  4.7085,  ..., -0.8272, -0.4477, -0.9416],\n",
      "          [-0.0476, -1.5020,  2.2025,  ..., -1.8196, -0.3548, -1.3899]],\n",
      "\n",
      "         [[ 2.7554,  2.6265,  1.5887,  ...,  2.4092, -0.4980, -3.8234],\n",
      "          [ 2.7995,  3.8942,  0.4381,  ...,  2.1083,  0.1589, -1.9861],\n",
      "          [ 4.2983,  4.3620,  0.4895,  ...,  1.0421,  1.0110, -1.0346],\n",
      "          [ 3.4343,  1.1181,  0.2655,  ..., -0.1285,  0.5928, -3.7902]],\n",
      "\n",
      "         [[ 0.6123, -1.0009,  0.3512,  ...,  2.1089,  1.7288, -0.4364],\n",
      "          [ 0.8980,  0.8622,  2.9906,  ...,  2.5778,  2.4499, -1.1884],\n",
      "          [-0.0417,  1.2300,  2.4064,  ...,  3.8026,  3.0977, -1.1455],\n",
      "          [-0.2733, -0.3121,  2.9020,  ...,  3.6255,  0.8539,  0.2289]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 4.7516, -2.3164,  2.5994,  ..., -1.0545, -0.2659,  2.5327],\n",
      "          [ 4.7319, -3.2716,  1.7733,  ..., -0.0765, -1.2291,  3.0335],\n",
      "          [ 3.6281, -2.7966,  1.8014,  ..., -1.2566, -2.7406,  3.9003],\n",
      "          [ 5.4789, -2.8988,  1.4947,  ..., -3.1307, -1.8220,  1.3715]],\n",
      "\n",
      "         [[-0.0766,  1.2846,  0.2554,  ..., -0.5047,  4.0252,  3.2049],\n",
      "          [-0.3130, -0.1525,  0.7248,  ..., -0.6825,  3.2334,  1.8537],\n",
      "          [-0.6529,  0.2559,  3.0557,  ..., -1.7195,  3.2942,  2.7749],\n",
      "          [ 2.6638,  2.5918,  2.8161,  ..., -0.0374,  3.8384,  2.0329]],\n",
      "\n",
      "         [[-0.5306,  3.2468, -2.0952,  ...,  1.5800, -2.9239,  3.4730],\n",
      "          [-0.2980,  2.0789, -1.9901,  ...,  1.6786, -3.0193,  2.5927],\n",
      "          [-1.5816,  2.5538, -0.1020,  ...,  1.1129, -4.7791,  2.5506],\n",
      "          [-2.0032,  2.3739, -2.4227,  ...,  0.2160, -5.0479,  3.0003]]]],\n",
      "       grad_fn=<TransposeBackward0>), tensor([[[[-2.4375,  0.6633, -2.9905,  ..., -6.6670,  3.0452,  8.3734],\n",
      "          [-2.0774,  0.9482, -2.0095,  ..., -6.0040,  5.4364,  7.9243],\n",
      "          [-4.0586,  1.7865, -3.4585,  ..., -3.7879,  4.1914,  5.7504],\n",
      "          [-4.3591, -1.1623, -3.9404,  ..., -1.1434,  2.8908, -0.5882]],\n",
      "\n",
      "         [[-4.2217, -0.2527, -6.5602,  ...,  0.2783,  3.3806, -4.1310],\n",
      "          [-2.1025, -1.4865, -7.2982,  ...,  2.0322,  2.0932, -2.0631],\n",
      "          [-2.5544, -1.9140, -6.2837,  ...,  4.1728,  0.2179, -0.4258],\n",
      "          [-3.1134,  4.7220, -7.4128,  ...,  4.9418,  2.4276,  1.6082]],\n",
      "\n",
      "         [[ 3.0075,  8.0081, -0.9571,  ...,  0.6359,  3.3010,  0.3403],\n",
      "          [ 1.4283,  7.6892, -1.8382,  ...,  0.3466,  4.5855,  1.0427],\n",
      "          [ 3.4469,  3.3245, -0.1126,  ...,  1.6042,  0.9887,  1.9286],\n",
      "          [ 3.8937,  3.4758, -1.6535,  ...,  3.7339,  2.3149, -2.9101]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.5287,  6.5791,  2.5277,  ..., 13.6174, -3.4135, -0.0254],\n",
      "          [ 1.6139,  5.2270,  0.6404,  ...,  8.1055, -4.3004, -1.8275],\n",
      "          [ 3.3297,  5.8407,  2.8742,  ...,  7.9351, -0.1824, -0.6428],\n",
      "          [ 5.5771,  3.7550,  0.6038,  ...,  2.9421,  1.8043,  0.6051]],\n",
      "\n",
      "         [[-5.7423, -5.0594, -4.6748,  ..., -1.5113, -1.5625, -2.5478],\n",
      "          [-3.9374, -2.0893, -2.2853,  ..., -1.3534, -1.8009,  0.3461],\n",
      "          [-3.9192, -3.7942, -0.4343,  ..., -2.8760, -0.2181, -1.8744],\n",
      "          [-1.7260, -3.4821, -1.8870,  ...,  3.4976, -0.8489,  1.9995]],\n",
      "\n",
      "         [[ 0.6794,  1.5159,  3.1483,  ...,  3.9099,  0.9031,  0.4101],\n",
      "          [-3.8336, -0.3206,  0.3156,  ...,  5.4331,  2.5339, -0.6704],\n",
      "          [ 0.8211,  4.4391,  2.3365,  ...,  0.4486,  0.6676,  1.7512],\n",
      "          [-0.3620,  4.9029,  3.4973,  ...,  0.9028,  2.4397,  1.7602]]]],\n",
      "       grad_fn=<TransposeBackward0>), tensor([[[[-1.7803e+00,  1.9041e+00, -4.7984e-01,  ...,  8.4511e-01,\n",
      "            3.3043e-01, -1.3892e+00],\n",
      "          [ 7.5985e-01, -6.8870e-01, -7.8521e-01,  ...,  3.9493e-01,\n",
      "           -4.7689e-02, -5.5306e-01],\n",
      "          [ 1.8610e+00, -7.0311e-01,  1.7431e-01,  ..., -1.0289e+00,\n",
      "           -2.2762e+00, -8.8859e-01],\n",
      "          ...,\n",
      "          [-1.2267e+00, -2.6456e+00,  4.1432e-01,  ..., -6.5068e-01,\n",
      "           -6.9488e-01,  2.2885e+00],\n",
      "          [ 2.1532e-01,  5.8605e-02, -1.1552e+00,  ...,  1.1140e+00,\n",
      "            2.3669e-01,  6.2165e-01],\n",
      "          [-2.0182e-01, -2.2687e+00, -1.0871e-01,  ..., -2.8650e-01,\n",
      "           -3.1532e+00,  1.8493e-01]],\n",
      "\n",
      "         [[ 3.1779e-01, -1.1188e+00, -3.6891e-01,  ...,  1.0907e+00,\n",
      "            2.6168e+00, -1.1897e-01],\n",
      "          [-1.5930e+00, -4.5168e+00, -2.6815e-02,  ..., -2.8949e+00,\n",
      "            1.1773e+00,  1.9813e+00],\n",
      "          [ 4.8927e-02,  1.0341e+00, -5.6716e-01,  ..., -1.8071e+00,\n",
      "            6.3032e-01, -1.3725e+00],\n",
      "          ...,\n",
      "          [ 2.3878e-01, -2.8273e+00, -3.5772e+00,  ...,  2.7185e-02,\n",
      "            1.1195e+00,  2.0946e-01],\n",
      "          [-2.0736e-01, -2.2211e+00, -2.1770e+00,  ..., -4.8192e-01,\n",
      "            1.0708e+00, -1.2985e-02],\n",
      "          [ 1.4951e+00, -1.2320e+00, -1.5880e+00,  ..., -1.1547e+00,\n",
      "            7.0303e-02,  1.0445e+00]],\n",
      "\n",
      "         [[ 4.7717e-01, -7.2814e-01,  5.6694e-01,  ..., -5.4992e-01,\n",
      "           -1.3906e+00, -1.1874e+00],\n",
      "          [-9.2782e-01,  7.7534e-01,  7.7770e-01,  ...,  4.9776e-01,\n",
      "            3.6928e+00, -3.0887e+00],\n",
      "          [-1.5636e+00, -2.2170e+00,  1.0422e+00,  ..., -1.2645e+00,\n",
      "           -4.5636e-01,  1.1974e+00],\n",
      "          ...,\n",
      "          [-5.9653e-01, -2.9867e+00, -3.6404e-01,  ...,  7.6237e-01,\n",
      "           -8.6446e-01, -3.7952e+00],\n",
      "          [-2.3967e-01, -3.5931e+00,  3.6507e-01,  ...,  2.4147e+00,\n",
      "           -3.3591e+00, -1.3146e+00],\n",
      "          [ 5.6938e-01, -2.9469e+00,  2.0362e+00,  ..., -1.6736e-01,\n",
      "            4.2934e-01, -5.7013e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-3.0395e+00, -1.2838e+00,  1.9418e+00,  ..., -6.5368e-04,\n",
      "           -8.9597e-01, -8.6165e-02],\n",
      "          [-9.4384e-02,  1.3193e+00, -7.4455e-01,  ...,  2.5525e+00,\n",
      "            8.1033e-01, -6.1451e-01],\n",
      "          [-3.9214e-01,  5.9225e-01,  3.0479e-01,  ..., -9.2014e+00,\n",
      "            7.1871e-01, -8.4815e-01],\n",
      "          ...,\n",
      "          [ 1.2002e+00,  3.5389e+00, -8.4501e-01,  ...,  3.5726e+00,\n",
      "           -3.9992e-01,  1.6556e+00],\n",
      "          [ 2.1083e+00,  5.1220e-01, -1.4913e+00,  ..., -5.7091e-01,\n",
      "            1.4403e-01,  3.3324e-01],\n",
      "          [ 1.2402e-01,  1.1695e+00, -4.1679e-01,  ..., -6.2488e+00,\n",
      "           -1.1107e+00, -4.1399e-01]],\n",
      "\n",
      "         [[-3.4745e-01,  1.8271e-01, -3.7641e-01,  ...,  1.9181e+00,\n",
      "           -8.2597e-01, -2.1579e+00],\n",
      "          [-9.9940e-01, -1.9027e+00, -3.4235e+00,  ..., -4.6471e+00,\n",
      "            1.7057e+00, -4.9781e+00],\n",
      "          [ 4.8873e-01, -4.7153e-01,  1.4780e+00,  ...,  4.3623e-01,\n",
      "            5.5830e-02,  5.6721e-01],\n",
      "          ...,\n",
      "          [ 3.8785e+00,  7.0416e-01, -1.4539e+00,  ...,  5.0917e-01,\n",
      "            2.5171e+00, -2.3004e+00],\n",
      "          [ 3.3373e+00, -2.8034e-01,  4.4873e-01,  ...,  3.1083e+00,\n",
      "            5.3608e+00, -3.4436e+00],\n",
      "          [ 1.5881e+00, -1.0107e+00,  6.5157e-01,  ...,  1.1889e+00,\n",
      "            9.8998e-01, -8.5161e-01]],\n",
      "\n",
      "         [[-8.3064e-01,  1.1059e+00,  6.1781e-01,  ..., -2.8683e+00,\n",
      "           -4.5422e-01, -5.0761e-01],\n",
      "          [ 1.1871e+00,  3.0253e+00, -2.4026e+00,  ...,  1.7017e+00,\n",
      "           -4.0112e+00, -1.5575e+00],\n",
      "          [-1.0739e+00, -3.1889e-01, -8.2492e-01,  ..., -6.8735e-01,\n",
      "            2.9182e+00,  2.6141e-02],\n",
      "          ...,\n",
      "          [ 6.3588e-01, -3.0935e-01, -2.2522e+00,  ...,  1.1532e+00,\n",
      "           -2.1546e+00,  2.9027e-01],\n",
      "          [-7.2638e-01, -4.8111e-01, -1.7214e+00,  ...,  8.0445e-01,\n",
      "            1.4791e+00,  1.7633e+00],\n",
      "          [ 3.1044e-01,  4.5501e-01,  1.7646e-01,  ...,  5.6470e-01,\n",
      "            1.7474e+00,  2.6703e-01]]]], grad_fn=<TransposeBackward0>), tensor([[[[ 7.3637e+00, -6.1831e+00,  3.1756e+00,  ..., -6.0358e+00,\n",
      "           -9.5986e+00, -3.5749e+00],\n",
      "          [-1.6428e-01, -1.2086e+00,  1.0813e+01,  ...,  3.8550e+00,\n",
      "           -9.0771e+00,  4.5531e+00],\n",
      "          [ 9.9058e-02, -5.3172e-01,  5.6129e-01,  ..., -5.2391e-01,\n",
      "           -9.6395e-02, -8.8958e-01],\n",
      "          ...,\n",
      "          [-3.8664e+00,  1.7513e+00,  8.0219e+00,  ...,  1.8369e+01,\n",
      "            4.7523e+00,  3.1098e+00],\n",
      "          [-2.7727e+00,  1.8436e+00,  1.8078e+00,  ...,  1.7415e+00,\n",
      "            3.7575e+00,  7.7518e+00],\n",
      "          [ 1.9745e+00,  5.2885e+00,  1.2310e+00,  ..., -1.8521e+00,\n",
      "           -8.6931e-01, -7.3930e-01]],\n",
      "\n",
      "         [[ 7.1844e+00, -2.8228e+00,  4.7331e+00,  ...,  6.9887e+00,\n",
      "           -8.0145e+00,  1.1217e-02],\n",
      "          [ 3.5169e+00, -5.5197e+00, -1.1503e+00,  ...,  4.9424e+00,\n",
      "            4.3227e+00,  1.4941e+00],\n",
      "          [-3.8314e-01, -7.2017e-01, -1.6707e-01,  ..., -5.2989e-02,\n",
      "            3.0445e-01,  5.5107e-01],\n",
      "          ...,\n",
      "          [-4.6351e-01,  5.9028e+00, -5.7060e-01,  ..., -1.1466e+01,\n",
      "            1.4449e+00,  5.4886e-01],\n",
      "          [ 2.1277e-01,  3.0569e+00, -7.5842e-01,  ..., -5.2092e+00,\n",
      "            2.5602e+00,  1.0209e+01],\n",
      "          [-3.0850e+00, -1.8442e+00,  2.5059e+00,  ...,  3.9837e-01,\n",
      "           -7.8201e-01,  1.6212e+00]],\n",
      "\n",
      "         [[ 1.0379e+01,  4.1642e+00, -5.9861e+00,  ..., -5.3208e+00,\n",
      "            2.0893e+00,  2.6121e+00],\n",
      "          [ 2.3780e+00, -4.7454e+00,  1.0108e+01,  ...,  1.7330e-01,\n",
      "           -2.5840e+00, -7.8241e+00],\n",
      "          [-3.1989e-01, -2.5116e-01, -5.8787e-02,  ...,  7.2279e-01,\n",
      "           -1.4506e-01, -4.1822e-01],\n",
      "          ...,\n",
      "          [ 7.2239e+00,  5.7120e+00,  5.8292e-01,  ...,  1.3847e+00,\n",
      "           -4.6441e+00, -4.0062e-01],\n",
      "          [ 5.4139e+00,  1.3233e+00, -4.9706e+00,  ...,  4.0287e+00,\n",
      "            4.3361e+00,  5.9860e+00],\n",
      "          [ 1.5289e+00,  2.0329e+00, -1.2370e+00,  ...,  4.0628e+00,\n",
      "            2.5270e+00, -3.0667e+00]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.1447e+01, -5.7623e+00,  3.2520e+00,  ...,  5.8541e+00,\n",
      "           -2.0028e+00,  8.9613e+00],\n",
      "          [ 1.5383e+00,  6.1939e+00,  2.1665e+00,  ..., -2.7564e+00,\n",
      "            1.6814e+00,  4.3421e+00],\n",
      "          [ 6.2854e-01,  5.9184e-01, -6.5429e-02,  ...,  2.4063e-02,\n",
      "            9.0575e-01, -2.4940e-01],\n",
      "          ...,\n",
      "          [ 1.4533e+00,  6.2617e+00,  4.4940e+00,  ...,  7.0893e-01,\n",
      "           -5.0623e+00, -3.6815e+00],\n",
      "          [-4.2777e+00, -1.1089e+01,  7.5072e-01,  ...,  1.2425e+01,\n",
      "           -4.4365e+00,  2.6579e+00],\n",
      "          [-1.5353e+00, -6.3272e+00, -1.1036e+00,  ...,  1.9023e+00,\n",
      "           -2.1212e+00, -2.4548e+00]],\n",
      "\n",
      "         [[ 9.6290e+00, -4.1068e-01,  3.1266e+00,  ..., -9.6551e+00,\n",
      "           -9.7092e+00,  1.1288e+01],\n",
      "          [ 2.5110e+00,  7.9684e+00,  3.1018e+00,  ..., -7.7931e+00,\n",
      "           -1.4220e+01, -2.9020e+00],\n",
      "          [-1.8484e-01,  1.9832e-01,  8.0292e-03,  ..., -1.0393e+00,\n",
      "            3.2222e-01,  1.4611e-01],\n",
      "          ...,\n",
      "          [-2.2252e+00,  6.4843e+00, -8.3649e+00,  ...,  8.1928e+00,\n",
      "           -5.5022e+00,  9.1374e+00],\n",
      "          [-1.7680e+00, -7.3426e-01,  2.9983e+00,  ..., -3.2611e-01,\n",
      "            1.3275e+00,  1.2646e+00],\n",
      "          [ 9.1926e-02,  1.6671e+00, -1.1304e+00,  ..., -5.4147e-01,\n",
      "           -4.8229e-01, -2.2320e+00]],\n",
      "\n",
      "         [[-1.2127e+00,  1.1699e+01,  1.0487e+01,  ..., -8.4434e+00,\n",
      "            4.0431e-01, -1.1242e+01],\n",
      "          [-4.5635e+00, -3.7231e+00, -6.9320e+00,  ...,  2.6084e+00,\n",
      "            2.5359e+00, -1.6921e+00],\n",
      "          [-6.0641e-01,  1.5291e-01,  2.7914e-01,  ...,  3.7757e-01,\n",
      "            7.2914e-01,  3.3150e-01],\n",
      "          ...,\n",
      "          [-6.0821e+00, -1.0561e+00,  3.2793e+00,  ...,  7.1469e+00,\n",
      "            1.2280e+01, -9.0018e+00],\n",
      "          [-3.0864e+00, -2.4437e+00, -1.6322e+00,  ..., -1.2517e+00,\n",
      "            8.2975e+00,  7.7372e+00],\n",
      "          [-1.1515e+00, -2.6104e-01, -2.1446e+00,  ...,  2.4308e-01,\n",
      "            2.8947e+00,  1.1305e+00]]]], grad_fn=<TransposeBackward0>))), decoder_hidden_states=None, decoder_attentions=None, cross_attentions=None, encoder_last_hidden_state=tensor([[[ 0.2828,  0.0979,  0.1090,  ..., -0.2794,  0.2154,  0.0370],\n",
      "         [ 0.1593,  0.0956, -0.0922,  ...,  0.1854,  0.0993,  0.1548],\n",
      "         [ 0.0076,  0.0090, -0.0092,  ...,  0.0050,  0.0019,  0.0076],\n",
      "         ...,\n",
      "         [-0.0119, -0.1228, -0.0638,  ..., -0.0602, -0.1791, -0.0499],\n",
      "         [-0.0342, -0.0511, -0.1488,  ...,  0.3007, -0.2612, -0.1452],\n",
      "         [ 0.0670,  0.1300, -0.0356,  ..., -0.0100,  0.0690, -0.0531]]],\n",
      "       grad_fn=<MulBackward0>), encoder_hidden_states=None, encoder_attentions=None)\n"
     ]
    }
   ],
   "source": [
    "# try this one it is from documentation T5\n",
    "\n",
    "from transformers import T5Tokenizer, T5Model\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "model = T5Model.from_pretrained(\"t5-small\")\n",
    "\n",
    "input_ids = tokenizer(\n",
    "    \"Studies have been shown that owning a dog is good for you\", return_tensors=\"pt\"\n",
    ").input_ids  # Batch size 1\n",
    "decoder_input_ids = tokenizer(\"Studies show that\", return_tensors=\"pt\").input_ids  # Batch size 1\n",
    "\n",
    "# forward pass\n",
    "outputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)\n",
    "last_hidden_states = outputs.last_hidden_state\n",
    "print(outputs)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qgmc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
