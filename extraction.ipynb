{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we are downloading PDF file, converting it to TXT and doing some \"pre-cleaning\": removing not meaningful parts of document and leaving just the most valuable leftovers for our future generator.\n",
    "THe outcome of the below code is pre-processed but still raw data.\n",
    "\n",
    "\n",
    "\"extracted_text\" variable has \"StringIO\" type: The StringIO object is part of Python's io module and is a class that provides an in-memory file-like object that can be used for reading from or writing to strings as if they were files. It allows you to treat strings as file-like objects, which can be useful in various situations, such as when you want to read from or write to a string in a way that mimics file operations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import of libraries\n",
    "from io import StringIO # extracted_text is the main variable, contains the whole text of document in stringIO format in memory\n",
    "import requests\n",
    "import re  # provides reg. exp. support\n",
    "import math\n",
    "import api\n",
    "from selenium import webdriver\n",
    "\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfdocument import PDFDocument\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.pdfparser import PDFParser\n",
    "import fitz\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import spacy\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "import sentencepiece\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer, BertForQuestionAnswering, BertTokenizer\n",
    "from transformers import AutoTokenizer\n",
    "from keybert import KeyBERT\n",
    "import gradio as gr # UI part for the quize\n",
    "import urllib3\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import fuzzywuzzy\n",
    "\n",
    "# libraries for conversion from csv to json, results of \"Flag\" button click CSV -> JSON\n",
    "import csv\n",
    "import json\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U pip setuptools wheel\n",
    "#!pip install -U spacy\n",
    "#!python -m spacy download en_core_web_sm\n",
    "#!pip install PyMuPDF # this is fitz\n",
    "#!pip install gradio\n",
    "#!pip install keybert\n",
    "#!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1113747"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# downloading pdf to '/data/' folder\n",
    "url = 'https://astqb.org/assets/documents/ISTQB_CTFL_Syllabus-v4.0.pdf'\n",
    "r = requests.get(url, allow_redirects=True)\n",
    "open('data/ISTQB_CTFL_Syllabus-v4.0.pdf', 'wb').write(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "r\"Page \\d{4,74} of 74\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted text for 'The Certified Tester Foundation Level in Software Testing' saved to 'data/ISTQB_CTFL_Syllabus-v4.0.txt'\n"
     ]
    }
   ],
   "source": [
    "#converting pdf to text and saving into .txt file initial version\n",
    "output_string = StringIO()\n",
    "output_file_path = 'data/ISTQB_CTFL_Syllabus-v4.0.txt'\n",
    "with open('data/ISTQB_CTFL_Syllabus-v4.0.pdf', 'rb') as in_file, open(output_file_path, 'w', encoding='utf-8') as out_file:\n",
    "    parser = PDFParser(in_file)\n",
    "    doc = PDFDocument(parser)\n",
    "    rsrcmgr = PDFResourceManager()\n",
    "    device = TextConverter(rsrcmgr, output_string, laparams=LAParams())\n",
    "    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "    for page in PDFPage.create_pages(doc):\n",
    "        interpreter.process_page(page)\n",
    "    # Getting the extracted text from StringIO, it means the entire text extracted from the PDF is stored as a single string in memory.\n",
    "    extracted_text = output_string.getvalue()\n",
    "    # Writing the extracted text to the output file\n",
    "    out_file.write(extracted_text)\n",
    "\n",
    "# Closing the stream\n",
    "output_string.close()\n",
    "\n",
    "\n",
    "# Printing message to indicate that the text has been saved to the file\n",
    "print(f\"Extracted text for 'The Certified Tester Foundation Level in Software Testing' saved to '{output_file_path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of string in bytes : 198489\n",
      "File size, document contains 70+ pages:  193.84 KB\n"
     ]
    }
   ],
   "source": [
    "# let us check size of StringIO on the full size of converted file, just out of curiosity\n",
    "size_bytes = len(extracted_text.encode('utf-8'))\n",
    "print ('The length of string in bytes : ' + str (size_bytes))\n",
    "\n",
    "# function's code is taken from stackoverflow ---\n",
    "def convert_size(size_bytes):\n",
    "   if size_bytes == 0:\n",
    "       return \"0B\"\n",
    "   size_name = (\"B\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\", \"EB\", \"ZB\", \"YB\")\n",
    "   i = int(math.floor(math.log(size_bytes, 1024)))\n",
    "   p = math.pow(1024, i)\n",
    "   s = round(size_bytes / p, 2)\n",
    "   return \"%s %s\" % (s, size_name[i])\n",
    "# ---\n",
    "print(\"File size, document contains 70+ pages: \", convert_size(size_bytes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted text for 'The Certified Tester Foundation Level in Software Testing' pre processed version 0.1 saved to 'data/ISTQB_CTFL_Syllabus-v4.0_v01.txt'\n"
     ]
    }
   ],
   "source": [
    "# Looking up for the text to remove everything before it\n",
    "target_text = \"1.1. What is Testing?\"\n",
    "\n",
    "# Finding the position of the target text in the extracted text\n",
    "start_position = extracted_text.find(target_text)\n",
    "\n",
    "# Checking if the target text was found, just in case\n",
    "if start_position != -1:\n",
    "    # Removing everything before the target text\n",
    "    extracted_text = extracted_text[start_position:]\n",
    "\n",
    "\n",
    "# let us save the content to .txt file with prefix '_v0.1' for further debugging purpose and human evaluation process\n",
    "\n",
    "output_file_path = 'data/ISTQB_CTFL_Syllabus-v4.0_v01.txt'\n",
    "with open(output_file_path, 'w', encoding='utf-8') as out_file:\n",
    "    # Writing the extracted text to the output file\n",
    "    out_file.write(extracted_text)\n",
    "\n",
    "# Closing the stream\n",
    "output_string.close()\n",
    "\n",
    "# Printing message to indicate that the text has been saved to the file\n",
    "print(f\"Extracted text for 'The Certified Tester Foundation Level in Software Testing' pre processed version 0.1 saved to '{output_file_path}'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing empty lines\n",
    "# _ - is iterator, if s.strip(): This part of the list comprehension checks whether the line s contains any non-whitespace characters. \n",
    "# If it does, the line is included in the resulting list.\n",
    "\n",
    "# extracted_text = \"\".join([_ for _ in extracted_text.strip().splitlines(True) if _.strip()])\n",
    "# \n",
    "# output_file_path = 'data/ISTQB_CTFL_Syllabus-v4.0_v02.txt'\n",
    "# with open('data/ISTQB_CTFL_Syllabus-v4.0_v01.txt', 'rb') as in_file, open(output_file_path, 'w', encoding='utf-8') as out_file:\n",
    " #   Writing the extracted text to the output file\n",
    "    # out_file.write(extracted_text)\n",
    "# \n",
    "#Closing the stream\n",
    "# output_string.close()\n",
    "# \n",
    "#Printing message to indicate that the text has been saved to the file\n",
    "# print(f\"Extracted text for 'The Certified Tester Foundation Level in Software Testing' pre processed version 0.2 saved to '{output_file_path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted text for 'The Certified Tester Foundation Level in Software Testing' pre processed version 0.3 saved to 'data/ISTQB_CTFL_Syllabus-v4.0_v03.txt'\n",
      "1.1. What is Testing? \n",
      "\n",
      "Software systems are an integral part of our daily life. Most people have had experience with software \n",
      "that did not work as expected. Software that does not work correctly can lead to many problems, \n",
      "including loss of money, time or business reputation, and, in extreme cases, even injury or death. \n",
      "Software testing assesses software quality and helps reducing the risk of software failure in operation. \n",
      "\n",
      "Software testing is a set of activities to discover defects and evaluate the quality of software artifacts. \n",
      "These artifacts, when being tested, are known as test objects. A common misconception about testing is \n",
      "that it only consists of executing tests (i.e., running the software and checking the test results). However, \n",
      "software testing also includes other activities and must be aligned with the software development lifecycle \n",
      "(see chapter 2). \n",
      "\n",
      "Another common misconception about testing is that testing focuses entirely on verifying the test object. \n",
      "Whilst testing involves verification, i.e., checking whether the system meets specified requirements, it also \n",
      "involves validation, which means checking whether the system meets users’ and other stakeholders’ \n",
      "needs in its operational environment. \n",
      "\n",
      "Testing may be dynamic or static. Dynamic testing involves the execution of software, while static testing \n",
      "does not. Static testing includes reviews (see chapter 3) and static analysis. Dynamic testing uses \n",
      "different types of test techniques and test approaches to derive test cases (see chapter 4). \n",
      "\n",
      "Testing is not only a technical activity. It also needs to be properly planned, managed, estimated, \n",
      "monitored and controlled (see chapter 5). \n",
      "\n",
      "Testers use tools (see chapter 6), but it is important to remember that testing is largely an intellectual \n",
      "activity, requiring the testers to have specialized knowledge, use analytical skills and apply critical \n",
      "thinking and systems thinking (Myers 2011, Roman 2018). \n",
      "\n",
      "The ISO/IEC/IEEE 29119-1 standard provides further information about software testing concepts. \n",
      "\n",
      "1.1.1.  Test Objectives \n",
      "\n",
      "The typical test objectives are: \n",
      "\n",
      "•  Evaluating work products such as requirements, user stories, designs, and code  \n",
      "\n",
      "•  Triggering failures and finding defects \n",
      "\n",
      "•  Ensuring required coverage of a test object \n",
      "\n",
      "•  Reducing the level of risk of inadequate software quality \n",
      "\n",
      "•  Verifying whether specified requirements have been fulfilled \n",
      "\n",
      "•  Verifying that a test object complies with contractual, legal, and regulatory requirements \n",
      "\n",
      "•  Providing information to stakeholders to allow them to make informed decisions \n",
      "\n",
      "•  Building confidence in the quality of the test object \n",
      "\n",
      "•  Validating whether the test object is complete and works as expected by the stakeholders \n",
      "\n",
      "Objectives of testing can vary, depending upon the context, which includes the work product being tested, \n",
      "the test level, risks, the software development lifecycle (SDLC) being followed, and factors related to the \n",
      "business context, e.g., corporate structure, competitive considerations, or time to market. \n",
      "\n",
      "v4.0 \n",
      "\n",
      "Page 15 of 74 \n",
      "\n",
      "2023-04-21 \n",
      "\n",
      "© International Software Testing Qualifications Board \n",
      "\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\fCertified Tester \n",
      "Foundation Level \n",
      "\n",
      "1.1.2.  Testing and Debugging \n",
      "\n",
      "Testing and debugging are separate activities. Testing can trigger failures that are caused by defects in \n",
      "the software (dynamic testing) or can directly find defects in the test object (static testing).  \n",
      "\n",
      "When dynamic testing (see chapter 4) triggers a failure, debugging is concerned with finding causes of \n",
      "this failure (defects), analyzing these causes, and eliminating them. The typical debugging process in this \n",
      "case involves:  \n",
      "\n",
      "•  Reproduction of a failure \n",
      "\n",
      "•  Diagnosis (finding the root cause) \n",
      "\n",
      "•  Fixing the cause \n",
      "\n",
      "Subsequent confirmation testing checks whether the fixes resolved the problem. Preferably, confirmation \n",
      "testing is done by the same person who performed the initial test. Subsequent regression testing can also \n",
      "be performed, to check whether the fixes are causing failures in other parts of the test object (see section \n",
      "2.2.3 for more information on confirmation testing and regression testing).  \n",
      "\n",
      "When static testing identifies a defect, debugging is concerned with removing it. There is no need for \n",
      "reproduction or diagnosis, since static testing directly finds defects, and cannot cause failures (see \n",
      "chapter 3). \n",
      "\n",
      "1.2. Why is Testing Necessary? \n",
      "\n",
      "Testing, as a form of quality control, helps in achieving the agreed upon goals within the set scope, time, \n",
      "quality, and budget constraints. Testing’s contribution to success should not be restricted to the test team \n",
      "activities. Any stakeholder can use their testing skills to bring the project closer to success. Testing \n",
      "components, systems, and associated documentation helps to identify defects in software,  \n",
      "\n",
      "1.2.1.  Testing’s Contributions to Success \n",
      "\n",
      "Testing provides a cost-effective means of detecting defects. These defects can then be removed (by \n",
      "debugging – a non-testing activity), so testing indirectly contributes to higher quality test objects. \n",
      "\n",
      "Testing provides a means of directly evaluating the quality of a test object at various stages in the SDLC. \n",
      "These measures are used as part of a larger project management activity, contributing to decisions to \n",
      "move to the next stage of the SDLC, such as the release decision. \n",
      "\n",
      "Testing provides users with indirect representation on the development project. Testers ensure that their \n",
      "understanding of users’ needs are considered throughout the development lifecycle. The alternative is to \n",
      "involve a representative set of users as part of the development project, which is not usually possible due \n",
      "to the high costs and lack of availability of suitable users. \n",
      "\n",
      "Testing may also be required to meet contractual or legal requirements, or to comply with regulatory \n",
      "standards. \n",
      "\n",
      "1.2.2.  Testing and Quality Assurance (QA) \n",
      "\n",
      "While people often use the terms “testing” and “quality assurance” (QA) interchangeably, testing and QA \n",
      "are not the same. Testing is a form of quality control (QC).  \n",
      "\n",
      "v4.0 \n",
      "\n",
      "Page 16 of 74 \n",
      "\n",
      "2023-04-21 \n",
      "\n",
      "© International Software Testing Qualifications Board \n",
      "\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\fCertified Tester \n",
      "Foundation Level \n",
      "\n",
      "QC is a product-oriented, corrective approach that focuses on those activities supporting the achievement \n",
      "of appropriate levels of quality. Testing is a major form of quality control, while others include formal \n",
      "methods (model checking and proof of correctness), simulation and prototyping. \n",
      "\n",
      "QA is a process-oriented, preventive approach that focuses on the implementation and improvement of \n",
      "processes. It works on the basis that if a good process is followed correctly, then it will generate a good \n",
      "product. QA applies to both the development and testing processes, and is the responsibility of everyone \n",
      "on a project. \n",
      "\n",
      "Test results are used by QA and QC. In QC they are used to fix defects, while in QA they provide \n",
      "feedback on how well the development and test processes are performing. \n",
      "\n",
      "1.2.3.  Errors, Defects, Failures, and Root Causes \n",
      "\n",
      "Human beings make errors (mistakes), which produce defects (faults, bugs), which in turn may result in \n",
      "failures. Humans make errors for various reasons, such as time pressure, complexity of work products, \n",
      "processes, infrastructure or interactions, or simply because they are tired or lack adequate training.  \n",
      "\n",
      "Defects can be found in documentation, such as a requirements specification or a test script, in source \n",
      "code, or in a supporting artifact such as a build file. Defects in artifacts produced earlier in the SDLC, if \n",
      "undetected, often lead to defective artifacts later in the lifecycle. If a defect in code is executed, the \n",
      "system may fail to do what it should do, or do something it shouldn’t, causing a failure. Some defects will \n",
      "always result in a failure if executed, while others will only result in a failure in specific circumstances, and \n",
      "some may never result in a failure. \n",
      "\n",
      "Errors and defects are not the only cause of failures. Failures can also be caused by environmental \n",
      "conditions, such as when radiation or electromagnetic field cause defects in firmware. \n",
      "\n",
      "A root cause is a fundamental reason for the occurrence of a problem (e.g., a situation that leads to an \n",
      "error). Root causes are identified through root cause analysis, which is typically performed when a failure \n",
      "occurs or a defect is identified. It is believed that further similar failures or defects can be prevented or \n",
      "their frequency reduced by addressing the root cause, such as by removing it. \n",
      "\n",
      "1.3. Testing Principles \n",
      "\n",
      "A number of testing principles offering general guidelines applicable to all testing have been suggested \n",
      "over the years. This syllabus describes seven such principles.  \n",
      "\n",
      "1. Testing shows the presence, not the absence of defects. Testing can show that defects are present \n",
      "in the test object, but cannot prove that there are no defects (Buxton 1970). Testing reduces the \n",
      "probability of defects remaining undiscovered in the test object, but even if no defects are found, testing \n",
      "cannot prove test object correctness. \n",
      "\n",
      "2. Exhaustive testing is impossible. Testing everything is not feasible except in trivial cases (Manna \n",
      "1978). Rather than attempting to test exhaustively, test techniques (see chapter 4), test case prioritization \n",
      "(see section 5.1.5), and risk-based testing (see section 5.2), should be used to focus test efforts. \n",
      "\n",
      "3. Early testing saves time and money. Defects that are removed early in the process will not cause \n",
      "subsequent defects in derived work products. The cost of quality will be reduced since fewer failures will \n",
      "occur later in the SDLC (Boehm 1981). To find defects early, both static testing (see chapter 3) and \n",
      "dynamic testing (see chapter 4) should be started as early as possible. \n",
      "\n",
      "4. Defects cluster together. A small number of system components usually contain most of the defects \n",
      "discovered or are responsible for most of the operational failures (Enders 1975). This phenomenon is an \n",
      "\n",
      "v4.0 \n",
      "\n",
      "Page 17 of 74 \n",
      "\n",
      "2023-04-21 \n",
      "\n",
      "© International Software Testing Qualifications Board \n",
      "\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\fCertified Tester \n",
      "Foundation Level \n",
      "\n",
      "illustration of the Pareto principle. Predicted defect clusters, and actual defect clusters observed during \n",
      "testing or in operation, are an important input for risk-based testing (see section 5.2). \n",
      "\n",
      "5. Tests wear out. If the same tests are repeated many times, they become increasingly ineffective in \n",
      "detecting new defects (Beizer 1990). To overcome this effect, existing tests and test data may need to be \n",
      "modified, and new tests may need to be written. However, in some cases, repeating the same tests can \n",
      "have a beneficial outcome, e.g., in automated regression testing (see section 2.2.3). \n",
      "\n",
      "6. Testing is context dependent. There is no single universally applicable approach to testing. Testing is \n",
      "done differently in different contexts (Kaner 2011). \n",
      "\n",
      "7. Absence-of-defects fallacy. It is a fallacy (i.e., a misconception) to expect that software verification \n",
      "will ensure the success of a system. Thoroughly testing all the specified requirements and fixing all the \n",
      "defects found could still produce a system that does not fulfill the users’ needs and expectations, that \n",
      "does not help in achieving the customer’s business goals, and that is inferior compared to other \n",
      "competing systems. In addition to verification, validation should also be carried out (Boehm 1981). \n",
      "\n",
      "1.4. Test Activities, Testware and Test Roles \n",
      "\n",
      "Testing is context dependent, but, at a high level, there are common sets of test activities without which \n",
      "testing is less likely to achieve test objectives. These sets of test activities form a test process. The test \n",
      "process can be tailored to a given situation based on various factors. Which test activities are included in \n",
      "this test process, how they are implemented, and when they occur is normally decided as part of the test \n",
      "planning for the specific situation (see section 5.1). \n",
      "\n",
      "The following sections describe the general aspects of this test process in terms of test activities and \n",
      "tasks, the impact of context, testware, traceability between the test basis and testware, and testing roles. \n",
      "\n",
      "The ISO/IEC/IEEE 29119-2 standard provides further information about test processes. \n",
      "\n",
      "1.4.1.  Test Activities and Tasks  \n",
      "\n",
      "A test process usually consists of the main groups of activities described below. Although many of these \n",
      "activities may appear to follow a logical sequence, they are often implemented iteratively or in parallel. \n",
      "These testing activities usually need to be tailored to the system and the project. \n",
      "\n",
      "Test planning consists of defining the test objectives and then selecting an approach that best achieves \n",
      "the objectives within the constraints imposed by the overall context. Test planning is further explained in \n",
      "section 5.1. \n",
      "\n",
      "Test monitoring and control. Test monitoring involves the ongoing checking of all test activities and the \n",
      "comparison of actual progress against the plan. Test control involves taking the actions necessary to \n",
      "meet the objectives of testing. Test monitoring and control are further explained in section 5.3. \n",
      "\n",
      "Test analysis includes analyzing the test basis to identify testable features and to define and prioritize \n",
      "associated test conditions, together with the related risks and risk levels (see section 5.2). The test basis \n",
      "and the test objects are also evaluated to identify defects they may contain and to assess their testability. \n",
      "Test analysis is often supported by the use of test techniques (see chapter 4). Test analysis answers the \n",
      "question “what to test?” in terms of measurable coverage criteria.  \n",
      "\n",
      "Test design includes elaborating the test conditions into test cases and other testware (e.g., test \n",
      "charters). This activity often involves the identification of coverage items, which serve as a guide to \n",
      "specify test case inputs. Test techniques (see chapter 4) can be used to support this activity. Test design \n",
      "\n",
      "v4.0 \n",
      "\n",
      "Page 18 of 74 \n",
      "\n",
      "2023-04-21 \n",
      "\n",
      "© International Software Testing Qualifications Board \n",
      "\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\fCertified Tester \n",
      "Foundation Level \n",
      "\n",
      "also includes defining the test data requirements, designing the test environment and identifying any \n",
      "other required infrastructure and tools. Test design answers the question “how to test?”. \n",
      "\n",
      "Test implementation includes creating or acquiring the testware necessary for test execution (e.g., test \n",
      "data). Test cases can be organized into test procedures and are often assembled into test suites. Manual \n",
      "and automated test scripts are created. Test procedures are prioritized and arranged within a test \n",
      "execution schedule for efficient test execution (see section 5.1.5). The test environment is built and \n",
      "verified to be set up correctly. \n",
      "\n",
      "Test execution includes running the tests in accordance with the test execution schedule (test runs). \n",
      "Test execution may be manual or automated. Test execution can take many forms, including continuous \n",
      "testing or pair testing sessions. Actual test results are compared with the expected results. The test \n",
      "results are logged. Anomalies are analyzed to identify their likely causes. This analysis allows us to report \n",
      "the anomalies based on the failures observed (see section 5.5). \n",
      "\n",
      "Test completion activities usually occur at project milestones (e.g., release, end of iteration, test level \n",
      "completion) for any unresolved defects, change requests or product backlog items created. Any testware \n",
      "that may be useful in the future is identified and archived or handed over to the appropriate teams. The \n",
      "test environment is shut down to an agreed state. The test activities are analyzed to identify lessons \n",
      "learned and improvements for future iterations, releases, or projects (see section 2.1.6). A test completion \n",
      "report is created and communicated to the stakeholders. \n",
      "\n",
      "1.4.2.  Test Process in Context \n",
      "\n",
      "Testing is not performed in isolation. Test activities are an integral part of the development processes \n",
      "carried out within an organization. Testing is also funded by stakeholders and its final goal is to help fulfill \n",
      "the stakeholders’ business needs. Therefore, the way the testing is carried out will depend on a number \n",
      "of contextual factors including: \n",
      "\n",
      "•  Stakeholders (needs, expectations, requirements, willingness to cooperate, etc.) \n",
      "\n",
      "•  Team members (skills, knowledge, level of experience, availability, training needs, etc.) \n",
      "\n",
      "•  Business domain (criticality of the test object, identified risks, market needs, specific legal \n",
      "\n",
      "regulations, etc.) \n",
      "\n",
      "•  Technical factors (type of software, product architecture, technology used, etc.) \n",
      "\n",
      "•  Project constraints (scope, time, budget, resources, etc.) \n",
      "\n",
      "•  Organizational factors (organizational structure, existing policies, practices used, etc.) \n",
      "\n",
      "•  Software development lifecycle (engineering practices, development methods, etc.) \n",
      "\n",
      "•  Tools (availability, usability, compliance, etc.) \n",
      "\n",
      "These factors will have an impact on many test-related issues, including: test strategy, test techniques \n",
      "used, degree of test automation, required level of coverage, level of detail of test documentation, \n",
      "reporting, etc. \n",
      "\n",
      "1.4.3.  Testware  \n",
      "\n",
      "Testware is created as output work products from the test activities described in section 1.4.1. There is a \n",
      "significant variation in how different organizations produce, shape, name, organize and manage their \n",
      "\n",
      "v4.0 \n",
      "\n",
      "Page 19 of 74 \n",
      "\n",
      "2023-04-21 \n",
      "\n",
      "© International Software Testing Qualifications Board \n",
      "\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\fCertified Tester \n",
      "Foundation Level \n",
      "\n",
      "work products. Proper configuration management (see section 5.4) ensures consistency and integrity of \n",
      "work products. The following list of work products is not exhaustive: \n",
      "\n",
      "•  Test planning work products include: test plan, test schedule, risk register, and entry and exit \n",
      "\n",
      "criteria (see section 5.1). Risk register is a list of risks together with risk likelihood, risk impact and \n",
      "information about risk mitigation (see section 5.2). Test schedule, risk register and entry and exit \n",
      "criteria are often a part of the test plan. \n",
      "\n",
      "•  Test monitoring and control work products include: test progress reports (see section 5.3.2), \n",
      "documentation of control directives (see section 5.3) and risk information (see section 5.2).  \n",
      "\n",
      "•  Test analysis work products include: (prioritized) test conditions (e.g., acceptance criteria, see \n",
      "\n",
      "section 4.5.2), and defect reports regarding defects in the test basis (if not fixed directly). \n",
      "\n",
      "•  Test design work products include: (prioritized) test cases, test charters, coverage items, test \n",
      "\n",
      "data requirements and test environment requirements. \n",
      "\n",
      "•  Test implementation work products include: test procedures, automated test scripts, test \n",
      "suites, test data, test execution schedule, and test environment elements. Examples of test \n",
      "environment elements include: stubs, drivers, simulators, and service virtualizations. \n",
      "\n",
      "•  Test execution work products include: test logs, and defect reports (see section 5.5). \n",
      "\n",
      "•  Test completion work products include: test completion report (see section 5.3.2), action items \n",
      "\n",
      "for improvement of subsequent projects or iterations, documented lessons learned, and change \n",
      "requests (e.g., as product backlog items). \n",
      "\n",
      "1.4.4.  Traceability between the Test Basis and Testware \n",
      "\n",
      "In order to implement effective test monitoring and control, it is important to establish and maintain \n",
      "traceability throughout the test process between the test basis elements, testware associated with these \n",
      "elements (e.g., test conditions, risks, test cases), test results, and detected defects. \n",
      "\n",
      "Accurate traceability supports coverage evaluation, so it is very useful if measurable coverage criteria are \n",
      "defined in the test basis. The coverage criteria can function as key performance indicators to drive the \n",
      "activities that show to what extent the test objectives have been achieved (see section 1.1.1). For \n",
      "example: \n",
      "\n",
      "•  Traceability of test cases to requirements can verify that the requirements are covered by test \n",
      "\n",
      "cases. \n",
      "\n",
      "•  Traceability of test results to risks can be used to evaluate the level of residual risk in a test \n",
      "\n",
      "object.  \n",
      "\n",
      "In addition to evaluating coverage, good traceability makes it possible to determine the impact of \n",
      "changes, facilitates test audits, and helps meet IT governance criteria. Good traceability also makes test \n",
      "progress and completion reports more easily understandable by including the status of test basis \n",
      "elements. This can also assist in communicating the technical aspects of testing to stakeholders in an \n",
      "understandable manner. Traceability provides information to assess product quality, process capability, \n",
      "and project progress against business goals.  \n",
      "\n",
      "v4.0 \n",
      "\n",
      "Page 20 of 74 \n",
      "\n",
      "2023-04-21 \n",
      "\n",
      "© International Software Testing Qualifications Board \n",
      "\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\fCertified Tester \n",
      "Foundation Level \n",
      "\n",
      "1.4.5.  Roles in Testing \n",
      "\n",
      "In this syllabus, two principal roles in testing are covered: a test management role and a testing role. The \n",
      "activities and tasks assigned to these two roles depend on factors such as the project and product \n",
      "context, the skills of the people in the roles, and the organization. \n",
      "\n",
      "The test management role takes overall responsibility for the test process, test team and leadership of the \n",
      "test activities. The test management role is mainly focused on the activities of test planning, test \n",
      "monitoring and control and test completion. The way in which the test management role is carried out \n",
      "varies depending on the context. For example, in Agile software development, some of the test \n",
      "management tasks may be handled by the Agile team. Tasks that span multiple teams or the entire \n",
      "organization may be performed by test managers outside of the development team. \n",
      "\n",
      "The testing role takes overall responsibility for the engineering (technical) aspect of testing. The testing \n",
      "role is mainly focused on the activities of test analysis, test design, test implementation and test \n",
      "execution. \n",
      "\n",
      "Different people may take on these roles at different times. For example, the test management role can \n",
      "be performed by a team leader, by a test manager, by a development manager, etc. It is also possible for \n",
      "one person to take on the roles of testing and test management at the same time. \n",
      "\n",
      "1.5. Essential Skills and Good Practices in Testing \n",
      "\n",
      "Skill is the ability to do something well that comes from one’s knowledge, practice and aptitude. Good \n",
      "testers should possess some essential skills to do their job well. Good testers should be effective team \n",
      "players and should be able to perform testing on different levels of test independence. \n",
      "\n",
      "1.5.1.  Generic Skills Required for Testing \n",
      "\n",
      "While being generic, the following skills are particularly relevant for testers: \n",
      "\n",
      "•  Testing knowledge (to increase effectiveness of testing, e.g., by using test techniques) \n",
      "\n",
      "•  Thoroughness, carefulness, curiosity, attention to details, being methodical (to identify defects, \n",
      "\n",
      "especially the ones that are difficult to find) \n",
      "\n",
      "•  Good communication skills, active listening, being a team player (to interact effectively with all \n",
      "stakeholders, to convey information to others, to be understood, and to report and discuss \n",
      "defects) \n",
      "\n",
      "•  Analytical thinking, critical thinking, creativity (to increase effectiveness of testing) \n",
      "\n",
      "•  Technical knowledge (to increase efficiency of testing, e.g., by using appropriate test tools) \n",
      "\n",
      "•  Domain knowledge (to be able to understand and to communicate with end users/business \n",
      "\n",
      "representatives) \n",
      "\n",
      "Testers are often the bearers of bad news. It is a common human trait to  blame the bearer of bad news. \n",
      "This  makes  communication  skills  crucial  for  testers.  Communicating  test  results  may  be  perceived  as \n",
      "criticism of the product and of its author. Confirmation bias can make it difficult to accept information that \n",
      "disagrees  with  currently  held  beliefs.  Some  people  may  perceive  testing  as  a  destructive  activity,  even \n",
      "though it contributes greatly to project success and product quality. To try to improve this view, information \n",
      "about defects and failures should be communicated in a constructive way. \n",
      "\n",
      "v4.0 \n",
      "\n",
      "Page 21 of 74 \n",
      "\n",
      "2023-04-21 \n",
      "\n",
      "© International Software Testing Qualifications Board \n",
      "\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\fCertified Tester \n",
      "Foundation Level \n",
      "\n",
      "1.5.2.  Whole Team Approach \n",
      "\n",
      "One of the important skills for a tester is the ability to work effectively in a team context and to contribute \n",
      "positively to the team goals. The whole team approach – a practice coming from Extreme Programming \n",
      "(see section 2.1) – builds upon this skill.  \n",
      "\n",
      "In the whole-team approach any team member with the necessary knowledge and skills can perform any \n",
      "task, and everyone is responsible for quality. The team members share the same workspace (physical or \n",
      "virtual), as co-location facilitates communication and interaction. The whole team approach improves \n",
      "team dynamics, enhances communication and collaboration within the team, and creates synergy by \n",
      "allowing the various skill sets within the team to be leveraged for the benefit of the project. \n",
      "\n",
      "Testers work closely with other team members to ensure that the desired quality levels are achieved. This \n",
      "includes collaborating with business representatives to help them create suitable acceptance tests and \n",
      "working with developers to agree on the test strategy and decide on test automation approaches. Testers \n",
      "can thus transfer testing knowledge to other team members and influence the development of the \n",
      "product. \n",
      "\n",
      "Depending on the context, the whole team approach may not always be appropriate. For instance, in \n",
      "some situations, such as safety-critical, a high level of test independence may be needed. \n",
      "\n",
      "1.5.3. \n",
      "\n",
      "Independence of Testing \n",
      "\n",
      "A certain degree of independence makes the tester more effective at finding defects due to differences \n",
      "between the author’s and the tester’s cognitive biases (cf. Salman 1995). Independence is not, however, \n",
      "a replacement for familiarity, e.g., developers can efficiently find many defects in their own code. \n",
      "\n",
      "Work products can be tested by their author (no independence), by the author's peers from the same \n",
      "team (some independence), by testers from outside the author's team but within the organization (high \n",
      "independence), or by testers from outside the organization (very high independence). For most projects, it \n",
      "is usually best to carry out testing with multiple levels of independence (e.g., developers performing \n",
      "component and component integration testing, test team performing system and system integration \n",
      "testing, and business representatives performing acceptance testing). \n",
      "\n",
      "The main benefit of independence of testing is that independent testers are likely to recognize different \n",
      "kinds of failures and defects compared to developers because of their different backgrounds, technical \n",
      "perspectives, and biases. Moreover, an independent tester can verify, challenge, or disprove \n",
      "assumptions made by stakeholders during specification and implementation of the system.  \n",
      "\n",
      "However, there are also some drawbacks. Independent testers may be isolated from the development \n",
      "team, which may lead to a lack of collaboration, communication problems, or an adversarial relationship \n",
      "with the development team. Developers may lose a sense of responsibility for quality. Independent \n",
      "testers may be seen as a bottleneck or be blamed for delays in release. \n",
      "\n",
      "v4.0 \n",
      "\n",
      "Page 22 of 74 \n",
      "\n",
      "2023-04-21 \n",
      "\n",
      "© International Software Testing Qualifications Board \n",
      "\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\fCertified Tester \n",
      "Foundation Level \n",
      "\n",
      "2. Testing Throughout the Software Development Lifecycle \n",
      "\n",
      "– 130 minutes \n",
      "\n",
      "Keywords \n",
      "\n",
      "acceptance testing, black-box testing, component integration testing, component testing, confirmation \n",
      "testing, functional testing, integration testing, maintenance testing, non-functional testing, regression \n",
      "testing, shift-left, system integration testing, system testing, test level, test object, test type, white-box \n",
      "testing \n",
      "\n",
      "Learning Objectives for Chapter 2: \n",
      "\n",
      "2.1  Testing in the Context of a Software Development Lifecycle \n",
      "\n",
      "FL-2.1.1 \n",
      "\n",
      "(K2) Explain the impact of the chosen software development lifecycle on testing \n",
      "\n",
      "FL-2.1.2 \n",
      "\n",
      "(K1) Recall good testing practices that apply to all software development lifecycles \n",
      "\n",
      "FL-2.1.3 \n",
      "\n",
      "(K1) Recall the examples of test-first approaches to development \n",
      "\n",
      "FL-2.1.4 \n",
      "\n",
      "(K2) Summarize how DevOps might have an impact on testing  \n",
      "\n",
      "FL-2.1.5 \n",
      "\n",
      "(K2) Explain the shift-left approach \n",
      "\n",
      "FL-2.1.6 \n",
      "\n",
      "(K2) Explain how retrospectives can be used as a mechanism for process improvement \n",
      "\n",
      "2.2 Test Levels and Test Types \n",
      "\n",
      "FL-2.2.1 \n",
      "\n",
      "(K2) Distinguish the different test levels \n",
      "\n",
      "FL-2.2.2 \n",
      "\n",
      "(K2) Distinguish the different test types \n",
      "\n",
      "FL-2.2.3 \n",
      "\n",
      "(K2) Distinguish confirmation testing from regression testing \n",
      "\n",
      "2.3 Maintenance Testing \n",
      "\n",
      "FL-2.3.1 \n",
      "\n",
      "(K2) Summarize maintenance testing and its triggers \n",
      "\n",
      "v4.0 \n",
      "\n",
      "Page 23 of 74 \n",
      "\n",
      "2023-04-21 \n",
      "\n",
      "© International Software Testing Qualifications Board \n",
      "\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\fCertified Tester \n",
      "Foundation Level \n",
      "\n",
      "2.1. Testing in the Context of a Software Development Lifecycle \n",
      "\n",
      "A software development lifecycle (SDLC) model is an abstract, high-level representation of the software \n",
      "development process. A SDLC model defines how different development phases and types of activities \n",
      "performed within this process relate to each other, both logically and chronologically. Examples of SDLC \n",
      "models include: sequential development models (e.g., waterfall model, V-model), iterative development \n",
      "models (e.g., spiral model, prototyping), and incremental development models (e.g., Unified Process). \n",
      "\n",
      "Some activities within software development processes can also be described by more detailed software \n",
      "development methods and Agile practices. Examples include: acceptance test-driven development \n",
      "(ATDD), behavior-driven development (BDD), domain-driven design (DDD), extreme programming (XP), \n",
      "feature-driven development (FDD), Kanban, Lean IT, Scrum, and test-driven development (TDD). \n",
      "\n",
      "2.1.1. \n",
      "\n",
      "Impact of the Software Development Lifecycle on Testing \n",
      "\n",
      "Testing must be adapted to the SDLC to succeed. The choice of the SDLC impacts on the: \n",
      "\n",
      "•  Scope and timing of test activities (e.g., test levels and test types) \n",
      "\n",
      "•  Level of detail of test documentation \n",
      "\n",
      "•  Choice of test techniques and test approach \n",
      "\n",
      "•  Extent of test automation \n",
      "\n",
      "•  Role and responsibilities of a tester \n",
      "\n",
      "In sequential development models, in the initial phases testers typically participate in requirement \n",
      "reviews, test analysis, and test design. The executable code is usually created in the later phases, so \n",
      "typically dynamic testing cannot be performed early in the SDLC. \n",
      "\n",
      "In some iterative and incremental development models, it is assumed that each iteration delivers a \n",
      "working prototype or product increment. This implies that in each iteration both static and dynamic testing \n",
      "may be performed at all test levels. Frequent delivery of increments requires fast feedback and extensive \n",
      "regression testing. \n",
      "\n",
      "Agile software development assumes that change may occur throughout the project. Therefore, \n",
      "lightweight work product documentation and extensive test automation to make regression testing easier \n",
      "are favored in agile projects. Also, most of the manual testing tends to be done using experience-based \n",
      "test techniques (see Section 4.4) that do not require extensive prior test analysis and design. \n",
      "\n",
      "2.1.2.  Software Development Lifecycle and Good Testing Practices \n",
      "\n",
      "Good testing practices, independent of the chosen SDLC model, include the following: \n",
      "\n",
      "•  For every software development activity, there is a corresponding test activity, so that all \n",
      "\n",
      "development activities are subject to quality control \n",
      "\n",
      "•  Different test levels (see chapter 2.2.1) have specific and different test objectives, which allows \n",
      "\n",
      "for testing to be appropriately comprehensive while avoiding redundancy \n",
      "\n",
      "•  Test analysis and design for a given test level begins during the corresponding development \n",
      "\n",
      "phase of the SDLC, so that testing can adhere to the principle of early testing (see section 1.3) \n",
      "\n",
      "v4.0 \n",
      "\n",
      "Page 24 of 74 \n",
      "\n",
      "2023-04-21 \n",
      "\n",
      "© International Software Testing Qualifications Board \n",
      "\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\fCertified Tester \n",
      "Foundation Level \n",
      "\n",
      "•  Testers are involved in reviewing work products as soon as drafts of this documentation are \n",
      "\n",
      "available, so that this earlier testing and defect detection can support the shift-left strategy (see \n",
      "section 2.1.5) \n",
      "\n",
      "2.1.3.  Testing as a Driver for Software Development \n",
      "\n",
      "TDD, ATDD and BDD are similar development approaches, where tests are defined as a means of \n",
      "directing development. Each of these approaches implements the principle of early testing (see section \n",
      "1.3) and follows a shift-left approach (see section 2.1.5), since the tests are defined before the code is \n",
      "written. They support an iterative development model. These approaches are characterized as follows: \n",
      "\n",
      "Test-Driven Development (TDD): \n",
      "\n",
      "•  Directs the coding through test cases (instead of extensive software design) (Beck 2003) \n",
      "\n",
      "•  Tests are written first, then the code is written to satisfy the tests, and then the tests and code are \n",
      "\n",
      "refactored \n",
      "\n",
      "Acceptance Test-Driven Development (ATDD) (see section 4.5.3): \n",
      "\n",
      "•  Derives tests from acceptance criteria as part of the system design process (Gärtner 2011) \n",
      "\n",
      "•  Tests are written before the part of the application is developed to satisfy the tests  \n",
      "\n",
      "Behavior-Driven Development (BDD): \n",
      "\n",
      "•  Expresses the desired behavior of an application with test cases written in a simple form of \n",
      "\n",
      "natural language, which is easy to understand by stakeholders – usually using the \n",
      "Given/When/Then format. (Chelimsky 2010) \n",
      "\n",
      "•  Test cases are then automatically translated into executable tests \n",
      "\n",
      "For all the above approaches, tests may persist as automated tests to ensure the code quality in future \n",
      "adaptions / refactoring. \n",
      "\n",
      "2.1.4.  DevOps and Testing \n",
      "\n",
      "DevOps is an organizational approach aiming to create synergy by getting development (including \n",
      "testing) and operations to work together to achieve a set of common goals. DevOps requires a cultural \n",
      "shift within an organization to bridge the gaps between development (including testing) and operations \n",
      "while treating their functions with equal value. DevOps promotes team autonomy, fast feedback, \n",
      "integrated toolchains, and technical practices like continuous integration (CI) and continuous delivery \n",
      "(CD). This enables the teams to build, test and release high-quality code faster through a DevOps \n",
      "delivery pipeline (Kim 2016). \n",
      "\n",
      "From the testing perspective, some of the benefits of DevOps are: \n",
      "\n",
      "•  Fast feedback on the code quality, and whether changes adversely affect existing code \n",
      "\n",
      "•  CI promotes a shift-left approach in testing (see section 2.1.5) by encouraging developers to \n",
      "\n",
      "submit high quality code accompanied by component tests and static analysis \n",
      "\n",
      "•  Promotes automated processes like CI/CD that facilitate establishing stable test environments \n",
      "\n",
      "• \n",
      "\n",
      "Increases the view on non-functional quality characteristics (e.g., performance, reliability) \n",
      "\n",
      "v4.0 \n",
      "\n",
      "Page 25 of 74 \n",
      "\n",
      "2023-04-21 \n",
      "\n",
      "© International Software Testing Qualifications Board \n",
      "\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\fCertified Tester \n",
      "Foundation Level \n",
      "\n",
      "•  Automation through a delivery pipeline reduces the need for repetitive manual testing \n",
      "\n",
      "•  The risk in regression is minimized due to the scale and range of automated regression tests \n",
      "\n",
      "DevOps is not without its risks and challenges, which include: \n",
      "\n",
      "•  The DevOps delivery pipeline must be defined and established  \n",
      "\n",
      "•  CI / CD tools must be introduced and maintained \n",
      "\n",
      "•  Test automation requires additional resources and may be difficult to establish and maintain \n",
      "\n",
      "Although DevOps comes with a high level of automated testing, manual testing – especially from the \n",
      "user's perspective – will still be needed. \n",
      "\n",
      "2.1.5.  Shift-Left Approach \n",
      "\n",
      "The principle of early testing (see section 1.3) is sometimes referred to as shift-left because it is an \n",
      "approach where testing is performed earlier in the SDLC. Shift-left normally suggests that testing should \n",
      "be done earlier (e.g., not waiting for code to be implemented or for components to be integrated), but it \n",
      "does not mean that testing later in the SDLC should be neglected. \n",
      "\n",
      "There are some good practices that illustrate how to achieve a “shift-left” in testing, which include: \n",
      "\n",
      "•  Reviewing the specification from the perspective of testing. These review activities on \n",
      "\n",
      "specifications often find potential defects, such as ambiguities, incompleteness, and \n",
      "inconsistencies  \n",
      "\n",
      "•  Writing test cases before the code is written and have the code run in a test harness during code \n",
      "\n",
      "implementation \n",
      "\n",
      "•  Using CI and even better CD as it comes with fast feedback and automated component tests to \n",
      "\n",
      "accompany source code when it is submitted to the code repository \n",
      "\n",
      "•  Completing static analysis of source code prior to dynamic testing, or as part of an automated \n",
      "\n",
      "process \n",
      "\n",
      "•  Performing non-functional testing starting at the component test level, where possible. This is a \n",
      "\n",
      "form of shift-left as these non-functional test types tend to be performed later in the SDLC when a \n",
      "complete system and a representative test environment are available \n",
      "\n",
      "A shift-left approach might result in extra training, effort and/or costs earlier in the process but is expected \n",
      "to save efforts and/or costs later in the process. \n",
      "\n",
      "For the shift-left approach it is important that stakeholders are convinced and bought into this concept. \n",
      "\n",
      "2.1.6.  Retrospectives and Process Improvement \n",
      "\n",
      "Retrospectives (also known as “post-project meetings” and project retrospectives) are often held at the \n",
      "end of a project or an iteration, at a release milestone, or can be held when needed. The timing and \n",
      "organization of the retrospectives depend on the particular SDLC model being followed. In these \n",
      "meetings the participants (not only testers, but also e.g., developers, architects, product owner, business \n",
      "analysts) discuss: \n",
      "\n",
      "•  What was successful, and should be retained? \n",
      "\n",
      "v4.0 \n",
      "\n",
      "Page 26 of 74 \n",
      "\n",
      "2023-04-21 \n",
      "\n",
      "© International Software Testing Qualifications Board \n",
      "\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\fCertified Tester \n",
      "Foundation Level \n",
      "\n",
      "•  What was not successful and could be improved? \n",
      "\n",
      "•  How to incorporate the improvements and retain the successes in the future? \n",
      "\n",
      "The results should be recorded and are normally part of the test completion report (see section 5.3.2). \n",
      "Retrospectives are critical for the successful implementation of continuous improvement and it is \n",
      "important that any recommended improvements are followed up. \n",
      "\n",
      "Typical benefits for testing include: \n",
      "\n",
      "• \n",
      "\n",
      "Increased test effectiveness / efficiency (e.g., by implementing suggestions for process \n",
      "improvement) \n",
      "\n",
      "• \n",
      "\n",
      "Increased quality of testware (e.g., by jointly reviewing the test processes) \n",
      "\n",
      "•  Team bonding and learning (e.g., as a result of the opportunity to raise issues and propose \n",
      "\n",
      "improvement points) \n",
      "\n",
      "• \n",
      "\n",
      "Improved quality of the test basis (e.g., as deficiencies in the extent and quality of the \n",
      "requirements could be addressed and solved) \n",
      "\n",
      "•  Better cooperation between development and testing (e.g., as collaboration is reviewed and \n",
      "\n",
      "optimized regularly) \n",
      "\n",
      "2.2. Test Levels and Test Types \n",
      "\n",
      "Test levels are groups of test activities that are organized and managed together. Each test level is an \n",
      "instance of the test process, performed in relation to software at a given stage of development, from \n",
      "individual components to complete systems or, where applicable, systems of systems.  \n",
      "\n",
      "Test levels are related to other activities within the SDLC. In sequential SDLC models, the test levels are \n",
      "often defined such that the exit criteria of one level are part of the entry criteria for the next level. In some \n",
      "iterative models, this may not apply. Development activities may span through multiple test levels. Test \n",
      "levels may overlap in time. \n",
      "\n",
      "Test types are groups of test activities related to specific quality characteristics and most of those test \n",
      "activities can be performed at every test level. \n",
      "\n",
      "2.2.1.  Test Levels \n",
      "\n",
      "In this syllabus, the following five test levels are described: \n",
      "\n",
      "•  Component testing (also known as unit testing) focuses on testing components in isolation. It \n",
      "often requires specific support, such as test harnesses or unit test frameworks. Component \n",
      "testing is normally performed by developers in their development environments. \n",
      "\n",
      "•  Component integration testing (also known as unit integration testing) focuses on testing the \n",
      "interfaces and interactions between components. Component integration testing is heavily \n",
      "dependent on the integration strategy approaches like bottom-up, top-down or big-bang. \n",
      "\n",
      "•  System testing focuses on the overall behavior and capabilities of an entire system or product, \n",
      "often including functional testing of end-to-end tasks and the non-functional testing of quality \n",
      "characteristics. For some non-functional quality characteristics, it is preferable to test them on a \n",
      "complete system in a representative test environment (e.g., usability). Using simulations of sub-\n",
      "\n",
      "v4.0 \n",
      "\n",
      "Page 27 of 74 \n",
      "\n",
      "2023-04-21 \n",
      "\n",
      "© International Software Testing Qualifications Board \n",
      "\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\fCertified Tester \n",
      "Foundation Level \n",
      "\n",
      "systems is also possible. System testing may be performed by an independent test team, and is \n",
      "related to specifications for the system. \n",
      "\n",
      "•  System integration testing focuses on testing the interfaces of the system under test and other \n",
      "systems and external services . System integration testing requires suitable test environments \n",
      "preferably similar to the operational environment. \n",
      "\n",
      "•  Acceptance testing focuses on validation and on demonstrating readiness for deployment, \n",
      "\n",
      "which means that the system fulfills the user’s business needs. Ideally, acceptance testing should \n",
      "be performed by the intended users. The main forms of acceptance testing are: user acceptance \n",
      "testing (UAT), operational acceptance testing, contractual and regulatory acceptance testing, \n",
      "alpha testing and beta testing. \n",
      "\n",
      "Test levels are distinguished by the following non-exhaustive list of attributes, to avoid overlapping of test \n",
      "activities: \n",
      "\n",
      "•  Test object  \n",
      "\n",
      "•  Test objectives \n",
      "\n",
      "•  Test basis \n",
      "\n",
      "•  Defects and failures \n",
      "\n",
      "•  Approach and responsibilities \n",
      "\n",
      "2.2.2.  Test Types \n",
      "\n",
      "A lot of test types exist and can be applied in projects. In this syllabus, the following four test types are \n",
      "addressed: \n",
      "\n",
      "Functional testing evaluates the functions that a component or system should perform. The functions \n",
      "are “what” the test object should do. The main objective of functional testing is checking the functional \n",
      "completeness, functional correctness and functional appropriateness. \n",
      "\n",
      "Non-functional testing evaluates attributes other than functional characteristics of a component or \n",
      "system. Non-functional testing is the testing of “how well the system behaves”. The main objective of non-\n",
      "functional testing is checking the non-functional software quality characteristics. The ISO/IEC 25010 \n",
      "standard provides the following classification of the non-functional software quality characteristics: \n",
      "\n",
      "•  Performance efficiency \n",
      "\n",
      "•  Compatibility \n",
      "\n",
      "•  Usability \n",
      "\n",
      "•  Reliability \n",
      "\n",
      "•  Security \n",
      "\n",
      "•  Maintainability \n",
      "\n",
      "•  Portability \n",
      "\n",
      "It is sometimes appropriate for non-functional testing to start early in the life cycle (e.g., as part of reviews \n",
      "and component testing or system testing). Many non-functional tests are derived from functional tests as \n",
      "\n",
      "v4.0 \n",
      "\n",
      "Page 28 of 74 \n",
      "\n",
      "2023-04-21 \n",
      "\n",
      "© International Software Testing Qualifications Board \n",
      "\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\fCertified Tester \n",
      "Foundation Level \n",
      "\n",
      "they use the same functional tests, but check that while performing the function, a non-functional \n",
      "constraint is satisfied (e.g., checking that a function performs within a specified time, or a function can be \n",
      "ported to a new platform). The late discovery of non-functional defects can pose a serious threat to the \n",
      "success of a project. Non-functional testing sometimes needs a very specific test environment, such as a \n",
      "usability lab for usability testing. \n",
      "\n",
      "Black-box testing (see section 4.2) is specification-based and derives tests from documentation external \n",
      "to the test object. The main objective of black-box testing is checking the system's behavior against its \n",
      "specifications. \n",
      "\n",
      "White-box testing (see section 4.3) is structure-based and derives tests from the system's \n",
      "implementation or internal structure (e.g., code, architecture, work flows, and data flows). The main \n",
      "objective of white-box testing is to cover the underlying structure by the tests to the acceptable level. \n",
      "\n",
      "All the four above mentioned test types can be applied to all test levels, although the focus will be \n",
      "different at each level. Different test techniques can be used to derive test conditions and test cases for \n",
      "all the mentioned test types.  \n",
      "\n",
      "2.2.3.  Confirmation Testing and Regression Testing \n",
      "\n",
      "Changes are typically made to a component or system to either enhance it by adding a new feature or to \n",
      "fix it by removing a defect. Testing should then also include confirmation testing and regression testing. \n",
      "\n",
      "Confirmation testing confirms that an original defect has been successfully fixed. Depending on the risk, \n",
      "one can test the fixed version of the software in several ways, including: \n",
      "\n",
      "•  executing all test cases that previously have failed due to the defect, or, also by \n",
      "\n",
      "•  adding new tests to cover any changes that were needed to fix the defect \n",
      "\n",
      "However, when time or money is short when fixing defects, confirmation testing might be restricted to \n",
      "simply exercising the steps that should reproduce the failure caused by the defect and checking that the \n",
      "failure does not occur. \n",
      "\n",
      "Regression testing confirms that no adverse consequences have been caused by a change, including a \n",
      "fix that has already been confirmation tested. These adverse consequences could affect the same \n",
      "component where the change was made, other components in the same system, or even other \n",
      "connected systems. Regression testing may not be restricted to the test object itself but can also be \n",
      "related to the environment. It is advisable first to perform an impact analysis to optimize the extent of the \n",
      "regression testing. Impact analysis shows which parts of the software could be affected. \n",
      "\n",
      "Regression test suites are run many times and generally the number of regression test cases will \n",
      "increase with each iteration or release, so regression testing is a strong candidate for automation. \n",
      "Automation of these tests should start early in the project. Where CI is used, such as in DevOps (see \n",
      "section 2.1.4), it is good practice to also include automated regression tests. Depending on the situation, \n",
      "this may include regression tests on different levels. \n",
      "\n",
      "Confirmation testing and/or regression testing for the test object are needed on all test levels if defects \n",
      "are fixed and/or changes are made on these test levels. \n",
      "\n",
      "v4.0 \n",
      "\n",
      "Page 29 of 74 \n",
      "\n",
      "2023-04-21 \n",
      "\n",
      "© International Software Testing Qualifications Board \n",
      "\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\fCertified Tester \n",
      "Foundation Level \n",
      "\n",
      "2.3. Maintenance Testing \n",
      "\n",
      "There are different categories of maintenance, it can be corrective, adaptive to changes in the \n",
      "environment or improve performance or maintainability (see ISO/IEC 14764 for details), so maintenance \n",
      "can involve planned releases/deployments and unplanned releases/deployments (hot fixes). Impact \n",
      "analysis may be done before a change is made, to help decide if the change should be made, based on \n",
      "the potential consequences in other areas of the system. Testing the changes to a system in production \n",
      "includes both evaluating the success of the implementation of the change and the checking for possible \n",
      "regressions in parts of the system that remain unchanged (which is usually most of the system).  \n",
      "\n",
      "The scope of maintenance testing typically depends on: \n",
      "\n",
      "•  The degree of risk of the change \n",
      "\n",
      "•  The size of the existing system \n",
      "\n",
      "•  The size of the change \n",
      "\n",
      "The triggers for maintenance and maintenance testing can be classified as follows: \n",
      "\n",
      "•  Modifications, such as planned enhancements (i.e., release-based), corrective changes or hot \n",
      "\n",
      "fixes. \n",
      "\n",
      "•  Upgrades or migrations of the operational environment, such as from one platform to another, \n",
      "\n",
      "which can require tests associated with the new environment as well as of the changed software, \n",
      "or tests of data conversion when data from another application is migrated into the system being \n",
      "maintained. \n",
      "\n",
      "•  Retirement, such as when an application reaches the end of its life. When a system is retired, this \n",
      "can require testing of data archiving if long data-retention periods are required. Testing of restore \n",
      "and retrieval procedures after archiving may also be needed in the event that certain data is \n",
      "required during the archiving period. \n",
      "\n",
      "v4.0 \n",
      "\n",
      "Page 30 of 74 \n",
      "\n",
      "2023-04-21 \n",
      "\n",
      "© International Software Testing Qualifications Board \n",
      "\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\fCertified Tester \n",
      "Foundation Level \n",
      "\n",
      "3. Static Testing – 80 minutes \n",
      "\n",
      "Keywords \n",
      "\n",
      "anomaly, dynamic testing, formal review, informal review, inspection, review, static analysis, static testing, \n",
      "technical review, walkthrough \n",
      "\n",
      "Learning Objectives for Chapter 3: \n",
      "\n",
      "3.1  Static Testing Basics \n",
      "\n",
      "FL-3.1.1 \n",
      "\n",
      "(K1) Recognize types of products that can be examined by the different static test techniques  \n",
      "\n",
      "FL-3.1.2 \n",
      "\n",
      "(K2) Explain the value of static testing  \n",
      "\n",
      "FL-3.1.3 \n",
      "\n",
      "(K2) Compare and contrast static and dynamic testing \n",
      "\n",
      "3.2  Feedback and Review Process  \n",
      "\n",
      "FL-3.2.1 \n",
      "\n",
      "(K1) Identify the benefits of early and frequent stakeholder feedback \n",
      "\n",
      "FL-3.2.2 \n",
      "\n",
      "(K2) Summarize the activities of the review process  \n",
      "\n",
      "FL-3.2.3 \n",
      "\n",
      "(K1) Recall which responsibilities are assigned to the principal roles when performing reviews \n",
      "\n",
      "FL-3.2.4 \n",
      "\n",
      "(K2) Compare and contrast the different review types \n",
      "\n",
      "FL-3.2.5 \n",
      "\n",
      "(K1) Recall the factors that contribute to a successful review \n",
      "\n",
      "v4.0 \n",
      "\n",
      "Page 31 of 74 \n",
      "\n",
      "2023-04-21 \n",
      "\n",
      "© International Software Testing Qualifications Board \n",
      "\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\fCertified Tester \n",
      "Foundation Level \n",
      "\n",
      "3.1. Static Testing Basics \n",
      "\n",
      "In contrast to dynamic testing, in static testing the software under test does not need to be executed. \n",
      "Code, process specification, system architecture specification or other work products are evaluated \n",
      "through manual examination (e.g., reviews) or with the help of a tool (e.g., static analysis). Test objectives \n",
      "include improving quality, detecting defects and assessing characteristics like readability, completeness, \n",
      "correctness, testability and consistency. Static testing can be applied for both verification and validation. \n",
      "\n",
      "Testers, business representatives and developers work together during example mappings, collaborative \n",
      "user story writing and backlog refinement sessions to ensure that user stories and related work products \n",
      "meet defined criteria, e.g., the Definition of Ready (see section 5.1.3). Review techniques can be applied \n",
      "to ensure user stories are complete and understandable and include testable acceptance criteria. By \n",
      "asking the right questions, testers explore, challenge and help improve the proposed user stories.  \n",
      "\n",
      "Static analysis can identify problems prior to dynamic testing while often requiring less effort, since no test \n",
      "cases are required, and tools (see chapter 6) are typically used. Static analysis is often incorporated into \n",
      "CI frameworks (see section 2.1.4). While largely used to detect specific code defects, static analysis is \n",
      "also used to evaluate maintainability and security. Spelling checkers and readability tools are other \n",
      "examples of static analysis tools. \n",
      "\n",
      "3.1.1.  Work Products Examinable by Static Testing \n",
      "\n",
      "Almost any work product can be examined using static testing. Examples include requirement \n",
      "specification documents, source code, test plans, test cases, product backlog items, test charters, project \n",
      "documentation, contracts and models. \n",
      "\n",
      "Any work product that can be read and understood can be the subject of a review. However, for static \n",
      "analysis, work products need a structure against which they can be checked (e.g., models, code or text \n",
      "with a formal syntax).  \n",
      "\n",
      "Work products that are not appropriate for static testing include those that are difficult to interpret by \n",
      "human beings and that should not be analyzed by tools (e.g., 3rd party executable code due to legal \n",
      "reasons). \n",
      "\n",
      "3.1.2.  Value of Static Testing \n",
      "\n",
      "Static testing can detect defects in the earliest phases of the SDLC, fulfilling the principle of early testing \n",
      "(see section 1.3). It can also identify defects which cannot be detected by dynamic testing (e.g., \n",
      "unreachable code, design patterns not implemented as desired, defects in non-executable work \n",
      "products).  \n",
      "\n",
      "Static testing provides the ability to evaluate the quality of, and to build confidence in work products. By \n",
      "verifying the documented requirements, the stakeholders can also make sure that these requirements \n",
      "describe their actual needs. Since static testing can be performed early in the SDLC, a shared \n",
      "understanding can be created among the involved stakeholders. Communication will also be improved \n",
      "between the involved stakeholders. For this reason, it is recommended to involve a wide variety of \n",
      "stakeholders in static testing.  \n",
      "\n",
      "Even though reviews can be costly to implement, the overall project costs are usually much lower than \n",
      "when no reviews are performed because less time and effort needs to be spent on fixing defects later in \n",
      "the project.  \n",
      "\n",
      "v4.0 \n",
      "\n",
      "Page 32 of 74 \n",
      "\n",
      "2023-04-21 \n",
      "\n",
      "© International Software Testing Qualifications Board \n",
      "\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\fCertified Tester \n",
      "Foundation Level \n",
      "\n",
      "Code defects can be detected using static analysis more efficiently than in dynamic testing, usually \n",
      "resulting in both fewer code defects and a lower overall development effort. \n",
      "\n",
      "3.1.3.  Differences between Static Testing and Dynamic Testing \n",
      "\n",
      "Static testing and dynamic testing practices complement each other. They have similar objectives, such \n",
      "as supporting the detection of defects in work products (see section 1.1.1), but there are also some \n",
      "differences, such as: \n",
      "\n",
      "•  Static and dynamic testing (with analysis of failures) can both lead to the detection of defects, \n",
      "\n",
      "however there are some defect types that can only be found by either static or dynamic testing.  \n",
      "\n",
      "•  Static testing finds defects directly, while dynamic testing causes failures from which the \n",
      "\n",
      "associated defects are determined through subsequent analysis \n",
      "\n",
      "•  Static testing may more easily detect defects that lay on paths through the code that are rarely \n",
      "\n",
      "executed or hard to reach using dynamic testing \n",
      "\n",
      "•  Static testing can be applied to non-executable work products, while dynamic testing can only be \n",
      "\n",
      "applied to executable work products \n",
      "\n",
      "•  Static testing can be used to measure quality characteristics that are not dependent on executing \n",
      "code (e.g., maintainability), while dynamic testing can be used to measure quality characteristics  \n",
      "that are dependent on executing code (e.g., performance efficiency) \n",
      "\n",
      "Typical defects that are easier and/or cheaper to find through static testing include: \n",
      "\n",
      "•  Defects in requirements (e.g., inconsistencies, ambiguities, contradictions, omissions, \n",
      "\n",
      "inaccuracies, duplications) \n",
      "\n",
      "•  Design defects (e.g., inefficient database structures, poor modularization) \n",
      "\n",
      "•  Certain types of coding defects (e.g., variables with undefined values, undeclared variables, \n",
      "\n",
      "unreachable or duplicated code, excessive code complexity) \n",
      "\n",
      "•  Deviations from standards (e.g., lack of adherence to naming conventions in coding standards) \n",
      "\n",
      "• \n",
      "\n",
      "Incorrect interface specifications (e.g., mismatched number, type or order of parameters) \n",
      "\n",
      "•  Specific types of security vulnerabilities (e.g., buffer overflows) \n",
      "\n",
      "•  Gaps or inaccuracies in test basis coverage (e.g., missing tests for an acceptance criterion) \n",
      "\n",
      "3.2. Feedback and Review Process \n",
      "\n",
      "3.2.1.  Benefits of Early and Frequent Stakeholder Feedback \n",
      "\n",
      "Early and frequent feedback allows for the early communication of potential quality problems. If there is \n",
      "little stakeholder involvement during the SDLC, the product being developed might not meet the \n",
      "stakeholder’s original or current vision. A failure to deliver what the stakeholder wants can result in costly \n",
      "rework, missed deadlines, blame games, and might even lead to complete project failure.  \n",
      "\n",
      "v4.0 \n",
      "\n",
      "Page 33 of 74 \n",
      "\n",
      "2023-04-21 \n",
      "\n",
      "© International Software Testing Qualifications Board \n",
      "\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\fCertified Tester \n",
      "Foundation Level \n",
      "\n",
      "Frequent stakeholder feedback throughout the SDLC can prevent misunderstandings about requirements \n",
      "and ensure that changes to requirements are understood and implemented earlier. This helps the \n",
      "development team to improve their understanding of what they are building. It allows them to focus on \n",
      "those features that deliver the most value to the stakeholders and that have the most positive impact on \n",
      "identified risks. \n",
      "\n",
      "3.2.2.  Review Process Activities \n",
      "\n",
      "The ISO/IEC 20246 standard defines a generic review process that provides a structured but flexible \n",
      "framework from which a specific review process may be tailored to a particular situation. If the required \n",
      "review is more formal, then more of the tasks described for the different activities will be needed.  \n",
      "\n",
      "The size of many work products makes them too large to be covered by a single review. The review \n",
      "process may be invoked a couple of times to complete the review for the entire work product.  \n",
      "\n",
      "The activities in the review process are: \n",
      "\n",
      "•  Planning. During the planning phase, the scope of the review, which comprises the purpose, the \n",
      "work product to be reviewed, quality characteristics to be evaluated, areas to focus on, exit \n",
      "criteria, supporting information such as standards, effort and the timeframes for the review, shall \n",
      "be defined. \n",
      "\n",
      "•  Review initiation. During review initiation, the goal is to make sure that everyone and everything \n",
      "involved is prepared to start the review. This includes making sure that every participant has \n",
      "access to the work product under review, understands their role and responsibilities and receives \n",
      "everything needed to perform the review. \n",
      "\n",
      "• \n",
      "\n",
      "Individual review. Every reviewer performs an individual review to assess the quality of the work \n",
      "product under review, and to identify anomalies, recommendations, and questions by applying \n",
      "one or more review techniques (e.g., checklist-based reviewing, scenario-based reviewing). The \n",
      "ISO/IEC 20246 standard provides more depth on different review techniques. The reviewers log \n",
      "all their identified anomalies, recommendations, and questions.  \n",
      "\n",
      "•  Communication and analysis. Since the anomalies identified during a review are not \n",
      "\n",
      "necessarily defects, all these anomalies need to be analyzed and discussed. For every anomaly, \n",
      "the decision should be made on its status, ownership and required actions. This is typically done \n",
      "in a review meeting, during which the participants also decide what the quality level of reviewed \n",
      "work product is and what follow-up actions are required. A follow-up review may be required to \n",
      "complete actions.  \n",
      "\n",
      "•  Fixing and reporting. For every defect, a defect report should be created so that corrective \n",
      "\n",
      "actions can be followed-up. Once the exit criteria are reached, the work product can be accepted. \n",
      "The review results are reported. \n",
      "\n",
      "3.2.3.  Roles and Responsibilities in Reviews \n",
      "\n",
      "Reviews involve various stakeholders, who may take on several roles. The principal roles and their \n",
      "responsibilities are: \n",
      "\n",
      "•  Manager – decides what is to be reviewed and provides resources, such as staff and time for the \n",
      "\n",
      "review \n",
      "\n",
      "•  Author – creates and fixes the work product under review \n",
      "\n",
      "v4.0 \n",
      "\n",
      "Page 34 of 74 \n",
      "\n",
      "2023-04-21 \n",
      "\n",
      "© International Software Testing Qualifications Board \n",
      "\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\fCertified Tester \n",
      "Foundation Level \n",
      "\n",
      "•  Moderator (also known as the facilitator) – ensures the effective running of review meetings, \n",
      "\n",
      "including mediation, time management, and a safe review environment in which everyone can \n",
      "speak freely \n",
      "\n",
      "•  Scribe (also known as recorder) – collates anomalies from reviewers and records review \n",
      "information, such as decisions and new anomalies found during the review meeting \n",
      "\n",
      "•  Reviewer – performs reviews. A reviewer may be someone working on the project, a subject \n",
      "\n",
      "matter expert, or any other stakeholder \n",
      "\n",
      "•  Review leader – takes overall responsibility for the review such as deciding who will be involved, \n",
      "\n",
      "and organizing when and where the review will take place \n",
      "\n",
      "Other, more detailed roles are possible, as described in the ISO/IEC 20246 standard. \n",
      "\n",
      "3.2.4.  Review Types \n",
      "\n",
      "There exist many review types ranging from informal reviews to formal reviews. The required level of \n",
      "formality depends on factors such as the SDLC being followed, the maturity of the development process, \n",
      "the criticality and complexity of the work product being reviewed, legal or regulatory requirements, and \n",
      "the need for an audit trail. The same work product can be reviewed with different review types, e.g., first \n",
      "an informal one and later a more formal one. \n",
      "\n",
      "Selecting the right review type is key to achieving the required review objectives (see section 3.2.5). The \n",
      "selection is not only based on the objectives, but also on factors such as the project needs, available \n",
      "resources, work product type and risks, business domain, and company culture.  \n",
      "\n",
      "Some commonly used review types are: \n",
      "\n",
      "• \n",
      "\n",
      "Informal review. Informal reviews do not follow a defined process and do not require a formal \n",
      "documented output. The main objective is detecting anomalies. \n",
      "\n",
      "•  Walkthrough. A walkthrough, which is led by the author, can serve many objectives, such as \n",
      "\n",
      "evaluating quality and building confidence in the work product, educating reviewers, gaining \n",
      "consensus, generating new ideas, motivating and enabling authors to improve and detecting \n",
      "anomalies. Reviewers might perform an individual review before the walkthrough, but this is not \n",
      "required. \n",
      "\n",
      "•  Technical Review. A technical review is performed by technically qualified reviewers and led by \n",
      "a moderator. The objectives of a technical review are to gain consensus and make decisions \n",
      "regarding a technical problem, but also to detect anomalies, evaluate quality and build confidence \n",
      "in the work product, generate new ideas, and to motivate and enable authors to improve. \n",
      "\n",
      "• \n",
      "\n",
      "Inspection. As inspections are the most formal type of review, they follow the complete generic \n",
      "process (see section 3.2.2). The main objective is to find the maximum number of anomalies. \n",
      "Other objectives are to evaluate quality, build confidence in the work product, and to motivate and \n",
      "enable authors to improve. Metrics are collected and used to improve the SDLC, including the \n",
      "inspection process. In inspections, the author cannot act as the review leader or scribe. \n",
      "\n",
      "3.2.5.  Success Factors for Reviews \n",
      "\n",
      "There are several factors that determine the success of reviews, which include: \n",
      "\n",
      "v4.0 \n",
      "\n",
      "Page 35 of 74 \n",
      "\n",
      "2023-04-21 \n",
      "\n",
      "© International Software Testing Qualifications Board \n",
      "\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\fCertified Tester \n",
      "Foundation Level \n",
      "\n",
      "•  Defining clear objectives and measurable exit criteria. Evaluation of participants should never be \n",
      "\n",
      "an objective \n",
      "\n",
      "•  Choosing the appropriate review type to achieve the given objectives, and to suit the type of work \n",
      "\n",
      "product, the review participants, the project needs and context \n",
      "\n",
      "•  Conducting reviews on small chunks, so that reviewers do not lose concentration during an \n",
      "\n",
      "individual review and/or the review meeting (when held)  \n",
      "\n",
      "•  Providing feedback from reviews to stakeholders and authors so they can improve the product \n",
      "\n",
      "and their activities (see section 3.2.1) \n",
      "\n",
      "•  Providing adequate time to participants to prepare for the review \n",
      "\n",
      "•  Support from management for the review process \n",
      "\n",
      "•  Making reviews part of the organization’s culture, to promote learning and process improvement \n",
      "\n",
      "•  Providing adequate training for all participants so they know how to fulfil their role \n",
      "\n",
      "•  Facilitating meetings \n",
      "\n",
      "v4.0 \n",
      "\n",
      "Page 36 of 74 \n",
      "\n",
      "2023-04-21 \n",
      "\n",
      "© International Software Testing Qualifications Board \n",
      "\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\fCertified Tester \n",
      "Foundation Level \n",
      "\n",
      "4. Test Analysis and Design – 390 minutes \n",
      "\n",
      "Keywords \n",
      "\n",
      "acceptance criteria, acceptance test-driven development, black-box test technique, boundary value \n",
      "analysis, branch coverage, checklist-based testing, collaboration-based test approach, coverage, \n",
      "coverage item, decision table testing, equivalence partitioning, error guessing, experience-based test \n",
      "technique, exploratory testing, state transition testing, statement coverage, test technique, white-box test \n",
      "technique \n",
      "\n",
      "Learning Objectives for Chapter 4: \n",
      "\n",
      "4.1  Test Techniques Overview \n",
      "\n",
      "FL-4.1.1 \n",
      "\n",
      "(K2) Distinguish black-box, white-box and experience-based test techniques \n",
      "\n",
      "4.2  Black-box Test Techniques \n",
      "\n",
      "FL-4.2.1 \n",
      "\n",
      "(K3) Use equivalence partitioning to derive test cases \n",
      "\n",
      "FL-4.2.2 \n",
      "\n",
      "(K3) Use boundary value analysis to derive test cases \n",
      "\n",
      "FL-4.2.3 \n",
      "\n",
      "(K3) Use decision table testing to derive test cases \n",
      "\n",
      "FL-4.2.4 \n",
      "\n",
      "(K3) Use state transition testing to derive test cases \n",
      "\n",
      "4.3  White-box Test Techniques \n",
      "\n",
      "FL-4.3.1 \n",
      "\n",
      "(K2) Explain statement testing \n",
      "\n",
      "FL-4.3.2 \n",
      "\n",
      "(K2) Explain branch testing \n",
      "\n",
      "FL-4.3.3 \n",
      "\n",
      "(K2) Explain the value of white-box testing \n",
      "\n",
      "4.4  Experience-based Test Techniques \n",
      "\n",
      "FL-4.4.1 \n",
      "\n",
      "(K2) Explain error guessing \n",
      "\n",
      "FL-4.4.2 \n",
      "\n",
      "(K2) Explain exploratory testing \n",
      "\n",
      "FL-4.4.3 \n",
      "\n",
      "(K2) Explain checklist-based testing \n",
      "\n",
      "4.5. Collaboration-based Test Approaches \n",
      "\n",
      "FL-4.5.1 \n",
      "\n",
      "(K2) Explain how to write user stories in collaboration with developers and business \n",
      "representatives \n",
      "\n",
      "FL-4.5.2 \n",
      "\n",
      "(K2) Classify the different options for writing acceptance criteria \n",
      "\n",
      "FL-4.5.3 \n",
      "\n",
      "(K3) Use acceptance test-driven development (ATDD) to derive test cases \n",
      "\n",
      "v4.0 \n",
      "\n",
      "Page 37 of 74 \n",
      "\n",
      "2023-04-21 \n",
      "\n",
      "© International Software Testing Qualifications Board \n",
      "\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\fCertified Tester \n",
      "Foundation Level \n",
      "\n",
      "4.1. Test Techniques Overview \n",
      "\n",
      "Test techniques support the tester in test analysis (what to test) and in test design (how to test). Test \n",
      "techniques help to develop a relatively small, but sufficient, set of test cases in a systematic way. Test \n",
      "techniques also help the tester to define test conditions, identify coverage items, and identify test data \n",
      "during the test analysis and design. Further information on test techniques and their corresponding \n",
      "measures can be found in the ISO/IEC/IEEE 29119-4 standard, and in (Beizer 1990, Craig 2002, \n",
      "Copeland 2004, Koomen 2006, Jorgensen 2014, Ammann 2016, Forgács 2019). \n",
      "\n",
      "In this syllabus, test techniques are classified as black-box, white-box, and experience-based. \n",
      "\n",
      "Black-box test techniques (also known as specification-based techniques) are based on an analysis of \n",
      "the specified behavior of the test object without reference to its internal structure. Therefore, the test \n",
      "cases are independent of how the software is implemented. Consequently, if the implementation \n",
      "changes, but the required behavior stays the same, then the test cases are still useful.  \n",
      "\n",
      "White-box test techniques (also known as structure-based techniques) are based on an analysis of the \n",
      "test object’s internal structure and processing. As the test cases are dependent on how the software is \n",
      "designed, they can only be created after the design or implementation of the test object.  \n",
      "\n",
      "Experience-based test techniques effectively use the knowledge and experience of testers for the \n",
      "design and implementation of test cases. The effectiveness of these techniques depends heavily on the \n",
      "tester’s skills. Experience-based test techniques can detect defects that may be missed using the black-\n",
      "box and white-box test techniques. Hence, experience-based test techniques are complementary to the \n",
      "black-box and white-box test techniques. \n",
      "\n",
      "4.2. Black-Box Test Techniques \n",
      "\n",
      "Commonly used black-box test techniques discussed in the following sections are: \n",
      "\n",
      "•  Equivalence Partitioning \n",
      "\n",
      "•  Boundary Value Analysis \n",
      "\n",
      "•  Decision Table Testing \n",
      "\n",
      "•  State Transition Testing \n",
      "\n",
      "4.2.1.  Equivalence Partitioning \n",
      "\n",
      "Equivalence Partitioning (EP) divides data into partitions (known as equivalence partitions) based on the \n",
      "expectation that all the elements of a given partition are to be processed in the same way by the test \n",
      "object. The theory behind this technique is that if a test case, that tests one value from an equivalence \n",
      "partition, detects a defect, this defect should also be detected by test cases that test any other value from \n",
      "the same partition. Therefore, one test for each partition is sufficient.  \n",
      "\n",
      "Equivalence partitions can be identified for any data element related to the test object, including inputs, \n",
      "outputs, configuration items, internal values, time-related values, and interface parameters. The partitions \n",
      "may be continuous or discrete, ordered or unordered, finite or infinite. The partitions must not overlap and \n",
      "must be non-empty sets. \n",
      "\n",
      "For simple test objects EP can be easy, but in practice, understanding how the test object will treat \n",
      "different values is often complicated. Therefore, partitioning should be done with care. \n",
      "\n",
      "v4.0 \n",
      "\n",
      "Page 38 of 74 \n",
      "\n",
      "2023-04-21 \n",
      "\n",
      "© International Software Testing Qualifications Board \n",
      "\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\fCertified Tester \n",
      "Foundation Level \n",
      "\n",
      "A partition containing valid values is called a valid partition. A partition containing invalid values is called \n",
      "an invalid partition. The definitions of valid and invalid values may vary among teams and organizations. \n",
      "For example, valid values may be interpreted as those that should be processed by the test object or as \n",
      "those for which the specification defines their processing. Invalid values may be interpreted as those that \n",
      "should be ignored or rejected by the test object or as those for which no processing is defined in the test \n",
      "object specification. \n",
      "\n",
      "In EP, the coverage items are the equivalence partitions. To achieve 100% coverage with this technique, \n",
      "test cases must exercise all identified partitions (including invalid partitions) by covering each partition at \n",
      "least once. Coverage is measured as the number of partitions exercised by at least one test case, divided \n",
      "by the total number of identified partitions, and is expressed as a percentage. \n",
      "\n",
      "Many test objects include multiple sets of partitions (e.g., test objects with more than one input \n",
      "parameter), which means that a test case will cover partitions from different sets of partitions. The \n",
      "simplest coverage criterion in the case of multiple sets of partitions is called Each Choice coverage \n",
      "(Ammann 2016). Each Choice coverage requires test cases to exercise each partition from each set of \n",
      "partitions at least once. Each Choice coverage does not take into account combinations of partitions. \n",
      "\n",
      "4.2.2.  Boundary Value Analysis \n",
      "\n",
      "Boundary Value Analysis (BVA) is a technique based on exercising the boundaries of equivalence \n",
      "partitions. Therefore, BVA can only be used for ordered partitions. The minimum and maximum values of \n",
      "a partition are its boundary values. In the case of BVA, if two elements belong to the same partition, all \n",
      "elements between them must also belong to that partition. \n",
      "\n",
      "BVA focuses on the boundary values of the partitions because developers are more likely to make errors \n",
      "with these boundary values. Typical defects found by BVA are located where implemented boundaries \n",
      "are misplaced to positions above or below their intended positions or are omitted altogether.  \n",
      "\n",
      "This syllabus covers two versions of the BVA: 2-value and 3-value BVA. They differ in terms of coverage \n",
      "items per boundary that need to be exercised to achieve 100% coverage. \n",
      "\n",
      "In 2-value BVA (Craig 2002, Myers 2011), for each boundary value there are two coverage items: this \n",
      "boundary value and its closest neighbor belonging to the adjacent partition. To achieve 100% coverage \n",
      "with 2-value BVA, test cases must exercise all coverage items, i.e., all identified boundary values. \n",
      "Coverage is measured as the number of boundary values that were exercised, divided by the total \n",
      "number of identified boundary values, and is expressed as a percentage. \n",
      "\n",
      "In 3-value BVA (Koomen 2006, O’Regan 2019), for each boundary value there are three coverage items: \n",
      "this boundary value and both its neighbors. Therefore, in 3-value BVA some of the coverage items may \n",
      "not be boundary values. To achieve 100% coverage with 3-value BVA, test cases must exercise all \n",
      "coverage items, i.e., identified boundary values and their neighbors. Coverage is measured as the \n",
      "number of boundary values and their neighbors exercised, divided by the total number of identified \n",
      "boundary values and their neighbors, and is expressed as a percentage. \n",
      "\n",
      "3-value BVA is more rigorous than 2-value BVA as it may detect defects overlooked by 2-value BVA. For \n",
      "example, if the decision “if (x ≤ 10) …” is incorrectly implemented as “if (x = 10) …”, no test data derived \n",
      "from the 2-value BVA (x = 10, x = 11) can detect the defect. However, x = 9, derived from the 3-value \n",
      "BVA, is likely to detect it. \n",
      "\n",
      "v4.0 \n",
      "\n",
      "Page 39 of 74 \n",
      "\n",
      "2023-04-21 \n",
      "\n",
      "© International Software Testing Qualifications Board \n",
      "\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\fCertified Tester \n",
      "Foundation Level \n",
      "\n",
      "4.2.3.  Decision Table Testing  \n",
      "\n",
      "Decision tables are used for testing the implementation of system requirements that specify how different \n",
      "combinations of conditions result in different outcomes. Decision tables are an effective way of recording \n",
      "complex logic, such as business rules. \n",
      "\n",
      "When creating decision tables, the conditions and the resulting actions of the system are defined. These \n",
      "form the rows of the table. Each column corresponds to a decision rule that defines a unique combination \n",
      "of conditions, along with the associated actions. In limited-entry decision tables all the values of the \n",
      "conditions and actions (except for irrelevant or infeasible ones; see below) are shown as Boolean values \n",
      "(true or false). Alternatively, in extended-entry decision tables some or all the conditions and actions may \n",
      "also take on multiple values (e.g., ranges of numbers, equivalence partitions, discrete values). \n",
      "\n",
      "The notation for conditions is as follows: “T” (true) means that the condition is satisfied. “F” (false) means \n",
      "that the condition is not satisfied. “–” means that the value of the condition is irrelevant for the action \n",
      "outcome. “N/A” means that the condition is infeasible for a given rule. For actions: “X” means that the \n",
      "action should occur. Blank means that the action should not occur. Other notations may also be used. \n",
      "\n",
      "A full decision table has enough columns to cover every combination of conditions. The table can be \n",
      "simplified by deleting columns containing infeasible combinations of conditions. The table can also be \n",
      "minimized by merging columns, in which some conditions do not affect the outcome, into a single column. \n",
      "Decision table minimization algorithms are out of scope of this syllabus. \n",
      "\n",
      "In decision table testing, the coverage items are the columns containing feasible combinations of \n",
      "conditions. To achieve 100% coverage with this technique, test cases must exercise all these columns. \n",
      "Coverage is measured as the number of exercised columns, divided by the total number of feasible \n",
      "columns, and is expressed as a percentage. \n",
      "\n",
      "The strength of decision table testing is that it provides a systematic approach to identify all the \n",
      "combinations of conditions, some of which might otherwise be overlooked. It also helps to find any gaps \n",
      "or contradictions in the requirements. If there are many conditions, exercising all the decision rules may \n",
      "be time consuming, since the number of rules grows exponentially with the number of conditions. In such \n",
      "a case, to reduce the number of rules that need to be exercised, a minimized decision table or a risk-\n",
      "based approach may be used. \n",
      "\n",
      "4.2.4.  State Transition Testing \n",
      "\n",
      "A state transition diagram models the behavior of a system by showing its possible states and valid state \n",
      "transitions. A transition is initiated by an event, which may be additionally qualified by a guard condition. \n",
      "The transitions are assumed to be instantaneous and may sometimes result in the software taking action. \n",
      "The common transition labeling syntax is as follows: “event [guard condition] / action”. Guard conditions \n",
      "and actions can be omitted if they do not exist or are irrelevant for the tester.  \n",
      "\n",
      "A state table is a model equivalent to a state transition diagram. Its rows represent states, and its \n",
      "columns represent events (together with guard conditions if they exist). Table entries (cells) represent \n",
      "transitions, and contain the target state, as well as the resulting actions, if defined. In contrast to the state \n",
      "transition diagram, the state table explicitly shows invalid transitions, which are represented by empty \n",
      "cells. \n",
      "\n",
      "A test case based on a state transition diagram or state table is usually represented as a sequence of \n",
      "events, which results in a sequence of state changes (and actions, if needed). One test case may, and \n",
      "usually will, cover several transitions between states.  \n",
      "\n",
      "There exist many coverage criteria for state transition testing. This syllabus discusses three of them. \n",
      "\n",
      "v4.0 \n",
      "\n",
      "Page 40 of 74 \n",
      "\n",
      "2023-04-21 \n",
      "\n",
      "© International Software Testing Qualifications Board \n",
      "\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\fCertified Tester \n",
      "Foundation Level \n",
      "\n",
      "In all states coverage, the coverage items are the states. To achieve 100% all states coverage, test \n",
      "cases must ensure that all the states are visited. Coverage is measured as the number of visited states \n",
      "divided by the total number of states, and is expressed as a percentage. \n",
      "\n",
      "In valid transitions coverage (also called 0-switch coverage), the coverage items are single valid \n",
      "transitions. To achieve 100% valid transitions coverage, test cases must exercise all the valid transitions. \n",
      "Coverage is measured as the number of exercised valid transitions divided by the total number of valid \n",
      "transitions, and is expressed as a percentage.  \n",
      "\n",
      "In all transitions coverage, the coverage items are all the transitions shown in a state table. To achieve \n",
      "100% all transitions coverage, test cases must exercise all the valid transitions and attempt to execute \n",
      "invalid transitions. Testing only one invalid transition in a single test case helps to avoid fault masking, \n",
      "i.e., a situation in which one defect prevents the detection of another. Coverage is measured as the \n",
      "number of valid and invalid transitions exercised or attempted to be covered by executed test cases, \n",
      "divided by the total number of valid and invalid transitions, and is expressed as a percentage. \n",
      "\n",
      "All states coverage is weaker than valid transitions coverage, because it can typically be achieved without \n",
      "exercising all the transitions. Valid transitions coverage is the most widely used coverage criterion. \n",
      "Achieving full valid transitions coverage guarantees full all states coverage. Achieving full all transitions \n",
      "coverage guarantees both full all states coverage and full valid transitions coverage and should be a \n",
      "minimum requirement for mission and safety-critical software.  \n",
      "\n",
      "4.3. White-Box Test Techniques \n",
      "\n",
      "Because of their popularity and simplicity, this section focuses on two code-related white-box test \n",
      "techniques: \n",
      "\n",
      "•  Statement testing \n",
      "\n",
      "•  Branch testing \n",
      "\n",
      "There are more rigorous techniques that are used in some safety-critical, mission-critical, or high-integrity \n",
      "environments to achieve more thorough code coverage. There are also white-box test techniques used in \n",
      "higher test levels (e.g., API testing), or using coverage not related to code (e.g., neuron coverage in \n",
      "neural network testing). These techniques are not discussed in this syllabus. \n",
      "\n",
      "4.3.1.  Statement Testing and Statement Coverage  \n",
      "\n",
      "In statement testing, the coverage items are executable statements. The aim is to design test cases that \n",
      "exercise statements in the code until an acceptable level of coverage is achieved. Coverage is measured \n",
      "as the number of statements exercised by the test cases divided by the total number of executable \n",
      "statements in the code, and is expressed as a percentage. \n",
      "\n",
      "When 100% statement coverage is achieved, it ensures that all executable statements in the code have \n",
      "been exercised at least once. In particular, this means that each statement with a defect will be executed, \n",
      "which may cause a failure demonstrating the presence of the defect. However, exercising a statement \n",
      "with a test case will not detect defects in all cases. For example, it may not detect defects that are data \n",
      "dependent (e.g., a division by zero that only fails when a denominator is set to zero). Also, 100% \n",
      "statement coverage does not ensure that all the decision logic has been tested as, for instance, it may not \n",
      "exercise all the branches (see chapter 4.3.2) in the code.  \n",
      "\n",
      "v4.0 \n",
      "\n",
      "Page 41 of 74 \n",
      "\n",
      "2023-04-21 \n",
      "\n",
      "© International Software Testing Qualifications Board \n",
      "\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\fCertified Tester \n",
      "Foundation Level \n",
      "\n",
      "4.3.2.  Branch Testing and Branch Coverage  \n",
      "\n",
      "A branch is a transfer of control between two nodes in the control flow graph, which shows the possible \n",
      "sequences in which source code statements are executed in the test object. Each transfer of control can \n",
      "be either unconditional (i.e., straight-line code) or conditional (i.e., a decision outcome). \n",
      "\n",
      "In branch testing the coverage items are branches and the aim is to design test cases to exercise \n",
      "branches in the code until an acceptable level of coverage is achieved. Coverage is measured as the \n",
      "number of branches exercised by the test cases divided by the total number of branches, and is \n",
      "expressed as a percentage. \n",
      "\n",
      "When 100% branch coverage is achieved, all branches in the code, unconditional and conditional, are \n",
      "exercised by test cases. Conditional branches typically correspond to a true or false outcome from an \n",
      "“if...then” decision, an outcome from a switch/case statement, or a decision to exit or continue in a loop. \n",
      "However, exercising a branch with a test case will not detect defects in all cases. For example, it may not \n",
      "detect defects requiring the execution of a specific path in a code. \n",
      "\n",
      "Branch coverage subsumes statement coverage. This means that any set of test cases achieving 100% \n",
      "branch coverage also achieves 100% statement coverage (but not vice versa). \n",
      "\n",
      "4.3.3.  The Value of White-box Testing  \n",
      "\n",
      "A fundamental strength that all white-box techniques share is that the entire software implementation is \n",
      "taken into account during testing, which facilitates defect detection even when the software specification \n",
      "is vague, outdated or incomplete. A corresponding weakness is that if the software does not implement \n",
      "one or more requirements, white box testing may not detect the resulting defects of omission (Watson \n",
      "1996). \n",
      "\n",
      "White-box techniques can be used in static testing (e.g., during dry runs of code). They are well suited to \n",
      "reviewing code that is not yet ready for execution (Hetzel 1988), as well as pseudocode and other high-\n",
      "level or top-down logic which can be modeled with a control flow graph. \n",
      "\n",
      "Performing only black-box testing does not provide a measure of actual code coverage. White-box \n",
      "coverage measures provide an objective measurement of coverage and provide the necessary \n",
      "information to allow additional tests to be generated to increase this coverage, and subsequently increase \n",
      "confidence in the code. \n",
      "\n",
      "4.4. Experience-based Test Techniques  \n",
      "\n",
      "Commonly used experience-based test techniques discussed in the following sections are: \n",
      "\n",
      "•  Error guessing \n",
      "\n",
      "•  Exploratory testing \n",
      "\n",
      "•  Checklist-based testing \n",
      "\n",
      "4.4.1.  Error Guessing \n",
      "\n",
      "Error guessing is a technique used to anticipate the occurrence of errors, defects, and failures, based on \n",
      "the tester’s knowledge, including: \n",
      "\n",
      "•  How the application has worked in the past \n",
      "\n",
      "v4.0 \n",
      "\n",
      "Page 42 of 74 \n",
      "\n",
      "2023-04-21 \n",
      "\n",
      "© International Software Testing Qualifications Board \n",
      "\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\fCertified Tester \n",
      "Foundation Level \n",
      "\n",
      "•  The types of errors the developers tend to make and the types of defects that result from these \n",
      "\n",
      "errors \n",
      "\n",
      "•  The types of failures that have occurred in other, similar applications \n",
      "\n",
      "In general, errors, defects and failures may be related to: input (e.g., correct input not accepted, \n",
      "parameters wrong or missing), output (e.g., wrong format, wrong result), logic (e.g., missing cases, wrong \n",
      "operator), computation (e.g., incorrect operand, wrong computation), interfaces (e.g., parameter \n",
      "mismatch, incompatible types), or data (e.g., incorrect initialization, wrong type). \n",
      "\n",
      "Fault attacks are a methodical approach to the implementation of error guessing. This technique requires \n",
      "the tester to create or acquire a list of possible errors, defects and failures, and to design tests that will \n",
      "identify defects associated with the errors, expose the defects, or cause the failures. These lists can be \n",
      "built based on experience, defect and failure data, or from common knowledge about why software fails. \n",
      "\n",
      "See (Whittaker 2002, Whittaker 2003, Andrews 2006) for more information on error guessing and fault \n",
      "attacks. \n",
      "\n",
      "4.4.2.  Exploratory Testing \n",
      "\n",
      "In exploratory testing, tests are simultaneously designed, executed, and evaluated while the tester learns \n",
      "about the test object. The testing is used to learn more about the test object, to explore it more deeply \n",
      "with focused tests, and to create tests for untested areas. \n",
      "\n",
      "Exploratory testing is sometimes conducted using session-based testing to structure the testing. In a \n",
      "session-based approach, exploratory testing is conducted within a defined time-box. The tester uses a \n",
      "test charter containing test objectives to guide the testing. The test session is usually followed by a \n",
      "debriefing that involves a discussion between the tester and stakeholders interested in the test results of \n",
      "the test session. In this approach test objectives may be treated as high-level test conditions. Coverage \n",
      "items are identified and exercised during the test session. The tester may use test session sheets to \n",
      "document the steps followed and the discoveries made. \n",
      "\n",
      "Exploratory testing is useful when there are few or inadequate specifications or there is significant time \n",
      "pressure on the testing. Exploratory testing is also useful to complement other more formal test \n",
      "techniques. Exploratory testing will be more effective if the tester is experienced, has domain knowledge \n",
      "and has a high degree of essential skills, like analytical skills, curiosity and creativeness (see section \n",
      "1.5.1). \n",
      "\n",
      "Exploratory testing can incorporate the use of other test techniques (e.g., equivalence partitioning). More \n",
      "information about exploratory testing can be found in (Kaner 1999, Whittaker 2009, Hendrickson 2013). \n",
      "\n",
      "4.4.3.  Checklist-Based Testing \n",
      "\n",
      "In checklist-based testing, a tester designs, implements, and executes tests to cover test conditions from \n",
      "a checklist. Checklists can be built based on experience, knowledge about what is important for the user, \n",
      "or an understanding of why and how software fails. Checklists should not contain items that can be \n",
      "checked automatically, items better suited as entry/exit criteria, or items that are too general (Brykczynski \n",
      "1999). \n",
      "\n",
      "Checklist items are often phrased in the form of a question. It should be possible to check each item \n",
      "separately and directly. These items may refer to requirements, graphical interface properties, quality \n",
      "characteristics or other forms of test conditions. Checklists can be created to support various test types, \n",
      "including functional and non-functional testing (e.g., 10 heuristics for usability testing (Nielsen 1994)). \n",
      "\n",
      "v4.0 \n",
      "\n",
      "Page 43 of 74 \n",
      "\n",
      "2023-04-21 \n",
      "\n",
      "© International Software Testing Qualifications Board \n",
      "\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\fCertified Tester \n",
      "Foundation Level \n",
      "\n",
      "Some checklist entries may gradually become less effective over time because the developers will learn \n",
      "to avoid making the same errors. New entries may also need to be added to reflect newly found high \n",
      "severity defects. Therefore, checklists should be regularly updated based on defect analysis. However, \n",
      "care should be taken to avoid letting the checklist become too long (Gawande 2009).  \n",
      "\n",
      "In the absence of detailed test cases, checklist-based testing can provide guidelines and some degree of \n",
      "consistency for the testing. If the checklists are high-level, some variability in the actual testing is likely to \n",
      "occur, resulting in potentially greater coverage but less repeatability. \n",
      "\n",
      "4.5. Collaboration-based Test Approaches \n",
      "\n",
      "Each of the above-mentioned techniques (see sections 4.2, 4.3, 4.4) has a particular objective with \n",
      "respect to defect detection. Collaboration-based approaches, on the other hand, focus also on defect \n",
      "avoidance by collaboration and communication. \n",
      "\n",
      "4.5.1.  Collaborative User Story Writing \n",
      "\n",
      "A user story represents a feature that will be valuable to either a user or purchaser of a system or \n",
      "software. User stories have three critical aspects (Jeffries 2000), called together the “3 C’s”: \n",
      "\n",
      "•  Card – the medium describing a user story (e.g., an index card, an entry in an electronic board)  \n",
      "\n",
      "•  Conversation – explains how the software will be used (can be documented or verbal)  \n",
      "\n",
      "•  Confirmation – the acceptance criteria (see section 4.5.2) \n",
      "\n",
      "The most common format for a user story is “As a [role], I want [goal to be accomplished], so that I can \n",
      "[resulting business value for the role]”, followed by the acceptance criteria. \n",
      "\n",
      "Collaborative authorship of the user story can use techniques such as brainstorming and mind mapping. \n",
      "The collaboration allows the team to obtain a shared vision of what should be delivered, by taking into \n",
      "account three perspectives: business, development and testing. \n",
      "\n",
      "Good user stories should be: Independent, Negotiable, Valuable, Estimable, Small and Testable \n",
      "(INVEST). If a stakeholder does not know how to test a user story, this may indicate that the user story is \n",
      "not clear enough, or that it does not reflect something valuable to them, or that the stakeholder just needs \n",
      "help in testing (Wake 2003). \n",
      "\n",
      "4.5.2.  Acceptance Criteria \n",
      "\n",
      "Acceptance criteria for a user story are the conditions that an implementation of the user story must meet \n",
      "to be accepted by stakeholders. From this perspective, acceptance criteria may be viewed as the test \n",
      "conditions that should be exercised by the tests. Acceptance criteria are usually a result of the \n",
      "Conversation (see section 4.5.1).  \n",
      "\n",
      "Acceptance criteria are used to: \n",
      "\n",
      "•  Define the scope of the user story  \n",
      "\n",
      "•  Reach consensus among the stakeholders \n",
      "\n",
      "•  Describe both positive and negative scenarios  \n",
      "\n",
      "•  Serve as a basis for the user story acceptance testing (see section 4.5.3)  \n",
      "\n",
      "v4.0 \n",
      "\n",
      "Page 44 of 74 \n",
      "\n",
      "2023-04-21 \n",
      "\n",
      "© International Software Testing Qualifications Board \n",
      "\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\fCertified Tester \n",
      "Foundation Level \n",
      "\n",
      "•  Allow accurate planning and estimation  \n",
      "\n",
      "There are several ways to write acceptance criteria for a user story. The two most common formats are: \n",
      "\n",
      "•  Scenario-oriented (e.g., Given/When/Then format used in BDD, see section 2.1.3)  \n",
      "\n",
      "•  Rule-oriented (e.g., bullet point verification list, or tabulated form of input-output mapping)  \n",
      "\n",
      "Most acceptance criteria can be documented in one of these two formats. However, the team may use \n",
      "another, custom format, as long as the acceptance criteria are well-defined and unambiguous. \n",
      "\n",
      "4.5.3.  Acceptance Test-driven Development (ATDD)  \n",
      "\n",
      "ATDD is a test-first approach (see section 2.1.3). Test cases are created prior to implementing the user \n",
      "story. The test cases are created by team members with different perspectives, e.g., customers, \n",
      "developers, and testers (Adzic 2009). Test cases may be executed manually or automated.  \n",
      "\n",
      "The first step is a specification workshop where the user story and (if not yet defined) its acceptance \n",
      "criteria are analyzed, discussed, and written by the team members. Incompleteness, ambiguities, or \n",
      "defects in the user story are resolved during this process. The next step is to create the test cases. This \n",
      "can be done by the team as a whole or by the tester individually. The test cases are based on the \n",
      "acceptance criteria and can be seen as examples of how the software works. This will help the team \n",
      "implement the user story correctly. \n",
      "\n",
      "Since examples and tests are the same, these terms are often used interchangeably. During the test \n",
      "design the test techniques described in sections 4.2, 4.3 and 4.4 may be applied. \n",
      "\n",
      "Typically, the first test cases are positive, confirming the correct behavior without exceptions or error \n",
      "conditions, and comprising the sequence of activities executed if everything goes as expected. After the \n",
      "positive test cases are done, the team should perform negative testing. Finally, the team should cover \n",
      "non-functional quality characteristics as well (e.g., performance efficiency, usability). Test cases should \n",
      "be expressed in a way that is understandable for the stakeholders. Typically, test cases contain \n",
      "sentences in natural language involving the necessary preconditions (if any), the inputs, and the \n",
      "postconditions.  \n",
      "\n",
      "The test cases must cover all the characteristics of the user story and should not go beyond the story. \n",
      "However, the acceptance criteria may detail some of the issues described in the user story. In addition, \n",
      "no two test cases should describe the same characteristics of the user story.  \n",
      "\n",
      "When captured in a format supported by a test automation framework, the developers can automate the \n",
      "test cases by writing the supporting code as they implement the feature described by a user story. The \n",
      "acceptance tests then become executable requirements. \n",
      "\n",
      "v4.0 \n",
      "\n",
      "Page 45 of 74 \n",
      "\n",
      "2023-04-21 \n",
      "\n",
      "© International Software Testing Qualifications Board \n",
      "\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\fCertified Tester \n",
      "Foundation Level \n",
      "\n",
      "5. Managing the Test Activities – 335 minutes \n",
      "\n",
      "Keywords \n",
      "\n",
      "defect management, defect report, entry criteria, exit criteria, product risk, project risk, risk, risk analysis, \n",
      "risk assessment, risk control, risk identification, risk level, risk management, risk mitigation, risk \n",
      "monitoring, risk-based testing, test approach, test completion report, test control, test monitoring, test \n",
      "plan, test planning, test progress report, test pyramid, testing quadrants \n",
      "\n",
      "Learning Objectives for Chapter 5: \n",
      "\n",
      "5.1 Test Planning \n",
      "\n",
      "FL-5.1.1  \n",
      "\n",
      "(K2) Exemplify the purpose and content of a test plan  \n",
      "\n",
      "FL-5.1.2 \n",
      "\n",
      "(K1) Recognize how a tester adds value to iteration and release planning \n",
      "\n",
      "FL-5.1.3 \n",
      "\n",
      "(K2) Compare and contrast entry criteria and exit criteria \n",
      "\n",
      "FL-5.1.4 \n",
      "\n",
      "(K3) Use estimation techniques to calculate the required test effort \n",
      "\n",
      "FL-5.1.5  \n",
      "\n",
      "(K3) Apply test case prioritization \n",
      "\n",
      "FL-5.1.6  \n",
      "\n",
      "(K1) Recall the concepts of the test pyramid \n",
      "\n",
      "FL-5.1.7 \n",
      "\n",
      "(K2) Summarize the testing quadrants and their relationships with test levels and test types \n",
      "\n",
      "5.2 Risk Management \n",
      "\n",
      "FL-5.2.1 \n",
      "\n",
      "(K1) Identify risk level by using risk likelihood and risk impact  \n",
      "\n",
      "FL-5.2.2 \n",
      "\n",
      "(K2) Distinguish between project risks and product risks \n",
      "\n",
      "FL-5.2.3 \n",
      "\n",
      "(K2) Explain how product risk analysis may influence thoroughness and scope of testing \n",
      "\n",
      "FL-5.2.4 \n",
      "\n",
      "(K2) Explain what measures can be taken in response to analyzed product risks \n",
      "\n",
      "5.3 Test Monitoring, Test Control and Test Completion \n",
      "\n",
      "FL-5.3.1 \n",
      "\n",
      "(K1) Recall metrics used for testing \n",
      "\n",
      "FL-5.3.2 \n",
      "\n",
      "(K2) Summarize the purposes, content, and audiences for test reports \n",
      "\n",
      "FL-5.3.3 \n",
      "\n",
      "(K2) Exemplify how to communicate the status of testing \n",
      "\n",
      "5.4 Configuration Management \n",
      "\n",
      "FL-5.4.1  \n",
      "\n",
      "(K2) Summarize how configuration management supports testing \n",
      "\n",
      "5.5 Defect Management \n",
      "\n",
      "FL-5.5.1  \n",
      "\n",
      "(K3) Prepare a defect report \n",
      "\n",
      "v4.0 \n",
      "\n",
      "Page 46 of 74 \n",
      "\n",
      "2023-04-21 \n",
      "\n",
      "© International Software Testing Qualifications Board \n",
      "\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\fCertified Tester \n",
      "Foundation Level \n",
      "\n",
      "5.1. Test Planning \n",
      "\n",
      "5.1.1.  Purpose and Content of a Test Plan \n",
      "\n",
      "A test plan describes the objectives, resources and processes for a test project. A test plan: \n",
      "\n",
      "•  Documents the means and schedule for achieving test objectives \n",
      "\n",
      "•  Helps to ensure that the performed test activities will meet the established criteria \n",
      "\n",
      "•  Serves as a means of communication with team members and other stakeholders \n",
      "\n",
      "•  Demonstrates that testing will adhere to the existing test policy and test strategy (or explains why \n",
      "\n",
      "the testing will deviate from them) \n",
      "\n",
      "Test planning guides the testers’ thinking and forces the testers to confront the future challenges related \n",
      "to risks, schedules, people, tools, costs, effort, etc. The process of preparing a test plan is a useful way to \n",
      "think through the efforts needed to achieve the test project objectives. \n",
      "\n",
      "The typical content of a test plan includes: \n",
      "\n",
      "•  Context of testing (e.g., scope, test objectives, constraints, test basis) \n",
      "\n",
      "•  Assumptions and constraints of the test project \n",
      "\n",
      "•  Stakeholders (e.g., roles, responsibilities, relevance to testing, hiring and training needs) \n",
      "\n",
      "•  Communication (e.g., forms and frequency of communication, documentation templates) \n",
      "\n",
      "•  Risk register (e.g., product risks, project risks) \n",
      "\n",
      "•  Test approach (e.g., test levels, test types, test techniques, test deliverables, entry criteria and \n",
      "exit criteria, independence of testing, metrics to be collected, test data requirements, test \n",
      "environment requirements, deviations from the organizational test policy and test strategy) \n",
      "\n",
      "•  Budget and schedule \n",
      "\n",
      "More details about the test plan and its content can be found in the ISO/IEC/IEEE 29119-3 standard. \n",
      "\n",
      "5.1.2.  Tester's Contribution to Iteration and Release Planning \n",
      "\n",
      "In iterative SDLCs, typically two kinds of planning occur: release planning and iteration planning.  \n",
      "\n",
      "Release planning looks ahead to the release of a product, defines and re-defines the product backlog, \n",
      "and may involve refining larger user stories into a set of smaller user stories. It also serves as the basis \n",
      "for the test approach and test plan across all iterations. Testers involved in release planning participate in \n",
      "writing testable user stories and acceptance criteria (see section 4.5), participate in project and quality \n",
      "risk analyses (see section 5.2), estimate test effort associated with user stories (see section 5.1.4), \n",
      "determine the test approach, and plan the testing for the release. \n",
      "\n",
      "Iteration planning looks ahead to the end of a single iteration and is concerned with the iteration backlog. \n",
      "Testers involved in iteration planning participate in the detailed risk analysis of user stories, determine the \n",
      "testability of user stories, break down user stories into tasks (particularly testing tasks), estimate test effort \n",
      "for all testing tasks, and identify and refine functional and non-functional aspects of the test object. \n",
      "\n",
      "v4.0 \n",
      "\n",
      "Page 47 of 74 \n",
      "\n",
      "2023-04-21 \n",
      "\n",
      "© International Software Testing Qualifications Board \n",
      "\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\fCertified Tester \n",
      "Foundation Level \n",
      "\n",
      "5.1.3.  Entry Criteria and Exit Criteria \n",
      "\n",
      "Entry criteria define the preconditions for undertaking a given activity. If entry criteria are not met, it is \n",
      "likely that the activity will prove to be more difficult, time-consuming, costly, and riskier. Exit criteria define \n",
      "what must be achieved in order to declare an activity completed. Entry criteria and exit criteria should be \n",
      "defined for each test level, and will differ based on the test objectives. \n",
      "\n",
      "Typical entry criteria include: availability of resources (e.g., people, tools, environments, test data, budget, \n",
      "time), availability of testware (e.g., test basis, testable requirements, user stories, test cases), and initial \n",
      "quality level of a test object (e.g., all smoke tests have passed). \n",
      "\n",
      "Typical exit criteria include: measures of thoroughness (e.g., achieved level of coverage, number of \n",
      "unresolved defects, defect density, number of failed test cases), and completion criteria (e.g., planned \n",
      "tests have been executed, static testing has been performed, all defects found are reported, all \n",
      "regression tests are automated). \n",
      "\n",
      "Running out of time or budget can also be viewed as valid exit criteria. Even without other exit criteria \n",
      "being satisfied, it can be acceptable to end testing under such circumstances, if the stakeholders have \n",
      "reviewed and accepted the risk to go live without further testing.  \n",
      "\n",
      "In Agile software development, exit criteria are often called Definition of Done, defining the team’s \n",
      "objective metrics for a releasable item. Entry criteria that a user story must fulfill to start the development \n",
      "and/or testing activities are called Definition of Ready. \n",
      "\n",
      "5.1.4.  Estimation Techniques \n",
      "\n",
      "Test effort estimation involves predicting the amount of test-related work needed to meet the objectives of \n",
      "a test project. It is important to make it clear to the stakeholders that the estimate is based on a number of \n",
      "assumptions and is always subject to estimation error. Estimation for small tasks is usually more accurate \n",
      "than for the large ones. Therefore, when estimating a large task, it can be decomposed into a set of \n",
      "smaller tasks which then in turn can be estimated. \n",
      "\n",
      "In this syllabus, the following four estimation techniques are described. \n",
      "\n",
      "Estimation based on ratios. In this metrics-based technique, figures are collected from previous projects \n",
      "within the organization, which makes it possible to derive “standard” ratios for similar projects. The ratios \n",
      "of an organization’s own projects (e.g., taken from historical data) are generally the best source to use in \n",
      "the estimation process. These standard ratios can then be used to estimate the test effort for the new \n",
      "project. For example, if in the previous project the development-to-test effort ratio was 3:2, and in the \n",
      "current project the development effort is expected to be 600 person-days, the test effort can be estimated \n",
      "to be 400 person-days. \n",
      "\n",
      "Extrapolation. In this metrics-based technique, measurements are made as early as possible in the \n",
      "current project to gather the data. Having enough observations, the effort required for the remaining work \n",
      "can be approximated by extrapolating this data (usually by applying a mathematical model). This method \n",
      "is very suitable in iterative SDLCs. For example, the team may extrapolate the test effort in the \n",
      "forthcoming iteration as the averaged effort from the last three iterations. \n",
      "\n",
      "Wideband Delphi. In this iterative, expert-based technique, experts make experience-based estimations. \n",
      "Each expert, in isolation, estimates the effort. The results are collected and if there are deviations that are \n",
      "out of range of the agreed upon boundaries, the experts discuss their current estimates. Each expert is \n",
      "then asked to make a new estimation based on that feedback, again in isolation. This process is repeated \n",
      "until a consensus is reached. Planning Poker is a variant of Wideband Delphi, commonly used in Agile \n",
      "\n",
      "v4.0 \n",
      "\n",
      "Page 48 of 74 \n",
      "\n",
      "2023-04-21 \n",
      "\n",
      "© International Software Testing Qualifications Board \n",
      "\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\fCertified Tester \n",
      "Foundation Level \n",
      "\n",
      "software development. In Planning Poker, estimates are usually made using cards with numbers that \n",
      "represent the effort size. \n",
      "\n",
      "Three-point estimation. In this expert-based technique, three estimations are made by the experts: the \n",
      "most optimistic estimation (a), the most likely estimation (m) and the most pessimistic estimation (b). The \n",
      "final estimate (E) is their weighted arithmetic mean. In the most popular version of this technique, the \n",
      "estimate is calculated as E = (a + 4*m + b) / 6. The advantage of this technique is that it allows the \n",
      "experts to calculate the measurement error: SD = (b – a) / 6. For example, if the estimates (in person-\n",
      "hours) are: a=6, m=9 and b=18, then the final estimation is 10±2 person-hours (i.e., between 8 and 12 \n",
      "person-hours), because E = (6 + 4*9 + 18) / 6 = 10 and SD = (18 – 6) / 6 = 2. \n",
      "\n",
      "See (Kan 2003, Koomen 2006, Westfall 2009) for these and many other test estimation techniques. \n",
      "\n",
      "5.1.5.  Test Case Prioritization \n",
      "\n",
      "Once the test cases and test procedures are specified and assembled into test suites, these test suites \n",
      "can be arranged in a test execution schedule that defines the order in which they are to be run. When \n",
      "prioritizing test cases, different factors can be taken into account. The most commonly used test case \n",
      "prioritization strategies are as follows: \n",
      "\n",
      "•  Risk-based prioritization, where the order of test execution is based on the results of risk analysis \n",
      "\n",
      "(see section 5.2.3). Test cases covering the most important risks are executed first. \n",
      "\n",
      "•  Coverage-based prioritization, where the order of test execution is based on coverage (e.g., \n",
      "\n",
      "statement coverage). Test cases achieving the highest coverage are executed first. In another \n",
      "variant, called additional coverage prioritization, the test case achieving the highest coverage is \n",
      "executed first; each subsequent test case is the one that achieves the highest additional \n",
      "coverage. \n",
      "\n",
      "•  Requirements-based prioritization, where the order of test execution is based on the priorities of \n",
      "the requirements traced back to the corresponding test cases. Requirement priorities are defined \n",
      "by stakeholders. Test cases related to the most important requirements are executed first. \n",
      "\n",
      "Ideally, test cases would be ordered to run based on their priority levels, using, for example, one of the \n",
      "above-mentioned prioritization strategies. However, this practice may not work if the test cases or the \n",
      "features being tested have dependencies. If a test case with a higher priority is dependent on a test case \n",
      "with a lower priority, the lower priority test case must be executed first.  \n",
      "\n",
      "The order of test execution must also take into account the availability of resources. For example, the \n",
      "required test tools, test environments or people that may only be available for a specific time window. \n",
      "\n",
      "5.1.6.  Test Pyramid \n",
      "\n",
      "The test pyramid is a model showing that different tests may have different granularity. The test pyramid \n",
      "model supports the team in test automation and in test effort allocation by showing that different goals are \n",
      "supported by different levels of test automation. The pyramid layers represent groups of tests. The higher \n",
      "the layer, the lower the test granularity, test isolation and test execution time. Tests in the bottom layer \n",
      "are small, isolated, fast, and check a small piece of functionality, so usually a lot of them are needed to \n",
      "achieve a reasonable coverage. The top layer represents complex, high-level, end-to-end tests. These \n",
      "high-level tests are generally slower than the tests from the lower layers, and they typically check a large \n",
      "piece of functionality, so usually just a few of them are needed to achieve a reasonable coverage. The \n",
      "number and naming of the layers may differ. For example, the original test pyramid model (Cohn 2009) \n",
      "defines three layers: “unit tests”, “service tests” and “UI tests”. Another popular model defines unit \n",
      "\n",
      "v4.0 \n",
      "\n",
      "Page 49 of 74 \n",
      "\n",
      "2023-04-21 \n",
      "\n",
      "© International Software Testing Qualifications Board \n",
      "\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\fCertified Tester \n",
      "Foundation Level \n",
      "\n",
      "(component) tests, integration (component integration) tests, and end-to-end tests. Other test levels (see \n",
      "section 2.2.1) can also be used. \n",
      "\n",
      "5.1.7.  Testing Quadrants \n",
      "\n",
      "The testing quadrants, defined by Brian Marick (Marick 2003, Crispin 2008), group the test levels with the \n",
      "appropriate test types, activities, test techniques and work products in the Agile software development. \n",
      "The model supports test management in visualizing these to ensure that all appropriate test types and \n",
      "test levels are included in the SDLC and in understanding that some test types are more relevant to \n",
      "certain test levels than others. This model also provides a way to differentiate and describe the types of \n",
      "tests to all stakeholders, including developers, testers, and business representatives.  \n",
      "\n",
      "In this model, tests can be business facing or technology facing. Tests can also support the team (i.e., \n",
      "guide the development) or critique the product (i.e., measure its behavior against the expectations). The \n",
      "combination of these two viewpoints determines the four quadrants: \n",
      "\n",
      "•  Quadrant Q1 (technology facing, support the team). This quadrant contains component and \n",
      "\n",
      "component integration tests. These tests should be automated and included in the CI process. \n",
      "\n",
      "•  Quadrant Q2 (business facing, support the team). This quadrant contains functional tests, \n",
      "\n",
      "examples, user story tests, user experience prototypes, API testing, and simulations. These tests \n",
      "check the acceptance criteria and can be manual or automated. \n",
      "\n",
      "•  Quadrant Q3 (business facing, critique the product). This quadrant contains exploratory testing, \n",
      "usability testing, user acceptance testing. These tests are user-oriented and often manual. \n",
      "\n",
      "•  Quadrant Q4 (technology facing, critique the product). This quadrant contains smoke tests and \n",
      "\n",
      "non-functional tests (except usability tests). These tests are often automated. \n",
      "\n",
      "5.2. Risk Management \n",
      "\n",
      "Organizations face many internal and external factors that make it uncertain whether and when they will \n",
      "achieve their objectives (ISO 31000). Risk management allows the organizations to increase the \n",
      "likelihood of achieving objectives, improve the quality of their products and increase the stakeholders’ \n",
      "confidence and trust.  \n",
      "\n",
      "The main risk management activities are: \n",
      "\n",
      "•  Risk analysis (consisting of risk identification and risk assessment; see section 5.2.3) \n",
      "\n",
      "•  Risk control (consisting of risk mitigation and risk monitoring; see section 5.2.4) \n",
      "\n",
      "The test approach, in which test activities are selected, prioritized, and managed based on risk analysis \n",
      "and risk control, is called risk-based testing. \n",
      "\n",
      "5.2.1.  Risk Definition and Risk Attributes \n",
      "\n",
      "Risk is a potential event, hazard, threat, or situation whose occurrence causes an adverse effect. A risk \n",
      "can be characterized by two factors: \n",
      "\n",
      "•  Risk likelihood – the probability of the risk occurrence (greater than zero and less than one) \n",
      "\n",
      "•  Risk impact (harm) – the consequences of this occurrence \n",
      "\n",
      "v4.0 \n",
      "\n",
      "Page 50 of 74 \n",
      "\n",
      "2023-04-21 \n",
      "\n",
      "© International Software Testing Qualifications Board \n",
      "\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\fCertified Tester \n",
      "Foundation Level \n",
      "\n",
      "These two factors express the risk level, which is a measure for the risk. The higher the risk level, the \n",
      "more important is its treatment. \n",
      "\n",
      "5.2.2.  Project Risks and Product Risks \n",
      "\n",
      "In software testing one is generally concerned with two types of risks: project risks and product risks. \n",
      "\n",
      "Project risks are related to the management and control of the project. Project risks include: \n",
      "\n",
      "•  Organizational issues (e.g., delays in work products deliveries, inaccurate estimates, cost-cutting) \n",
      "\n",
      "•  People issues (e.g., insufficient skills, conflicts, communication problems, shortage of staff) \n",
      "\n",
      "•  Technical issues (e.g., scope creep, poor tool support) \n",
      "\n",
      "•  Supplier issues (e.g., third-party delivery failure, bankruptcy of the supporting company) \n",
      "\n",
      "Project risks, when they occur, may have an impact on the project schedule, budget or scope, which \n",
      "affects the project's ability to achieve its objectives. \n",
      "\n",
      "Product risks are related to the product quality characteristics (e.g., described in the ISO 25010 quality \n",
      "model). Examples of product risks include: missing or wrong functionality, incorrect calculations, runtime \n",
      "errors, poor architecture, inefficient algorithms, inadequate response time, poor user experience, security \n",
      "vulnerabilities. Product risks, when they occur, may result in various negative consequences, including: \n",
      "\n",
      "•  User dissatisfaction \n",
      "\n",
      "•  Loss of revenue, trust, reputation \n",
      "\n",
      "•  Damage to third parties \n",
      "\n",
      "•  High maintenance costs, overload of the helpdesk \n",
      "\n",
      "•  Criminal penalties \n",
      "\n",
      "• \n",
      "\n",
      "In extreme cases, physical damage, injuries or even death \n",
      "\n",
      "5.2.3.  Product Risk Analysis \n",
      "\n",
      "From a testing perspective, the goal of product risk analysis is to provide an awareness of product risk in \n",
      "order to focus the testing effort in a way that minimizes the residual level of product risk. Ideally, product \n",
      "risk analysis begins early in the SDLC. \n",
      "\n",
      "Product risk analysis consists of risk identification and risk assessment. Risk identification is about \n",
      "generating a comprehensive list of risks. Stakeholders can identify risks by using various techniques and \n",
      "tools, e.g., brainstorming, workshops, interviews, or cause-effect diagrams. Risk assessment involves: \n",
      "categorization of identified risks, determining their risk likelihood, risk impact and level, prioritizing, and \n",
      "proposing ways to handle them. Categorization helps in assigning mitigation actions, because usually \n",
      "risks falling into the same category can be mitigated using a similar approach.  \n",
      "\n",
      "Risk assessment can use a quantitative or qualitative approach, or a mix of them. In the quantitative \n",
      "approach the risk level is calculated as the multiplication of risk likelihood and risk impact. In the \n",
      "qualitative approach the risk level can be determined using a risk matrix. \n",
      "\n",
      "Product risk analysis may influence the thoroughness and scope of testing. Its results are used to: \n",
      "\n",
      "v4.0 \n",
      "\n",
      "Page 51 of 74 \n",
      "\n",
      "2023-04-21 \n",
      "\n",
      "© International Software Testing Qualifications Board \n",
      "\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\fCertified Tester \n",
      "Foundation Level \n",
      "\n",
      "•  Determine the scope of testing to be carried out  \n",
      "\n",
      "•  Determine the particular test levels and propose test types to be performed  \n",
      "\n",
      "•  Determine the test techniques to be employed and the coverage to be achieved \n",
      "\n",
      "•  Estimate the test effort required for each task \n",
      "\n",
      "•  Prioritize testing in an attempt to find the critical defects as early as possible \n",
      "\n",
      "•  Determine whether any activities in addition to testing could be employed to reduce risk \n",
      "\n",
      "5.2.4.  Product Risk Control \n",
      "\n",
      "Product risk control comprises all measures that are taken in response to identified and assessed product \n",
      "risks. Product risk control consists of risk mitigation and risk monitoring. Risk mitigation involves \n",
      "implementing the actions proposed in risk assessment to reduce the risk level. The aim of risk monitoring \n",
      "is to ensure that the mitigation actions are effective, to obtain further information to improve risk \n",
      "assessment, and to identify emerging risks. \n",
      "\n",
      "With respect to product risk control, once a risk has been analyzed, several response options to risk are \n",
      "possible, e.g., risk mitigation by testing, risk acceptance, risk transfer, or contingency plan (Veenendaal \n",
      "2012). Actions that can be taken to mitigate the product risks by testing are as follows: \n",
      "\n",
      "•  Select the testers with the right level of experience and skills, suitable for a given risk type \n",
      "\n",
      "•  Apply an appropriate level of independence of testing \n",
      "\n",
      "•  Conduct reviews and perform static analysis \n",
      "\n",
      "•  Apply the appropriate test techniques and coverage levels \n",
      "\n",
      "•  Apply the appropriate test types addressing the affected quality characteristics \n",
      "\n",
      "•  Perform dynamic testing, including regression testing \n",
      "\n",
      "5.3. Test Monitoring, Test Control and Test Completion \n",
      "\n",
      "Test monitoring is concerned with gathering information about testing. This information is used to assess \n",
      "test progress and to measure whether the test exit criteria or the test tasks associated with the exit criteria \n",
      "are satisfied, such as meeting the targets for coverage of product risks, requirements, or acceptance \n",
      "criteria.  \n",
      "\n",
      "Test control uses the information from test monitoring to provide, in a form of the control directives, \n",
      "guidance and the necessary corrective actions to achieve the most effective and efficient testing. \n",
      "Examples of control directives include: \n",
      "\n",
      "•  Reprioritizing tests when an identified risk becomes an issue \n",
      "\n",
      "•  Re-evaluating whether a test item meets entry criteria or exit criteria due to rework \n",
      "\n",
      "•  Adjusting the test schedule to address a delay in the delivery of the test environment \n",
      "\n",
      "•  Adding new resources when and where needed \n",
      "\n",
      "v4.0 \n",
      "\n",
      "Page 52 of 74 \n",
      "\n",
      "2023-04-21 \n",
      "\n",
      "© International Software Testing Qualifications Board \n",
      "\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\fCertified Tester \n",
      "Foundation Level \n",
      "\n",
      "Test completion collects data from completed test activities to consolidate experience, testware, and any \n",
      "other relevant information. Test completion activities occur at project milestones such as when a test level \n",
      "is completed, an agile iteration is finished, a test project is completed (or cancelled), a software system is \n",
      "released, or a maintenance release is completed. \n",
      "\n",
      "5.3.1.  Metrics used in Testing \n",
      "\n",
      "Test metrics are gathered to show progress against the planned schedule and budget, the current quality \n",
      "of the test object, and the effectiveness of the test activities with respect to the objectives or an iteration \n",
      "goal. Test monitoring gathers a variety of metrics to support the test control and test completion.  \n",
      "\n",
      "Common test metrics include: \n",
      "\n",
      "•  Project progress metrics (e.g., task completion, resource usage, test effort) \n",
      "\n",
      "•  Test progress metrics (e.g., test case implementation progress, test environment preparation \n",
      "\n",
      "progress, number of test cases run/not run, passed/failed, test execution time) \n",
      "\n",
      "•  Product quality metrics (e.g., availability, response time, mean time to failure) \n",
      "\n",
      "•  Defect metrics (e.g., number and priorities of defects found/fixed, defect density, defect detection \n",
      "\n",
      "percentage) \n",
      "\n",
      "•  Risk metrics (e.g., residual risk level) \n",
      "\n",
      "•  Coverage metrics (e.g., requirements coverage, code coverage) \n",
      "\n",
      "•  Cost metrics (e.g., cost of testing, organizational cost of quality) \n",
      "\n",
      "5.3.2.  Purpose, Content and Audience for Test Reports \n",
      "\n",
      "Test reporting summarizes and communicates test information during and after testing. Test progress \n",
      "reports support the ongoing control of the testing and must provide enough information to make \n",
      "modifications to the test schedule, resources, or test plan, when such changes are needed due to \n",
      "deviation from the plan or changed circumstances. Test completion reports summarize a specific stage of \n",
      "testing (e.g., test level, test cycle, iteration) and can give information for subsequent testing.  \n",
      "\n",
      "During test monitoring and control, the test team generates test progress reports for stakeholders to keep \n",
      "them informed. Test progress reports are usually generated on a regular basis (e.g., daily, weekly, etc.) \n",
      "and include: \n",
      "\n",
      "•  Test period \n",
      "\n",
      "•  Test progress (e.g., ahead or behind schedule), including any notable deviations \n",
      "\n",
      "• \n",
      "\n",
      "Impediments for testing, and their workarounds \n",
      "\n",
      "•  Test metrics (see section 5.3.1 for examples) \n",
      "\n",
      "•  New and changed risks within testing period \n",
      "\n",
      "•  Testing planned for the next period \n",
      "\n",
      "v4.0 \n",
      "\n",
      "Page 53 of 74 \n",
      "\n",
      "2023-04-21 \n",
      "\n",
      "© International Software Testing Qualifications Board \n",
      "\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\fCertified Tester \n",
      "Foundation Level \n",
      "\n",
      "A test completion report is prepared during test completion, when a project, test level, or test type is \n",
      "complete and when, ideally, its exit criteria have been met. This report uses test progress reports and \n",
      "other data. Typical test completion reports include: \n",
      "\n",
      "•  Test summary \n",
      "\n",
      "•  Testing and product quality evaluation based on the original test plan (i.e., test objectives and exit \n",
      "\n",
      "criteria) \n",
      "\n",
      "•  Deviations from the test plan (e.g., differences from the planned schedule, duration, and effort). \n",
      "\n",
      "•  Testing impediments and workarounds \n",
      "\n",
      "•  Test metrics based on test progress reports \n",
      "\n",
      "•  Unmitigated risks, defects not fixed \n",
      "\n",
      "•  Lessons learned that are relevant to the testing \n",
      "\n",
      "Different audiences require different information in the reports, and influence the degree of formality and \n",
      "the frequency of reporting. Reporting on test progress to others in the same team is often frequent and \n",
      "informal, while reporting on testing for a completed project follows a set template and occurs only once.  \n",
      "\n",
      "The ISO/IEC/IEEE 29119-3 standard includes templates and examples for test progress reports (called \n",
      "test status reports) and test completion reports. \n",
      "\n",
      "5.3.3.  Communicating the Status of Testing \n",
      "\n",
      "The best means of communicating test status varies, depending on test management concerns, \n",
      "organizational test strategies, regulatory standards, or, in the case of self-organizing teams (see section \n",
      "1.5.2), on the team itself. The options include: \n",
      "\n",
      "•  Verbal communication with team members and other stakeholders \n",
      "\n",
      "•  Dashboards (e.g., CI/CD dashboards, task boards, and burn-down charts) \n",
      "\n",
      "•  Electronic communication channels (e.g., email, chat)  \n",
      "\n",
      "•  Online documentation \n",
      "\n",
      "•  Formal test reports (see section 5.3.2) \n",
      "\n",
      "One or more of these options can be used. More formal communication may be more appropriate for \n",
      "distributed teams where direct face-to-face communication is not always possible due to geographical \n",
      "distance or time differences. Typically, different stakeholders are interested in different types of \n",
      "information, so communication should be tailored accordingly. \n",
      "\n",
      "5.4. Configuration Management \n",
      "\n",
      "In testing, configuration management (CM) provides a discipline for identifying, controlling, and tracking \n",
      "work products such as test plans, test strategies, test conditions, test cases, test scripts, test results, test \n",
      "logs, and test reports as configuration items. \n",
      "\n",
      "v4.0 \n",
      "\n",
      "Page 54 of 74 \n",
      "\n",
      "2023-04-21 \n",
      "\n",
      "© International Software Testing Qualifications Board \n",
      "\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\fCertified Tester \n",
      "Foundation Level \n",
      "\n",
      "For a complex configuration item (e.g., a test environment), CM records the items it consists of, their \n",
      "relationships, and versions. If the configuration item is approved for testing, it becomes a baseline and \n",
      "can only be changed through a formal change control process. \n",
      "\n",
      "Configuration management keeps a record of changed configuration items when a new baseline is \n",
      "created. It is possible to revert to a previous baseline to reproduce previous test results. \n",
      "\n",
      "To properly support testing, CM ensures the following:  \n",
      "\n",
      "•  All configuration items, including test items (individual parts of the test object), are uniquely \n",
      "\n",
      "identified, version controlled, tracked for changes, and related to other configuration items so that \n",
      "traceability can be maintained throughout the test process  \n",
      "\n",
      "•  All identified documentation and software items are referenced unambiguously in test \n",
      "\n",
      "documentation \n",
      "\n",
      "Continuous integration, continuous delivery, continuous deployment and the associated testing are \n",
      "typically implemented as part of an automated DevOps pipeline (see section 2.1.4), in which automated \n",
      "CM is normally included. \n",
      "\n",
      "5.5. Defect Management \n",
      "\n",
      "Since one of the major test objectives is to find defects, an established defect management process is \n",
      "essential. Although we refer to \"defects\" here, the reported anomalies may turn out to be real defects or \n",
      "something else (e.g., false positive, change request) - this is resolved during the process of dealing with \n",
      "the defect reports. Anomalies may be reported during any phase of the SDLC and the form depends on \n",
      "the SDLC. At a minimum, the defect management process includes a workflow for handling individual \n",
      "anomalies from their discovery to their closure and rules for their classification. The workflow typically \n",
      "comprises activities to log the reported anomalies, analyze and classify them, decide on a suitable \n",
      "response such as to fix or keep it as it is and finally to close the defect report. The process must be \n",
      "followed by all involved stakeholders. It is advisable to handle defects from static testing (especially static \n",
      "analysis) in a similar way.  \n",
      "\n",
      "Typical defect reports have the following objectives: \n",
      "\n",
      "•  Provide those responsible for handling and resolving reported defects with sufficient information \n",
      "\n",
      "to resolve the issue \n",
      "\n",
      "•  Provide a means of tracking the quality of the work product \n",
      "\n",
      "•  Provide ideas for improvement of the development and test process \n",
      "\n",
      "A defect report logged during dynamic testing typically includes: \n",
      "\n",
      "•  Unique identifier \n",
      "\n",
      "•  Title with a short summary of the anomaly being reported \n",
      "\n",
      "•  Date when the anomaly was observed, issuing organization, and author, including their role  \n",
      "\n",
      "• \n",
      "\n",
      "Identification of the test object and test environment \n",
      "\n",
      "•  Context of the defect (e.g., test case being run, test activity being performed, SDLC phase, and \n",
      "\n",
      "other relevant information such as the test technique, checklist or test data being used) \n",
      "\n",
      "v4.0 \n",
      "\n",
      "Page 55 of 74 \n",
      "\n",
      "2023-04-21 \n",
      "\n",
      "© International Software Testing Qualifications Board \n",
      "\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\fCertified Tester \n",
      "Foundation Level \n",
      "\n",
      "•  Description of the failure to enable reproduction and resolution including the steps that detected \n",
      "\n",
      "the anomaly, and any relevant test logs, database dumps, screenshots, or recordings \n",
      "\n",
      "•  Expected results and actual results \n",
      "\n",
      "•  Severity of the defect (degree of impact) on the interests of stakeholders or requirements \n",
      "\n",
      "•  Priority to fix \n",
      "\n",
      "•  Status of the defect (e.g., open, deferred, duplicate, waiting to be fixed, awaiting confirmation \n",
      "\n",
      "testing, re-opened, closed, rejected) \n",
      "\n",
      "•  References (e.g., to the test case) \n",
      "\n",
      "Some of this data may be automatically included when using defect management tools (e.g., identifier, \n",
      "date, author and initial status). Document templates for a defect report and example defect reports can be \n",
      "found in the ISO/IEC/IEEE 29119-3 standard, which refers to defect reports as incident reports. \n",
      "\n",
      "v4.0 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Removing text from 'Page 56 of 74' till the end of the text\n",
    "\n",
    "# Looking up for the text to remove everything after it\n",
    "target_text = \"Page 56 of 74\"\n",
    "#print(extracted_text)\n",
    "# Finding the position of the target text in the extracted text\n",
    "end_position = extracted_text.find(target_text)\n",
    "\n",
    "# Checking if the target text was found, just in case\n",
    "if end_position != -1:\n",
    "    # Removing everything before the target text\n",
    "    extracted_text = extracted_text[:end_position]\n",
    "\n",
    "\n",
    "# let us save the content to .txt file with prefix '_v0.1' for further debugging purpose and human evaluation process\n",
    "\n",
    "output_file_path = 'data/ISTQB_CTFL_Syllabus-v4.0_v03.txt'\n",
    "with open(output_file_path, 'w', encoding='utf-8') as out_file:\n",
    "    # Writing the extracted text to the output file\n",
    "    out_file.write(extracted_text)\n",
    "\n",
    "# Closing the stream\n",
    "output_string.close()\n",
    "\n",
    "# Printing message to indicate that the text has been saved to the file\n",
    "print(f\"Extracted text for 'The Certified Tester Foundation Level in Software Testing' pre processed version 0.3 saved to '{output_file_path}'\")\n",
    "print(extracted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(extracted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your stop words list\n",
    "# stop_words = [\"v4.0\", \"Page\", \"74\", \"18\", \"15\", \"of\", \"2023-04-21\", \"©\", \"Certified Tester\", \"Foundation\", \"Level\", \"International Software Testing Qualifications Board\"]\n",
    "# \n",
    "#Split the extracted_text into words\n",
    "# words = extracted_text.split()\n",
    "# \n",
    "#Filter out words that are in the stop words list\n",
    "# filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "# \n",
    "#Join the filtered words back into a text\n",
    "# extracted_text = \" \".join(filtered_words)\n",
    "# \n",
    "#Print the cleaned text\n",
    "#print(extracted_text)\n",
    "# \n",
    "# \n",
    "# output_file_path = 'data/ISTQB_CTFL_Syllabus-v4.0_v05.txt'\n",
    "# with open(output_file_path, 'w', encoding='utf-8') as out_file:\n",
    "#    Writing the extracted text to the output file\n",
    "    # out_file.write(extracted_text)\n",
    "# \n",
    "#Closing the stream\n",
    "# output_string.close()\n",
    "# \n",
    "#Printing message to indicate that the text has been saved to the file\n",
    "# print(f\"Extracted text for 'The Certified Tester Foundation Level in Software Testing' pre processed version 0.5 saved to '{output_file_path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to lower case all words in stringIO\n",
    "#extracted_text = extracted_text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#punctuation\n",
    "# Load the language model\n",
    "#nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Process the text with SpaCy\n",
    "###doc = nlp(extracted_text)\n",
    "\n",
    "# Create a list of tokens that are not punctuation\n",
    "#filtered_tokens = [token.text for token in doc if not token.is_punct]\n",
    "\n",
    "# Join the filtered tokens back into a text\n",
    "#extracted_text = \" \".join(filtered_tokens)\n",
    "\n",
    "# Print the text without punctuation\n",
    "#print(extracted_text)\n",
    "\n",
    "#output_file_path = 'data/ISTQB_CTFL_Syllabus-v4.0_v04.txt'\n",
    "#with open(output_file_path, 'w', encoding='utf-8') as out_file:\n",
    "    # Writing the extracted text to the output file\n",
    "#    out_file.write(extracted_text)\n",
    "\n",
    "# Closing the stream\n",
    "#output_string.close()\n",
    "\n",
    "# Printing message to indicate that the text has been saved to the file\n",
    "#print(f\"Extracted text for 'The Certified Tester Foundation Level in Software Testing' pre processed version 0.1 saved to '{output_file_path}'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted text for 'The Certified Tester Foundation Level in Software Testing' pre processed version 0.5 saved to 'data/ISTQB_CTFL_Syllabus-v4.0_v05.txt'\n"
     ]
    }
   ],
   "source": [
    "# experiment\n",
    "\n",
    "# Define the stop words list\n",
    "stop_words = [\n",
    "    \"©\", \"15\", \"16\", \"17\", \"18\", \"19\", \"20\", r\"\\b20\\b\", \"21\", \"22\", \"23\", \"24\", \"25\", \"26\", \"27\", \"28\", \"29\", \"37\", \"38\", \"39\", \"40\", \"41\", \"42\", \"43\", \"44\", \"45\", \"46\", \"47\", \"48\", \"49\", \"50\", \"51\", \"52\", \"53\", \"54\", \"International Software Testing Qualifications Board Certified Tester Foundation Level\", \"21.04.2023\", \"01.07.2021\", \"11.11.2019\", \"27.04.2018\", \"1.04.2011\", \"30.03.2010\", \"01.05.2007\", \"01.07.2005\", \"25.02.1999\", \"the\", \"market\", \"(\", \")\", \"in\", \"or\"]\n",
    "\n",
    "# Split the extracted_text into words\n",
    "words = extracted_text.split()\n",
    "#print (words)\n",
    "\n",
    "# Initialize an empty list to store the filtered words\n",
    "filtered_words = []\n",
    "# Iterate through the words in the extracted_text\n",
    "\n",
    "filtered_words = [word for word in words if word not in stop_words]\n",
    "\n",
    "# Join the filtered words back into a text with spaces and line breaks\n",
    "extracted_text = \" \".join(filtered_words)\n",
    "\n",
    "\n",
    "\n",
    "# Join the filtered words back into a text\n",
    "extracted_text = \" \".join(filtered_words)\n",
    "#print(extracted_text)\n",
    "\n",
    "output_file_path = 'data/ISTQB_CTFL_Syllabus-v4.0_v05.txt' \n",
    "with open(output_file_path, 'w', encoding='utf-8') as out_file:\n",
    "    # Writing the extracted text to the output file\n",
    "    out_file.write(extracted_text)\n",
    "output_string.close()\n",
    " \n",
    "# Printing message to indicate that the text has been saved to the file\n",
    "print(f\"Extracted text for 'The Certified Tester Foundation Level in Software Testing' pre processed version 0.5 saved to '{output_file_path}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check results, looks like some parts are not removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(extracted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted text for 'The Certified Tester Foundation Level in Software Testing' pre processed version 0.5 saved to 'data/ISTQB_CTFL_Syllabus-v4.0_v05.txt'\n"
     ]
    }
   ],
   "source": [
    "# built by chatgpt on provided context from my side, I used a part of text of file above, reviewed and customized by me as well\n",
    "#stop_words = [\"buxton\", \"a\", \"about\", \"above\", \"additional\", \"an\", \"and\", \"another\", \"are\", \"as\", \"be\", \"being\", \"by\", \"can\", \"common\", \"commonly\", \"do\", \"does\", \"each\", \"even\", \"for\", \"from\", \"has\", \"have\", \"in\", \"including\", \"is\", \"it\", \"its\", \"it's\", \"many\", \"may\", \"more\", \"most\", \"not\", \"of\", \"74\", \"often\", \"on\", \"or\", \"over\", \"such\", \"than\", \"that\", \"the\", \"there\", \"these\", \"this\", \"to\", \"under\", \"was\", \"we\", \"what\", \"when\", \"which\", \"who\", \"why\", \"will\", \"with\", \"within\", \"work\", \"you\", \"2023\", \"04\", \"21\", \"v4.0\", \"page\", \"2023-04-21\", \"©\", \"international\", \"qualifications\", \"board\", \"certified\", \"tester\",  \"foundation\", \"level\", \"FL-\", \"K2\", \"see\", \"section\" , \"didn't\", \"doesn't\", \"don't\", \"i.e.\", \"it's\", \"let's\", \"that's\", \"there's\", \"they're\", \"you're\", \"e.g.\"]\n",
    "stop_words = [ \"©\", \"15\", \"16\", \"17\", \"18\", \"19\", \"20\", r\"\\b20\\b\", \"21\", \"22\", \"23\", \"24\", \"25\", \"26\", \"27\", \"28\", \"29\", \"30\", \"31\", \"32\", \"33\", \"34\", \"35\", \"36\",\n",
    "              \"37\", \"38\",\"39\", \"40\",\"41\", \"42\",\"43\", \"44\",\"45\", \"46\", \"47\", \"48\", \"49\", \"50\", \"51\", \"52\", \"53\", \"54\", \n",
    "              \"International Software Testing Qualifications Board Certified Tester Foundation Level\", \"21.04.2023\", \"01.07.2021\",\n",
    "              \"11.11.2019\", \"27.04.2018\", \"1.04.2011\", \"30.03.2010\", \"01.05.2007\", \"01.07.2005\", \"25.02.1999\", \"the\", \"market\", \"(\", \")\", \"in\", \"or\"]\n",
    " \n",
    "# Regular expression pattern to match phrases like \"15 74\", \"16 74\", ..., \"54 74\"\n",
    "pattern = re.compile(r\"(?s)^v4.0.*Foundation Level$\", re.DOTALL)\n",
    "# Split the extracted_text into words\n",
    "words = re.split(r'\\s+', extracted_text)\n",
    "# \n",
    "# Filter out words that match the regular expression pattern or are in the stop words list\n",
    "filtered_words = [word for word in words if not re.match(pattern, word) and word.lower() not in stop_words] #match\n",
    "# Join the filtered words back into a text\n",
    "extracted_text = \" \".join(filtered_words)\n",
    "\n",
    "# Ph. removal\n",
    "phrase_to_remove = \"International Software Testing Qualifications Board Certified Tester Foundation Level\"\n",
    "phrase_to_remove_v = \"v4.0 Page of 74 2023-04-21\"\n",
    "\n",
    "# Replace the phrase with an empty string and comas removal (across the whole text)\n",
    "extracted_text = extracted_text.replace(phrase_to_remove, \"\")\n",
    "#extracted_text = extracted_text.replace(\",\", \"\")\n",
    "extracted_text = extracted_text.replace(phrase_to_remove_v, \"\")\n",
    "# Regular expression pattern to match and remove text inside brackets and brackets as well\n",
    "pattern_brackets = r'\\([^)]*\\)'\n",
    "\n",
    "# removal of text inside brackets and brackets\n",
    "extracted_text = re.sub(pattern_brackets, '', extracted_text)\n",
    "\n",
    "output_file_path = 'data/ISTQB_CTFL_Syllabus-v4.0_v05.txt' \n",
    "with open(output_file_path, 'w', encoding='utf-8') as out_file:\n",
    "    # Writing the extracted text to the output file\n",
    "    out_file.write(extracted_text)\n",
    "output_string.close()\n",
    " \n",
    "# Printing message to indicate that the text has been saved to the file\n",
    "print(f\"Extracted text for 'The Certified Tester Foundation Level in Software Testing' pre processed version 0.5 saved to '{output_file_path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pattern = r'[0-9]'\n",
    "\n",
    "# Match all digits in the string and replace them with an empty string\n",
    "#extracted_text = re.sub(pattern, '', extracted_text)\n",
    "\n",
    "#extracted_text = ''.join((x for x in extracted_text if not x.isdigit()))\n",
    "\n",
    "\n",
    "#output_file_path = 'data/ISTQB_CTFL_Syllabus-v4.0_v06.txt'\n",
    "#with open(output_file_path, 'w', encoding='utf-8') as out_file:\n",
    "    # Writing the extracted text to the output file\n",
    "#    out_file.write(extracted_text)\n",
    "# \n",
    "# Closing the stream\n",
    "#output_string.close()\n",
    "# \n",
    "# Printing message to indicate that the text has been saved to the file\n",
    "#print(f\"Extracted text for 'The Certified Tester Foundation Level in Software Testing' pre processed version 0.6 saved to '{output_file_path}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# # Load the language model\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# # Your text\n",
    "# text = extracted_text\n",
    "\n",
    "# # Process the text with SpaCy\n",
    "# doc = nlp(text)\n",
    "\n",
    "# # Create a StringIO object to store the NER results\n",
    "# output_string = io.StringIO()\n",
    "\n",
    "# # Extract named entities and write them to the StringIO object\n",
    "# for ent in doc.ents:\n",
    "#     output_string.write(f\"Entity: {ent.text}, Type: {ent.label_}\\n\")\n",
    "\n",
    "# # Get the NER results as a string\n",
    "# ner_results = output_string.getvalue()\n",
    "\n",
    "# output_file_path = 'data/NER.txt'\n",
    "# with open(output_file_path, 'w', encoding='utf-8') as out_file:\n",
    "#     # Writing the extracted text to the output file\n",
    "#       out_file.write(ner_results)\n",
    "\n",
    "# # Closing the stream\n",
    "# output_string.close()\n",
    "\n",
    "# # Printing message to indicate that the text has been saved to the file\n",
    "# print(f\"Extracted NER list for 'The Certified Tester Foundation Level in Software Testing; {output_file_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point NER dict is saved into /data folder, edited manually and now let us import this file into stop_list StringIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Check the content of stop_list_stringio\n",
    "#content = stop_list_stringio.getvalue()\n",
    "#print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Markov Chain\n",
    "# Sample text (replace with your extracted_text)\n",
    "# Tokenize the text into words\n",
    "#tokens = nltk.word_tokenize(extracted_text)\n",
    "\n",
    "# Create a dictionary to store transition probabilities\n",
    "#transition_probabilities = {}\n",
    "\n",
    "# Build the transition probability matrix\n",
    "#for i in range(len(tokens) - 1):\n",
    "#    current_token = tokens[i]\n",
    "#    next_token = tokens[i + 1]\n",
    "    \n",
    "#    if current_token in transition_probabilities:\n",
    "#        transition_probabilities[current_token].append(next_token)\n",
    "#    else:\n",
    "#        transition_probabilities[current_token] = [next_token]\n",
    "\n",
    "# Start with an initial word\n",
    "#current_word = random.choice(tokens)\n",
    "\n",
    "# Generate a sentence of a certain length\n",
    "#generated_text = [current_word]\n",
    "#sentence_length = 10\n",
    "\n",
    "#for _ in range(sentence_length - 1):\n",
    "#    if current_word in transition_probabilities:\n",
    "#        next_word = random.choice(transition_probabilities[current_word])\n",
    "#        generated_text.append(next_word)\n",
    "#        current_word = next_word\n",
    "#    else:\n",
    "#        break\n",
    "\n",
    "# Join the generated words into a sentence\n",
    "#generated_sentence = \" \".join(generated_text)\n",
    "#print(generated_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted text for 'The Certified Tester Foundation Level in Software Testing' pre processed version 0.7 saved to 'data/ISTQB_CTFL_Syllabus-v4.0_v07.txt'\n"
     ]
    }
   ],
   "source": [
    "# remove chapter 4 beginning\n",
    "\n",
    "# Define the regular expression pattern for the text to remove\n",
    "pattern = r'4\\. Test Analysis and Design – 390 minutes.*?(K3) Use acceptance test-driven development (ATDD) to derive test cases'\n",
    "\n",
    "# Use re.sub to replace the matched text with a marker (e.g., 'REMOVED')\n",
    "extracted_text = re.sub(pattern, \"\", extracted_text, flags=re.DOTALL)\n",
    "\n",
    "# Define the phrase you want to remove\n",
    "phrase_to_remove = \"Learning Objectives for Chapter 4:\"\n",
    "\n",
    "# Replace the phrase with an empty string\n",
    "extracted_text = extracted_text.replace(phrase_to_remove, \"\")\n",
    "# \n",
    "output_file_path = 'data/ISTQB_CTFL_Syllabus-v4.0_v07.txt'\n",
    "with open(output_file_path, 'w', encoding='utf-8') as out_file:\n",
    "    # Writing the extracted text to the output file\n",
    "    out_file.write(extracted_text)\n",
    "# \n",
    "# Closing the stream\n",
    "output_string.close()\n",
    "# \n",
    "# Printing message to indicate that the text has been saved to the file\n",
    "print(f\"Extracted text for 'The Certified Tester Foundation Level in Software Testing' pre processed version 0.7 saved to '{output_file_path}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted text for 'The Certified Tester Foundation Level in Software Testing' pre processed version 0.8 saved to 'data/ISTQB_CTFL_Syllabus-v4.0_v08.txt'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# remove chapter 4 beginning\n",
    "# Define the regular expression pattern to remove the desired text\n",
    "pattern = r'4\\.1 Test Techniques Overview.*?4\\.5\\.3 \\(K3\\) Use acceptance test-driven development \\(ATDD\\) to derive test cases'\n",
    "\n",
    "# Use re.sub to replace the matched text with an empty string\n",
    "extracted_text = re.sub(pattern, '', extracted_text, flags=re.DOTALL)\n",
    "\n",
    "\n",
    "\n",
    "output_file_path = 'data/ISTQB_CTFL_Syllabus-v4.0_v08.txt'\n",
    "with open(output_file_path, 'w', encoding='utf-8') as out_file:\n",
    "    # Writing the extracted text to the output file\n",
    "    out_file.write(extracted_text)\n",
    "# \n",
    "# Closing the stream\n",
    "output_string.close()\n",
    "# \n",
    "# Printing message to indicate that the text has been saved to the file\n",
    "print(f\"Extracted text for 'The Certified Tester Foundation Level in Software Testing' pre processed version 0.8 saved to '{output_file_path}'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted text for 'The Certified Tester Foundation Level in Software Testing' pre processed version 0.9 saved to 'data/ISTQB_CTFL_Syllabus-v4.0_v09.txt'\n"
     ]
    }
   ],
   "source": [
    "# remove chapter 3 beginning\n",
    "# Define the regular expression pattern for the text to remove\n",
    "pattern = r'3\\. Static Testing – 80 minutes.*?FL-3\\.2\\.5 \\(K1\\) Recall the factors that contribute to a successful review'\n",
    "\n",
    "# Use re.sub to replace the matched text with a marker (e.g., 'REMOVED')\n",
    "extracted_text = re.sub(pattern, \"\", extracted_text, flags=re.DOTALL)\n",
    "\n",
    "# Define the phrase you want to remove\n",
    "phrase_to_remove = \"Learning Objectives for Chapter 4:\"\n",
    "\n",
    "# Replace the phrase with an empty string\n",
    "extracted_text = extracted_text.replace(phrase_to_remove, \"\")\n",
    "# \n",
    "output_file_path = 'data/ISTQB_CTFL_Syllabus-v4.0_v09.txt'\n",
    "with open(output_file_path, 'w', encoding='utf-8') as out_file:\n",
    "    # Writing the extracted text to the output file\n",
    "    out_file.write(extracted_text)\n",
    "# \n",
    "# Closing the stream\n",
    "output_string.close()\n",
    "# \n",
    "# Printing message to indicate that the text has been saved to the file\n",
    "print(f\"Extracted text for 'The Certified Tester Foundation Level in Software Testing' pre processed version 0.9 saved to '{output_file_path}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted text for 'The Certified Tester Foundation Level in Software Testing' pre processed version 0.10 saved to 'data/ISTQB_CTFL_Syllabus-v4.0_v10.txt'\n"
     ]
    }
   ],
   "source": [
    "# remove chapter 2 beginning\n",
    "\n",
    "# Define the regular expression pattern for the text to remove\n",
    "pattern = r'2\\. Testing Throughout the Software Development Lifecycle.*?FL-2\\.3\\.1 \\(K2\\) Summarize maintenance testing and its triggers'\n",
    "# Use re.sub to replace the matched text with a marker (e.g., 'REMOVED')\n",
    "extracted_text = re.sub(pattern, \"\", extracted_text, flags=re.DOTALL)\n",
    "\n",
    "# Define the phrase you want to remove\n",
    "phrase_to_remove = \"Learning Objectives for Chapter 4:\"\n",
    "\n",
    "# Replace the phrase with an empty string\n",
    "extracted_text = extracted_text.replace(phrase_to_remove, \"\")\n",
    "# \n",
    "output_file_path = 'data/ISTQB_CTFL_Syllabus-v4.0_v10.txt'\n",
    "with open(output_file_path, 'w', encoding='utf-8') as out_file:\n",
    "    # Writing the extracted text to the output file\n",
    "    out_file.write(extracted_text)\n",
    "# \n",
    "# Closing the stream\n",
    "output_string.close()\n",
    "# \n",
    "# Printing message to indicate that the text has been saved to the file\n",
    "print(f\"Extracted text for 'The Certified Tester Foundation Level in Software Testing' pre processed version 0.10 saved to '{output_file_path}'\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted text for 'The Certified Tester Foundation Level in Software Testing' pre processed version 0.11 saved to 'data/ISTQB_CTFL_Syllabus-v4.0_v11.txt'\n"
     ]
    }
   ],
   "source": [
    "# remove chapter 2 beginning\n",
    "\n",
    "# Define the regular expression pattern for the text to remove\n",
    "\n",
    "pattern = r'5\\. Managing the Test Activities – 335 minutes.*?FL-5\\.5\\.1 \\(K3\\) Prepare a defect report'\n",
    "# Use re.sub to replace the matched text with a marker (e.g., 'REMOVED')\n",
    "extracted_text = re.sub(pattern, \"\", extracted_text, flags=re.DOTALL)\n",
    "\n",
    "# Define the phrase you want to remove\n",
    "phrase_to_remove = \"Learning Objectives for Chapter 4:\"\n",
    "\n",
    "# Replace the phrase with an empty string\n",
    "extracted_text = extracted_text.replace(phrase_to_remove, \"\")\n",
    "# \n",
    "output_file_path = 'data/ISTQB_CTFL_Syllabus-v4.0_v11.txt'\n",
    "with open(output_file_path, 'w', encoding='utf-8') as out_file:\n",
    "    # Writing the extracted text to the output file\n",
    "    out_file.write(extracted_text)\n",
    "# \n",
    "# Closing the stream\n",
    "output_string.close()\n",
    "# \n",
    "# Printing message to indicate that the text has been saved to the file\n",
    "print(f\"Extracted text for 'The Certified Tester Foundation Level in Software Testing' pre processed version 0.11 saved to '{output_file_path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#pattern = r'\\d+\\.\\d+\\.\\d+\\.'\n",
    "#matches = re.findall(pattern, extracted_text)\n",
    "\n",
    "#for match in matches:\n",
    "#    match_without_dot = match[:-1]  # Remove the last dot\n",
    "#    print(match_without_dot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted text for 'The Certified Tester Foundation Level in Software Testing' pre processed version 0.12 saved to 'data/ISTQB_CTFL_Syllabus-v4.0_v12.txt'\n"
     ]
    }
   ],
   "source": [
    "# Remove bullet points using regular expressions\n",
    "extracted_text = re.sub(r'•', '', extracted_text)\n",
    "\n",
    "output_file_path = 'data/ISTQB_CTFL_Syllabus-v4.0_v12.txt'\n",
    "with open(output_file_path, 'w', encoding='utf-8') as out_file:\n",
    "    # Writing the extracted text to the output file\n",
    "    out_file.write(extracted_text)\n",
    "# \n",
    "# Closing the stream\n",
    "output_string.close()\n",
    "# \n",
    "# Printing message to indicate that the text has been saved to the file\n",
    "print(f\"Extracted text for 'The Certified Tester Foundation Level in Software Testing' pre processed version 0.12 saved to '{output_file_path}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a regular expression pattern to match section titles\n",
    "#section_pattern = r'\\d+\\.\\d+\\.\\d+\\.'\n",
    "\n",
    "# using combined reg. exp to extract 1.1.1. and 1.2.\n",
    "#section_pattern_3d = r'\\d+\\.\\d+\\.\\d+\\.'  # Pattern for \"1.1.1.\"\n",
    "#section_pattern_2d = r'\\d+\\.\\d+\\.'    # Pattern for \"1.2.\"\n",
    "#combined_pattern = f\"({section_pattern_3d}|{section_pattern_2d})\"#\n",
    "\n",
    "## Using re.finditer to find all section titles and their starting positions\n",
    "#section_matches = re.finditer(combined_pattern, extracted_text)#\n",
    "\n",
    "## Create lists to store sections\n",
    "#sections = []#\n",
    "\n",
    "## Iterate through section matches\n",
    "#for match in section_matches:\n",
    "#    start_pos = match.start()\n",
    "#    end_pos = (\n",
    "#        match.end()\n",
    "#        if match.end() < len(extracted_text)\n",
    "#        else len(extracted_text)\n",
    "#    )\n",
    "#    section_title = match.group().strip()\n",
    "#    \n",
    "#    # Remove the last dot from the section title\n",
    "#    section_title = section_title[:-1]  # Remove the last dot\n",
    "#    \n",
    "#    try:\n",
    "#        # Find the corresponding section content based on section title position\n",
    "#        next_match = next(section_matches)\n",
    "#        content_start = end_pos\n",
    "#        content_end = (\n",
    "#            next_match.start()\n",
    "#            if content_start < len(extracted_text)\n",
    "#            else len(extracted_text)\n",
    "#        )\n",
    "#        section_content = extracted_text[content_start:content_end].strip()\n",
    "#    except StopIteration:\n",
    "#        # Handle the case when there are no more matches\n",
    "#        section_content = extracted_text[end_pos:].strip()\n",
    "#    \n",
    "#    sections.append((section_title, section_content))#\n",
    "\n",
    "## Print the extracted sections\n",
    "#if sections:\n",
    "#    for section in sections:\n",
    "#        print(\"Section Title:\", section[0])\n",
    "#        print(\"Section Content:\", section[1])\n",
    "#        print(\"-\" * 40)\n",
    "#else:\n",
    "#    print(\"No sections found in the text, you did something wrong check once more\")#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(sections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding name?\n",
    "\n",
    "#text = \"1.1.2. Testing and Debugging\\nTesting and debugging are separate activities. Testing can trigger failures that are caused by defects in the software (dynamic testing) or can directly find defects in the test object (static testing). When dynamic testing (see chapter 4) triggers a failure, debugging is concerned with finding causes of this failure (defects), analyzing these causes, and eliminating the\"\n",
    "\n",
    "# Define a regular expression pattern to match section titles and names\n",
    "# section_pattern = r'(\\d+\\.\\d+\\.\\d+\\.\\s[^\\n]+)'\n",
    "# \n",
    "#Using re.finditer to find all section titles and their starting positions\n",
    "# section_matches = re.finditer(section_pattern, extracted_text)\n",
    "# \n",
    "#Create lists to store sections\n",
    "# sections = []\n",
    "# \n",
    "#Iterate through section matches\n",
    "# for match in section_matches:\n",
    "    # section_info = match.group(1).strip()\n",
    "# \n",
    " #   Find the corresponding section content based on section title position\n",
    "    # start_pos = match.end()\n",
    "    # end_pos = (\n",
    "        # next(section_matches, None)\n",
    "        # if start_pos < len(text)\n",
    "        # else None\n",
    "    # )\n",
    "    # \n",
    "    # if end_pos:\n",
    "        # section_content = text[start_pos:end_pos.start()].strip()\n",
    "    # else:\n",
    "        # section_content = text[start_pos:].strip()\n",
    "# \n",
    "    # sections.append((section_info, section_content))\n",
    "# \n",
    "#Print the extracted sections\n",
    "# if sections:\n",
    "    # for section in sections:\n",
    "        # print(\"Section Info:\", section[0])\n",
    "        # print(\"Section Content:\", section[1])\n",
    "        # print(\"-\" * 40)\n",
    "# else:\n",
    "    # print(\"No sections found in the text.\")\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DRAFT\n",
    "\n",
    "# Sample text (replace this with your actual text)\n",
    "\n",
    "# Define a regular expression pattern to match section titles\n",
    "# section_pattern = r'\\b\\d+\\.\\d+(?:\\.\\d+)?(?=\\s)'\n",
    "# \n",
    "#Using re.finditer to find all section titles and their starting positions\n",
    "# section_matches = re.finditer(section_pattern, extracted_text)\n",
    "# \n",
    "#Create lists to store sections\n",
    "# sections = []\n",
    "# \n",
    "#Iterate through section matches\n",
    "# for match in section_matches:\n",
    "    # start_pos = match.start()\n",
    "    # end_pos = (\n",
    "        # match.end()\n",
    "        # if match.end() < len(extracted_text)\n",
    "        # else len(extracted_text)\n",
    "    # )\n",
    "    # section_title = match.group().strip()\n",
    "    # \n",
    "#   Remove the last dot from the section title\n",
    "    # section_title = section_title[:-1]  # Remove the last dot\n",
    "    # \n",
    "#    Find the corresponding section content based on section title position\n",
    "    # content_start = end_pos\n",
    "    # content_end = (\n",
    "        # next(section_matches).start()\n",
    "        # if content_start < len(extracted_text)\n",
    "        # else len(extracted_text)\n",
    "    # )\n",
    "    # section_content = extracted_text[content_start:content_end].strip()\n",
    "    # \n",
    "    # sections.append((section_title, section_content))\n",
    "# \n",
    "#Print the extracted sections\n",
    "# if sections:\n",
    "    # for section in sections:\n",
    "        # print(\"Section Title:\", section[0])\n",
    "        # print(\"Section Content:\", section[1])\n",
    "        # print(\"-\" * 40)\n",
    "# else:\n",
    "    # print(\"No sections found in the text.\")\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!Base Modeling!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1.1. What is Testing?': '', '1.1.1. Test Objectives': '', '1.1.2. Testing and Debugging': '', '1.2. Why is Testing Necessary?': '', '1.2.1. Testing’s Contributions to Success': '', '1.2.2. Testing and Quality Assurance (QA)': '', '1.2.3. Errors, Defects, Failures, and Root Causes': '', '1.3. Testing Principles': '', '1.4. Test Activities, Testware and Test Roles': '', '1.4.1. Test Activities and Tasks': '', '1.4.2. Test Process in Context': '', '1.4.3. Testware': '', '1.4.4. Traceability between the Test Basis and Testware': '', '1.4.5. Roles in Testing': '', '1.5. Essential Skills and Good Practices in Testing': '', '1.5.1. Generic Skills Required for Testing': '', '1.5.2. Whole Team Approach': '', '1.5.3. Independence of Testing': '', '2.1. Testing in the Context of a Software Development Lifecycle': '', '2.1.1. Impact of the Software Development Lifecycle on Testing': '', '2.1.2. Software Development Lifecycle and Good Testing Practices': '', '2.1.3. Testing as a Driver for Software Development': '', '2.1.4. DevOps and Testing': '', '2.1.5. Shift-Left Approach': '', '2.1.6. Retrospectives and Process Improvement': '', '2.2. Test Levels and Test Types': '', '2.2.1. Test Levels': '', '2.2.2. Test Types': '', '2.2.3. Confirmation Testing and Regression Testing': '', '2.3. Maintenance Testing': '', '3.1. Static Testing Basics': '', '3.1.1. Work Products Examinable by Static Testing': '', '3.1.2. Value of Static Testing': '', '3.1.3. Differences between Static Testing and Dynamic Testing': '', '3.2.1. Benefits of Early and Frequent Stakeholder Feedback': '', '3.2.2. Review Process Activities': '', '3.2.3. Roles and Responsibilities in Reviews': '', '3.2.4. Review Types': '', '3.2.5. Success Factors for Reviews': '', '4.1. Test Techniques Overview': '', '4.2. Black-Box Test Techniques': '', '4.2.1. Equivalence Partitioning': '', '4.2.2. Boundary Value Analysis': '', '4.2.3. Decision Table Testing': '', '4.2.4. State Transition Testing': '', '4.3. White-Box Test Techniques': '', '4.3.1. Statement Testing and Statement Coverage': '', '4.3.2. Branch Testing and Branch Coverage': '', '4.3.3. The Value of White-box Testing': '', '4.4. Experience-based Test Techniques': '', '4.4.1. Error Guessing': '', '4.4.2. Exploratory Testing': '', '4.4.3. Checklist-Based Testing': '', '4.5. Collaboration-based Test Approaches': '', '4.5.1. Collaborative User Story Writing': '', '4.5.2. Acceptance Criteria': '', '4.5.3. Acceptance Test-driven Development (ATDD)': '', '5.1.1. Purpose and Content of a Test Plan': '', \"5.1.2. Tester's Contribution to Iteration and Release Planning\": '', '5.1.3. Entry Criteria and Exit Criteria': '', '5.1.4. Estimation Techniques': '', '5.1.5. Test Case Prioritization': '', '5.1.6. Test Pyramid': '', '5.1.7. Testing Quadrants': '', '5.2. Risk Management': '', '5.2.1. Risk Definition and Risk Attributes': '', '5.2.2. Project Risks and Product Risks': '', '5.2.3. Product Risk Analysis': '', '5.2.4. Product Risk Control': '', '5.3. Test Monitoring, Test Control and Test Completion': '', '5.3.1. Metrics used in Testing': '', '5.3.2. Purpose, Content and Audience for Test Reports': '', '5.3.3. Communicating the Status of Testing': '', '5.4. Configuration Management': '', '5.5. Defect Management': '', '6.1. Tool Support for Testing': '', '6.2. Benefits and Risks of Test Automation': ''}\n"
     ]
    }
   ],
   "source": [
    "# make a dictionary of key, which section_title and value, which is summary of each section title\n",
    "# Sample list of section titles\n",
    "section_titles = [\n",
    "    \"1.1. What is Testing?\",\n",
    "    \"1.1.1. Test Objectives\",\n",
    "    \"1.1.2. Testing and Debugging\",\n",
    "    \"1.2. Why is Testing Necessary?\",\n",
    "    \"1.2.1. Testing’s Contributions to Success\",\n",
    "    \"1.2.2. Testing and Quality Assurance (QA)\",\n",
    "    \"1.2.3. Errors, Defects, Failures, and Root Causes\",\n",
    "    \"1.3. Testing Principles\",\n",
    "    \"1.4. Test Activities, Testware and Test Roles\",\n",
    "    \"1.4.1. Test Activities and Tasks\",\n",
    "    \"1.4.2. Test Process in Context\",\n",
    "    \"1.4.3. Testware\",\n",
    "    \"1.4.4. Traceability between the Test Basis and Testware\",\n",
    "    \"1.4.5. Roles in Testing\",\n",
    "    \"1.5. Essential Skills and Good Practices in Testing\",\n",
    "    \"1.5.1. Generic Skills Required for Testing\",\n",
    "    \"1.5.2. Whole Team Approach\",\n",
    "    \"1.5.3. Independence of Testing\",\n",
    "    \"2.1. Testing in the Context of a Software Development Lifecycle\",\n",
    "    \"2.1.1. Impact of the Software Development Lifecycle on Testing\",\n",
    "    \"2.1.2. Software Development Lifecycle and Good Testing Practices\",\n",
    "    \"2.1.3. Testing as a Driver for Software Development\",\n",
    "    \"2.1.4. DevOps and Testing\",\n",
    "    \"2.1.5. Shift-Left Approach\",\n",
    "    \"2.1.6. Retrospectives and Process Improvement\",\n",
    "    \"2.2. Test Levels and Test Types\",\n",
    "    \"2.2.1. Test Levels\",\n",
    "    \"2.2.2. Test Types\",\n",
    "    \"2.2.3. Confirmation Testing and Regression Testing\",\n",
    "    \"2.3. Maintenance Testing\",\n",
    "    \"3.1. Static Testing Basics\",\n",
    "    \"3.1.1. Work Products Examinable by Static Testing\",\n",
    "    \"3.1.2. Value of Static Testing\",\n",
    "    \"3.1.3. Differences between Static Testing and Dynamic Testing\",\n",
    "    \"3.2.1. Benefits of Early and Frequent Stakeholder Feedback\",\n",
    "    \"3.2.2. Review Process Activities\",\n",
    "    \"3.2.3. Roles and Responsibilities in Reviews\",\n",
    "    \"3.2.4. Review Types\",\n",
    "    \"3.2.5. Success Factors for Reviews\",\n",
    "    \"4.1. Test Techniques Overview\",\n",
    "    \"4.2. Black-Box Test Techniques\",\n",
    "    \"4.2.1. Equivalence Partitioning\",\n",
    "    \"4.2.2. Boundary Value Analysis\",\n",
    "    \"4.2.3. Decision Table Testing\",\n",
    "    \"4.2.4. State Transition Testing\",\n",
    "    \"4.3. White-Box Test Techniques\",\n",
    "    \"4.3.1. Statement Testing and Statement Coverage\",\n",
    "    \"4.3.2. Branch Testing and Branch Coverage\",\n",
    "    \"4.3.3. The Value of White-box Testing\",\n",
    "    \"4.4. Experience-based Test Techniques\",\n",
    "    \"4.4.1. Error Guessing\",\n",
    "    \"4.4.2. Exploratory Testing\",\n",
    "    \"4.4.3. Checklist-Based Testing\",\n",
    "    \"4.5. Collaboration-based Test Approaches\",\n",
    "    \"4.5.1. Collaborative User Story Writing\",\n",
    "    \"4.5.2. Acceptance Criteria\",\n",
    "    \"4.5.3. Acceptance Test-driven Development (ATDD)\",\n",
    "    \"5.1.1. Purpose and Content of a Test Plan\",\n",
    "    \"5.1.2. Tester's Contribution to Iteration and Release Planning\",\n",
    "    \"5.1.3. Entry Criteria and Exit Criteria\",\n",
    "    \"5.1.4. Estimation Techniques\",\n",
    "    \"5.1.5. Test Case Prioritization\",\n",
    "    \"5.1.6. Test Pyramid\",\n",
    "    \"5.1.7. Testing Quadrants\",\n",
    "    \"5.2. Risk Management\",\n",
    "    \"5.2.1. Risk Definition and Risk Attributes\",\n",
    "    \"5.2.2. Project Risks and Product Risks\",\n",
    "    \"5.2.3. Product Risk Analysis\",\n",
    "    \"5.2.4. Product Risk Control\",\n",
    "    \"5.3. Test Monitoring, Test Control and Test Completion\",\n",
    "    \"5.3.1. Metrics used in Testing\",\n",
    "    \"5.3.2. Purpose, Content and Audience for Test Reports\",\n",
    "    \"5.3.3. Communicating the Status of Testing\",\n",
    "    \"5.4. Configuration Management\",\n",
    "    \"5.5. Defect Management\",\n",
    "    \"6.1. Tool Support for Testing\",\n",
    "    \"6.2. Benefits and Risks of Test Automation\",\n",
    "    # Add more section titles here\n",
    "]\n",
    "\n",
    "# Create an empty dictionary with section titles as keys\n",
    "section_dict = {title: \"\" for title in section_titles}\n",
    "\n",
    "# Print the dictionary to verify the structure\n",
    "print(section_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77\n"
     ]
    }
   ],
   "source": [
    "print(len(section_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Elena\\github\\Quiz_Generator_Markov_Chain\\extraction.ipynb Cell 38\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Elena/github/Quiz_Generator_Markov_Chain/extraction.ipynb#Y112sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m keywords \u001b[39m=\u001b[39m []\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Elena/github/Quiz_Generator_Markov_Chain/extraction.ipynb#Y112sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_keywords):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Elena/github/Quiz_Generator_Markov_Chain/extraction.ipynb#Y112sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     output \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mgenerate(inputs[\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m], max_length\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(keywords) \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m, num_return_sequences\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, no_repeat_ngram_size\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Elena/github/Quiz_Generator_Markov_Chain/extraction.ipynb#Y112sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     keyword \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mdecode(output[\u001b[39m0\u001b[39m], skip_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Elena/github/Quiz_Generator_Markov_Chain/extraction.ipynb#Y112sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     keywords\u001b[39m.\u001b[39mappend(keyword)\n",
      "File \u001b[1;32mc:\\Users\\Elena\\anaconda3\\envs\\qgmc\\Lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Elena\\anaconda3\\envs\\qgmc\\Lib\\site-packages\\transformers\\generation\\utils.py:1602\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[0;32m   1585\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39massisted_decoding(\n\u001b[0;32m   1586\u001b[0m         input_ids,\n\u001b[0;32m   1587\u001b[0m         assistant_model\u001b[39m=\u001b[39massistant_model,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1598\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   1599\u001b[0m     )\n\u001b[0;32m   1600\u001b[0m \u001b[39mif\u001b[39;00m generation_mode \u001b[39m==\u001b[39m GenerationMode\u001b[39m.\u001b[39mGREEDY_SEARCH:\n\u001b[0;32m   1601\u001b[0m     \u001b[39m# 11. run greedy search\u001b[39;00m\n\u001b[1;32m-> 1602\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgreedy_search(\n\u001b[0;32m   1603\u001b[0m         input_ids,\n\u001b[0;32m   1604\u001b[0m         logits_processor\u001b[39m=\u001b[39mlogits_processor,\n\u001b[0;32m   1605\u001b[0m         stopping_criteria\u001b[39m=\u001b[39mstopping_criteria,\n\u001b[0;32m   1606\u001b[0m         pad_token_id\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mpad_token_id,\n\u001b[0;32m   1607\u001b[0m         eos_token_id\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39meos_token_id,\n\u001b[0;32m   1608\u001b[0m         output_scores\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39moutput_scores,\n\u001b[0;32m   1609\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mreturn_dict_in_generate,\n\u001b[0;32m   1610\u001b[0m         synced_gpus\u001b[39m=\u001b[39msynced_gpus,\n\u001b[0;32m   1611\u001b[0m         streamer\u001b[39m=\u001b[39mstreamer,\n\u001b[0;32m   1612\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   1613\u001b[0m     )\n\u001b[0;32m   1615\u001b[0m \u001b[39melif\u001b[39;00m generation_mode \u001b[39m==\u001b[39m GenerationMode\u001b[39m.\u001b[39mCONTRASTIVE_SEARCH:\n\u001b[0;32m   1616\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m model_kwargs[\u001b[39m\"\u001b[39m\u001b[39muse_cache\u001b[39m\u001b[39m\"\u001b[39m]:\n",
      "File \u001b[1;32mc:\\Users\\Elena\\anaconda3\\envs\\qgmc\\Lib\\site-packages\\transformers\\generation\\utils.py:2450\u001b[0m, in \u001b[0;36mGenerationMixin.greedy_search\u001b[1;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[0;32m   2447\u001b[0m model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)\n\u001b[0;32m   2449\u001b[0m \u001b[39m# forward pass to get next token\u001b[39;00m\n\u001b[1;32m-> 2450\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m(\n\u001b[0;32m   2451\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_inputs,\n\u001b[0;32m   2452\u001b[0m     return_dict\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m   2453\u001b[0m     output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[0;32m   2454\u001b[0m     output_hidden_states\u001b[39m=\u001b[39moutput_hidden_states,\n\u001b[0;32m   2455\u001b[0m )\n\u001b[0;32m   2457\u001b[0m \u001b[39mif\u001b[39;00m synced_gpus \u001b[39mand\u001b[39;00m this_peer_finished:\n\u001b[0;32m   2458\u001b[0m     \u001b[39mcontinue\u001b[39;00m  \u001b[39m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Elena\\anaconda3\\envs\\qgmc\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Elena\\anaconda3\\envs\\qgmc\\Lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:1746\u001b[0m, in \u001b[0;36mT5ForConditionalGeneration.forward\u001b[1;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1743\u001b[0m         decoder_attention_mask \u001b[39m=\u001b[39m decoder_attention_mask\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder\u001b[39m.\u001b[39mfirst_device)\n\u001b[0;32m   1745\u001b[0m \u001b[39m# Decode\u001b[39;00m\n\u001b[1;32m-> 1746\u001b[0m decoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder(\n\u001b[0;32m   1747\u001b[0m     input_ids\u001b[39m=\u001b[39mdecoder_input_ids,\n\u001b[0;32m   1748\u001b[0m     attention_mask\u001b[39m=\u001b[39mdecoder_attention_mask,\n\u001b[0;32m   1749\u001b[0m     inputs_embeds\u001b[39m=\u001b[39mdecoder_inputs_embeds,\n\u001b[0;32m   1750\u001b[0m     past_key_values\u001b[39m=\u001b[39mpast_key_values,\n\u001b[0;32m   1751\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39mhidden_states,\n\u001b[0;32m   1752\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39mattention_mask,\n\u001b[0;32m   1753\u001b[0m     head_mask\u001b[39m=\u001b[39mdecoder_head_mask,\n\u001b[0;32m   1754\u001b[0m     cross_attn_head_mask\u001b[39m=\u001b[39mcross_attn_head_mask,\n\u001b[0;32m   1755\u001b[0m     use_cache\u001b[39m=\u001b[39muse_cache,\n\u001b[0;32m   1756\u001b[0m     output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[0;32m   1757\u001b[0m     output_hidden_states\u001b[39m=\u001b[39moutput_hidden_states,\n\u001b[0;32m   1758\u001b[0m     return_dict\u001b[39m=\u001b[39mreturn_dict,\n\u001b[0;32m   1759\u001b[0m )\n\u001b[0;32m   1761\u001b[0m sequence_output \u001b[39m=\u001b[39m decoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m   1763\u001b[0m \u001b[39m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Elena\\anaconda3\\envs\\qgmc\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Elena\\anaconda3\\envs\\qgmc\\Lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:1123\u001b[0m, in \u001b[0;36mT5Stack.forward\u001b[1;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1110\u001b[0m     layer_outputs \u001b[39m=\u001b[39m checkpoint(\n\u001b[0;32m   1111\u001b[0m         create_custom_forward(layer_module),\n\u001b[0;32m   1112\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1120\u001b[0m         \u001b[39mNone\u001b[39;00m,  \u001b[39m# past_key_value is always None with gradient checkpointing\u001b[39;00m\n\u001b[0;32m   1121\u001b[0m     )\n\u001b[0;32m   1122\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1123\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[0;32m   1124\u001b[0m         hidden_states,\n\u001b[0;32m   1125\u001b[0m         attention_mask\u001b[39m=\u001b[39mextended_attention_mask,\n\u001b[0;32m   1126\u001b[0m         position_bias\u001b[39m=\u001b[39mposition_bias,\n\u001b[0;32m   1127\u001b[0m         encoder_hidden_states\u001b[39m=\u001b[39mencoder_hidden_states,\n\u001b[0;32m   1128\u001b[0m         encoder_attention_mask\u001b[39m=\u001b[39mencoder_extended_attention_mask,\n\u001b[0;32m   1129\u001b[0m         encoder_decoder_position_bias\u001b[39m=\u001b[39mencoder_decoder_position_bias,\n\u001b[0;32m   1130\u001b[0m         layer_head_mask\u001b[39m=\u001b[39mlayer_head_mask,\n\u001b[0;32m   1131\u001b[0m         cross_attn_layer_head_mask\u001b[39m=\u001b[39mcross_attn_layer_head_mask,\n\u001b[0;32m   1132\u001b[0m         past_key_value\u001b[39m=\u001b[39mpast_key_value,\n\u001b[0;32m   1133\u001b[0m         use_cache\u001b[39m=\u001b[39muse_cache,\n\u001b[0;32m   1134\u001b[0m         output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[0;32m   1135\u001b[0m     )\n\u001b[0;32m   1137\u001b[0m \u001b[39m# layer_outputs is a tuple with:\u001b[39;00m\n\u001b[0;32m   1138\u001b[0m \u001b[39m# hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\u001b[39;00m\n\u001b[0;32m   1139\u001b[0m \u001b[39mif\u001b[39;00m use_cache \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Elena\\anaconda3\\envs\\qgmc\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Elena\\anaconda3\\envs\\qgmc\\Lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:695\u001b[0m, in \u001b[0;36mT5Block.forward\u001b[1;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict)\u001b[0m\n\u001b[0;32m    692\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    693\u001b[0m     self_attn_past_key_value, cross_attn_past_key_value \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 695\u001b[0m self_attention_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer[\u001b[39m0\u001b[39m](\n\u001b[0;32m    696\u001b[0m     hidden_states,\n\u001b[0;32m    697\u001b[0m     attention_mask\u001b[39m=\u001b[39mattention_mask,\n\u001b[0;32m    698\u001b[0m     position_bias\u001b[39m=\u001b[39mposition_bias,\n\u001b[0;32m    699\u001b[0m     layer_head_mask\u001b[39m=\u001b[39mlayer_head_mask,\n\u001b[0;32m    700\u001b[0m     past_key_value\u001b[39m=\u001b[39mself_attn_past_key_value,\n\u001b[0;32m    701\u001b[0m     use_cache\u001b[39m=\u001b[39muse_cache,\n\u001b[0;32m    702\u001b[0m     output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[0;32m    703\u001b[0m )\n\u001b[0;32m    704\u001b[0m hidden_states, present_key_value_state \u001b[39m=\u001b[39m self_attention_outputs[:\u001b[39m2\u001b[39m]\n\u001b[0;32m    705\u001b[0m attention_outputs \u001b[39m=\u001b[39m self_attention_outputs[\u001b[39m2\u001b[39m:]  \u001b[39m# Keep self-attention outputs and relative position weights\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Elena\\anaconda3\\envs\\qgmc\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Elena\\anaconda3\\envs\\qgmc\\Lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:601\u001b[0m, in \u001b[0;36mT5LayerSelfAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, position_bias, layer_head_mask, past_key_value, use_cache, output_attentions)\u001b[0m\n\u001b[0;32m    591\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[0;32m    592\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    593\u001b[0m     hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    599\u001b[0m     output_attentions\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    600\u001b[0m ):\n\u001b[1;32m--> 601\u001b[0m     normed_hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer_norm(hidden_states)\n\u001b[0;32m    602\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mSelfAttention(\n\u001b[0;32m    603\u001b[0m         normed_hidden_states,\n\u001b[0;32m    604\u001b[0m         mask\u001b[39m=\u001b[39mattention_mask,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    609\u001b[0m         output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[0;32m    610\u001b[0m     )\n\u001b[0;32m    611\u001b[0m     hidden_states \u001b[39m=\u001b[39m hidden_states \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(attention_output[\u001b[39m0\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\Elena\\anaconda3\\envs\\qgmc\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Elena\\anaconda3\\envs\\qgmc\\Lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:254\u001b[0m, in \u001b[0;36mT5LayerNorm.forward\u001b[1;34m(self, hidden_states)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, hidden_states):\n\u001b[0;32m    249\u001b[0m     \u001b[39m# T5 uses a layer_norm which only scales and doesn't shift, which is also known as Root Mean\u001b[39;00m\n\u001b[0;32m    250\u001b[0m     \u001b[39m# Square Layer Normalization https://arxiv.org/abs/1910.07467 thus varience is calculated\u001b[39;00m\n\u001b[0;32m    251\u001b[0m     \u001b[39m# w/o mean and there is no bias. Additionally we want to make sure that the accumulation for\u001b[39;00m\n\u001b[0;32m    252\u001b[0m     \u001b[39m# half-precision inputs is done in fp32\u001b[39;00m\n\u001b[1;32m--> 254\u001b[0m     variance \u001b[39m=\u001b[39m hidden_states\u001b[39m.\u001b[39mto(torch\u001b[39m.\u001b[39mfloat32)\u001b[39m.\u001b[39mpow(\u001b[39m2\u001b[39m)\u001b[39m.\u001b[39mmean(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, keepdim\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    255\u001b[0m     hidden_states \u001b[39m=\u001b[39m hidden_states \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39mrsqrt(variance \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvariance_epsilon)\n\u001b[0;32m    257\u001b[0m     \u001b[39m# convert into half-precision if necessary\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "# Load the T5 model and tokenizer\n",
    "model_name = \"t5-small\"\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Read the text from the StringIO file (replace with your own text)\n",
    "\n",
    "# Tokenize the text\n",
    "inputs = tokenizer(\"summarize: \" + extracted_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "\n",
    "# Generate keywords one at a time\n",
    "num_keywords = 500  # Adjustable!!!!\n",
    "keywords = []\n",
    "\n",
    "for _ in range(num_keywords):\n",
    "    output = model.generate(inputs[\"input_ids\"], max_length=len(keywords) + 1, num_return_sequences=1, no_repeat_ngram_size=2)\n",
    "    keyword = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    keywords.append(keyword)\n",
    "\n",
    "# Print the generated keywords\n",
    "print(\"Keywords:\", keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Elena\\github\\Quiz_Generator_Markov_Chain\\extraction.ipynb Cell 39\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Elena/github/Quiz_Generator_Markov_Chain/extraction.ipynb#Y113sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m paraphrases \u001b[39m=\u001b[39m []\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Elena/github/Quiz_Generator_Markov_Chain/extraction.ipynb#Y113sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_paraphrases):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Elena/github/Quiz_Generator_Markov_Chain/extraction.ipynb#Y113sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     output \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mgenerate(inputs[\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m], max_length\u001b[39m=\u001b[39m\u001b[39m512\u001b[39m, num_return_sequences\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, no_repeat_ngram_size\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Elena/github/Quiz_Generator_Markov_Chain/extraction.ipynb#Y113sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     paraphrase \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mdecode(output[\u001b[39m0\u001b[39m], skip_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Elena/github/Quiz_Generator_Markov_Chain/extraction.ipynb#Y113sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     paraphrases\u001b[39m.\u001b[39mappend(paraphrase)\n",
      "File \u001b[1;32mc:\\Users\\Elena\\anaconda3\\envs\\qgmc\\Lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Elena\\anaconda3\\envs\\qgmc\\Lib\\site-packages\\transformers\\generation\\utils.py:1602\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[0;32m   1585\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39massisted_decoding(\n\u001b[0;32m   1586\u001b[0m         input_ids,\n\u001b[0;32m   1587\u001b[0m         assistant_model\u001b[39m=\u001b[39massistant_model,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1598\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   1599\u001b[0m     )\n\u001b[0;32m   1600\u001b[0m \u001b[39mif\u001b[39;00m generation_mode \u001b[39m==\u001b[39m GenerationMode\u001b[39m.\u001b[39mGREEDY_SEARCH:\n\u001b[0;32m   1601\u001b[0m     \u001b[39m# 11. run greedy search\u001b[39;00m\n\u001b[1;32m-> 1602\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgreedy_search(\n\u001b[0;32m   1603\u001b[0m         input_ids,\n\u001b[0;32m   1604\u001b[0m         logits_processor\u001b[39m=\u001b[39mlogits_processor,\n\u001b[0;32m   1605\u001b[0m         stopping_criteria\u001b[39m=\u001b[39mstopping_criteria,\n\u001b[0;32m   1606\u001b[0m         pad_token_id\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mpad_token_id,\n\u001b[0;32m   1607\u001b[0m         eos_token_id\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39meos_token_id,\n\u001b[0;32m   1608\u001b[0m         output_scores\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39moutput_scores,\n\u001b[0;32m   1609\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mreturn_dict_in_generate,\n\u001b[0;32m   1610\u001b[0m         synced_gpus\u001b[39m=\u001b[39msynced_gpus,\n\u001b[0;32m   1611\u001b[0m         streamer\u001b[39m=\u001b[39mstreamer,\n\u001b[0;32m   1612\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   1613\u001b[0m     )\n\u001b[0;32m   1615\u001b[0m \u001b[39melif\u001b[39;00m generation_mode \u001b[39m==\u001b[39m GenerationMode\u001b[39m.\u001b[39mCONTRASTIVE_SEARCH:\n\u001b[0;32m   1616\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m model_kwargs[\u001b[39m\"\u001b[39m\u001b[39muse_cache\u001b[39m\u001b[39m\"\u001b[39m]:\n",
      "File \u001b[1;32mc:\\Users\\Elena\\anaconda3\\envs\\qgmc\\Lib\\site-packages\\transformers\\generation\\utils.py:2450\u001b[0m, in \u001b[0;36mGenerationMixin.greedy_search\u001b[1;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[0;32m   2447\u001b[0m model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)\n\u001b[0;32m   2449\u001b[0m \u001b[39m# forward pass to get next token\u001b[39;00m\n\u001b[1;32m-> 2450\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m(\n\u001b[0;32m   2451\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_inputs,\n\u001b[0;32m   2452\u001b[0m     return_dict\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m   2453\u001b[0m     output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[0;32m   2454\u001b[0m     output_hidden_states\u001b[39m=\u001b[39moutput_hidden_states,\n\u001b[0;32m   2455\u001b[0m )\n\u001b[0;32m   2457\u001b[0m \u001b[39mif\u001b[39;00m synced_gpus \u001b[39mand\u001b[39;00m this_peer_finished:\n\u001b[0;32m   2458\u001b[0m     \u001b[39mcontinue\u001b[39;00m  \u001b[39m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Elena\\anaconda3\\envs\\qgmc\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Elena\\anaconda3\\envs\\qgmc\\Lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:1746\u001b[0m, in \u001b[0;36mT5ForConditionalGeneration.forward\u001b[1;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1743\u001b[0m         decoder_attention_mask \u001b[39m=\u001b[39m decoder_attention_mask\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder\u001b[39m.\u001b[39mfirst_device)\n\u001b[0;32m   1745\u001b[0m \u001b[39m# Decode\u001b[39;00m\n\u001b[1;32m-> 1746\u001b[0m decoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder(\n\u001b[0;32m   1747\u001b[0m     input_ids\u001b[39m=\u001b[39mdecoder_input_ids,\n\u001b[0;32m   1748\u001b[0m     attention_mask\u001b[39m=\u001b[39mdecoder_attention_mask,\n\u001b[0;32m   1749\u001b[0m     inputs_embeds\u001b[39m=\u001b[39mdecoder_inputs_embeds,\n\u001b[0;32m   1750\u001b[0m     past_key_values\u001b[39m=\u001b[39mpast_key_values,\n\u001b[0;32m   1751\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39mhidden_states,\n\u001b[0;32m   1752\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39mattention_mask,\n\u001b[0;32m   1753\u001b[0m     head_mask\u001b[39m=\u001b[39mdecoder_head_mask,\n\u001b[0;32m   1754\u001b[0m     cross_attn_head_mask\u001b[39m=\u001b[39mcross_attn_head_mask,\n\u001b[0;32m   1755\u001b[0m     use_cache\u001b[39m=\u001b[39muse_cache,\n\u001b[0;32m   1756\u001b[0m     output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[0;32m   1757\u001b[0m     output_hidden_states\u001b[39m=\u001b[39moutput_hidden_states,\n\u001b[0;32m   1758\u001b[0m     return_dict\u001b[39m=\u001b[39mreturn_dict,\n\u001b[0;32m   1759\u001b[0m )\n\u001b[0;32m   1761\u001b[0m sequence_output \u001b[39m=\u001b[39m decoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m   1763\u001b[0m \u001b[39m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Elena\\anaconda3\\envs\\qgmc\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Elena\\anaconda3\\envs\\qgmc\\Lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:1123\u001b[0m, in \u001b[0;36mT5Stack.forward\u001b[1;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1110\u001b[0m     layer_outputs \u001b[39m=\u001b[39m checkpoint(\n\u001b[0;32m   1111\u001b[0m         create_custom_forward(layer_module),\n\u001b[0;32m   1112\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1120\u001b[0m         \u001b[39mNone\u001b[39;00m,  \u001b[39m# past_key_value is always None with gradient checkpointing\u001b[39;00m\n\u001b[0;32m   1121\u001b[0m     )\n\u001b[0;32m   1122\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1123\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[0;32m   1124\u001b[0m         hidden_states,\n\u001b[0;32m   1125\u001b[0m         attention_mask\u001b[39m=\u001b[39mextended_attention_mask,\n\u001b[0;32m   1126\u001b[0m         position_bias\u001b[39m=\u001b[39mposition_bias,\n\u001b[0;32m   1127\u001b[0m         encoder_hidden_states\u001b[39m=\u001b[39mencoder_hidden_states,\n\u001b[0;32m   1128\u001b[0m         encoder_attention_mask\u001b[39m=\u001b[39mencoder_extended_attention_mask,\n\u001b[0;32m   1129\u001b[0m         encoder_decoder_position_bias\u001b[39m=\u001b[39mencoder_decoder_position_bias,\n\u001b[0;32m   1130\u001b[0m         layer_head_mask\u001b[39m=\u001b[39mlayer_head_mask,\n\u001b[0;32m   1131\u001b[0m         cross_attn_layer_head_mask\u001b[39m=\u001b[39mcross_attn_layer_head_mask,\n\u001b[0;32m   1132\u001b[0m         past_key_value\u001b[39m=\u001b[39mpast_key_value,\n\u001b[0;32m   1133\u001b[0m         use_cache\u001b[39m=\u001b[39muse_cache,\n\u001b[0;32m   1134\u001b[0m         output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[0;32m   1135\u001b[0m     )\n\u001b[0;32m   1137\u001b[0m \u001b[39m# layer_outputs is a tuple with:\u001b[39;00m\n\u001b[0;32m   1138\u001b[0m \u001b[39m# hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\u001b[39;00m\n\u001b[0;32m   1139\u001b[0m \u001b[39mif\u001b[39;00m use_cache \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Elena\\anaconda3\\envs\\qgmc\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Elena\\anaconda3\\envs\\qgmc\\Lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:695\u001b[0m, in \u001b[0;36mT5Block.forward\u001b[1;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict)\u001b[0m\n\u001b[0;32m    692\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    693\u001b[0m     self_attn_past_key_value, cross_attn_past_key_value \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 695\u001b[0m self_attention_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer[\u001b[39m0\u001b[39m](\n\u001b[0;32m    696\u001b[0m     hidden_states,\n\u001b[0;32m    697\u001b[0m     attention_mask\u001b[39m=\u001b[39mattention_mask,\n\u001b[0;32m    698\u001b[0m     position_bias\u001b[39m=\u001b[39mposition_bias,\n\u001b[0;32m    699\u001b[0m     layer_head_mask\u001b[39m=\u001b[39mlayer_head_mask,\n\u001b[0;32m    700\u001b[0m     past_key_value\u001b[39m=\u001b[39mself_attn_past_key_value,\n\u001b[0;32m    701\u001b[0m     use_cache\u001b[39m=\u001b[39muse_cache,\n\u001b[0;32m    702\u001b[0m     output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[0;32m    703\u001b[0m )\n\u001b[0;32m    704\u001b[0m hidden_states, present_key_value_state \u001b[39m=\u001b[39m self_attention_outputs[:\u001b[39m2\u001b[39m]\n\u001b[0;32m    705\u001b[0m attention_outputs \u001b[39m=\u001b[39m self_attention_outputs[\u001b[39m2\u001b[39m:]  \u001b[39m# Keep self-attention outputs and relative position weights\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Elena\\anaconda3\\envs\\qgmc\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Elena\\anaconda3\\envs\\qgmc\\Lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:602\u001b[0m, in \u001b[0;36mT5LayerSelfAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, position_bias, layer_head_mask, past_key_value, use_cache, output_attentions)\u001b[0m\n\u001b[0;32m    591\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[0;32m    592\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    593\u001b[0m     hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    599\u001b[0m     output_attentions\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    600\u001b[0m ):\n\u001b[0;32m    601\u001b[0m     normed_hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer_norm(hidden_states)\n\u001b[1;32m--> 602\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mSelfAttention(\n\u001b[0;32m    603\u001b[0m         normed_hidden_states,\n\u001b[0;32m    604\u001b[0m         mask\u001b[39m=\u001b[39mattention_mask,\n\u001b[0;32m    605\u001b[0m         position_bias\u001b[39m=\u001b[39mposition_bias,\n\u001b[0;32m    606\u001b[0m         layer_head_mask\u001b[39m=\u001b[39mlayer_head_mask,\n\u001b[0;32m    607\u001b[0m         past_key_value\u001b[39m=\u001b[39mpast_key_value,\n\u001b[0;32m    608\u001b[0m         use_cache\u001b[39m=\u001b[39muse_cache,\n\u001b[0;32m    609\u001b[0m         output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[0;32m    610\u001b[0m     )\n\u001b[0;32m    611\u001b[0m     hidden_states \u001b[39m=\u001b[39m hidden_states \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(attention_output[\u001b[39m0\u001b[39m])\n\u001b[0;32m    612\u001b[0m     outputs \u001b[39m=\u001b[39m (hidden_states,) \u001b[39m+\u001b[39m attention_output[\u001b[39m1\u001b[39m:]  \u001b[39m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Elena\\anaconda3\\envs\\qgmc\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Elena\\anaconda3\\envs\\qgmc\\Lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:524\u001b[0m, in \u001b[0;36mT5Attention.forward\u001b[1;34m(self, hidden_states, mask, key_value_states, position_bias, past_key_value, layer_head_mask, query_length, use_cache, output_attentions)\u001b[0m\n\u001b[0;32m    521\u001b[0m query_states \u001b[39m=\u001b[39m shape(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mq(hidden_states))  \u001b[39m# (batch_size, n_heads, seq_length, dim_per_head)\u001b[39;00m\n\u001b[0;32m    523\u001b[0m \u001b[39m# get key/value states\u001b[39;00m\n\u001b[1;32m--> 524\u001b[0m key_states \u001b[39m=\u001b[39m project(\n\u001b[0;32m    525\u001b[0m     hidden_states, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mk, key_value_states, past_key_value[\u001b[39m0\u001b[39m] \u001b[39mif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    526\u001b[0m )\n\u001b[0;32m    527\u001b[0m value_states \u001b[39m=\u001b[39m project(\n\u001b[0;32m    528\u001b[0m     hidden_states, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mv, key_value_states, past_key_value[\u001b[39m1\u001b[39m] \u001b[39mif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    529\u001b[0m )\n\u001b[0;32m    531\u001b[0m \u001b[39m# compute scores\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Elena\\anaconda3\\envs\\qgmc\\Lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:498\u001b[0m, in \u001b[0;36mT5Attention.forward.<locals>.project\u001b[1;34m(hidden_states, proj_layer, key_value_states, past_key_value)\u001b[0m\n\u001b[0;32m    494\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"projects hidden states correctly to key/query states\"\"\"\u001b[39;00m\n\u001b[0;32m    495\u001b[0m \u001b[39mif\u001b[39;00m key_value_states \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    496\u001b[0m     \u001b[39m# self-attn\u001b[39;00m\n\u001b[0;32m    497\u001b[0m     \u001b[39m# (batch_size, n_heads, seq_length, dim_per_head)\u001b[39;00m\n\u001b[1;32m--> 498\u001b[0m     hidden_states \u001b[39m=\u001b[39m shape(proj_layer(hidden_states))\n\u001b[0;32m    499\u001b[0m \u001b[39melif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    500\u001b[0m     \u001b[39m# cross-attn\u001b[39;00m\n\u001b[0;32m    501\u001b[0m     \u001b[39m# (batch_size, n_heads, seq_length, dim_per_head)\u001b[39;00m\n\u001b[0;32m    502\u001b[0m     hidden_states \u001b[39m=\u001b[39m shape(proj_layer(key_value_states))\n",
      "File \u001b[1;32mc:\\Users\\Elena\\anaconda3\\envs\\qgmc\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Elena\\anaconda3\\envs\\qgmc\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mlinear(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#try as soon as possible\n",
    "#paraphrasing text --> takes too long\n",
    "\n",
    "#from transformers import T5ForConditionalGeneration, T5Tokenizer#\n",
    "\n",
    "## Load the T5 model and tokenizer\n",
    "#model_name = \"t5-large\"\n",
    "#model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "#tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "# \n",
    "## Tokenize the text\n",
    "#inputs = tokenizer(\"paraphrase: \" + extracted_text, return_tensors=\"pt\", max_length=512, truncation=True)#\n",
    "\n",
    "## Generate paraphrased sentences\n",
    "#num_paraphrases = 155  # Adjust as needed\n",
    "#paraphrases = []#\n",
    "\n",
    "#for _ in range(num_paraphrases):\n",
    "#    output = model.generate(inputs[\"input_ids\"], max_length=512, num_return_sequences=1, no_repeat_ngram_size=2)\n",
    "#    paraphrase = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "#    paraphrases.append(paraphrase)#\n",
    "\n",
    "## Print the generated paraphrases\n",
    "#for i, paraphrase in enumerate(paraphrases):\n",
    "#    print(f\"Paraphrase {i + 1}: {paraphrase}\")#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## generation of summary\n",
    "##Check if there are sections available before accessing them\n",
    "#if len(sections) >= 2:\n",
    "#    # Load the pre-trained T5 model and tokenizer\n",
    "#    model_name = \"t5-large\"\n",
    "#    model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "#    tokenizer = T5Tokenizer.from_pretrained(model_name)#\n",
    "\n",
    "#    for section_index, (section_title, section_content) in enumerate(sections):\n",
    "#        # Tokenize the input section\n",
    "#        input_ids = tokenizer.encode(\"summarize: \" + section_content, return_tensors=\"pt\", max_length=1024, truncation=True)#\n",
    "\n",
    "#        # Generate the summary\n",
    "#        summary_ids = model.generate(input_ids, max_length=150, min_length=30, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "#        summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)#\n",
    "\n",
    "#        # Print the summary for each section\n",
    "#        print(f\"Summary for Section {section_index + 1} - Title: {section_title}\")\n",
    "#        print(\"Summary:\", summary)\n",
    "#        print(\"-\" * 40)\n",
    "#else:\n",
    "#    print(\"Not enough sections found in the list.\")#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary for Section 1 - Title: 1.1\n",
      "Summary: software testing is a set of activities to discover defects and evaluate quality of software artifacts. a common misconception about testing is that it only consists of executing tests. testing is not only a technical activity. it also needs to be properly planned, managed, estimated, monitored and controlled.\n",
      "----------------------------------------\n",
      "Summary for Section 2 - Title: 1.1.2\n",
      "Summary: testing can trigger failures that are caused by defects software can directly find defects test object. debugging is concerned with finding causes of this failure, analyzing these causes, and eliminating them. when static testing identifies a defect, debugging is concerned with removing it.\n",
      "----------------------------------------\n",
      "Summary for Section 3 - Title: 1.2.1\n",
      "Summary: testing provides a cost-effective means of detecting defects. testing indirectly contributes to higher quality test objects. testing may also be required to meet contractual legal requirements.\n",
      "----------------------------------------\n",
      "Summary for Section 4 - Title: 1.2.3\n",
      "Summary: human beings make errors for various reasons, such as time pressure, complexity of work products, processes, infrastructure interactions, simply because they are tired lack adequate training. errors and defects are not only cause of failures. failures can also be caused by environmental conditions, such as when radiation electromagnetic field cause defects firmware.\n",
      "----------------------------------------\n",
      "Summary for Section 5 - Title: 1.4\n",
      "Summary: testing is context dependent, but, at a high level, there are common sets of test activities without which testing is less likely to achieve test objectives. these sets of test activities form a test process. which test activities are included this test process, how they are implemented, and when they occur is normally decided as part of test planning for specific situation. which test activities are included this test process, how they are implemented, and when they occur is normally decided as part of test planning for specific situation.\n",
      "----------------------------------------\n",
      "Summary for Section 6 - Title: 5.1\n",
      "Summary: test monitoring involves ongoing checking of all test activities and comparison of actual progress against plan. test control involves taking actions necessary to meet objectives of testing.\n",
      "----------------------------------------\n",
      "Summary for Section 7 - Title: 1.4.2\n",
      "Summary: test activities are integral part of development processes carried out within an organization. way testing is carried out will depend on a number of contextual factors. these factors will have an impact on many test-related issues.\n",
      "----------------------------------------\n",
      "Summary for Section 8 - Title: 1.4.1\n",
      "Summary: test planning work products include: test plan, test schedule, risk register. test monitoring and control work products include: test progress reports. test execution work products include: test logs, and defect reports.\n",
      "----------------------------------------\n",
      "Summary for Section 9 - Title: 1.4.5\n",
      "Summary: test management role takes overall responsibility for test process, test team and leadership of test activities. testing role is mainly focused on activities of test analysis, test design, test implementation and test execution. different people may take on roles of testing and test management at different times.\n",
      "----------------------------------------\n",
      "Summary for Section 10 - Title: 1.5.1\n",
      "Summary: testing knowledge Thoroughness, carefulness, curiosity, attention to details, being methodical Good communication skills, active listening, being a team player Communication skills crucial. some people may perceive testing as a destructive activity, even though it contributes greatly to project success and product quality.\n",
      "----------------------------------------\n",
      "Summary for Section 11 - Title: 1.5.3\n",
      "Summary: independent testers are likely to recognize different kinds of failures and defects compared to developers because of their different backgrounds, technical perspectives, and biases. independent testers may be isolated from development team, which may lead to a lack of collaboration, communication problems, an adversarial relationship with development team. independent testers may be seen as a bottleneck be blamed for delays release.\n",
      "----------------------------------------\n",
      "Summary for Section 12 - Title: 2.1\n",
      "Summary: good testing practices that apply to all software development lifecycles. fl-testing focuses on identifying and identifying bugs early in the software development lifecycle.\n",
      "----------------------------------------\n",
      "Summary for Section 13 - Title: 2.1\n",
      "Summary: Devops might have an impact on testing FL- 4 summarize how devops might have an impact on testing FL- 5 summarize how devops might have an impact on testing FL- 6 summarize how devops might have an impact on testing FL- 7 summarize how devops might have an impact on testing FL- 8 summarize how devops might have an impact on testing FL- 8 summarize how devops might have an impact on testing FL- 8 summarize how de\n",
      "----------------------------------------\n",
      "Summary for Section 14 - Title: 2.1\n",
      "Summary: retrospectives can be used as a mechanism for process improvement. retrospectives can be used to identify areas for process improvement. retrospectives can be used to identify areas for process improvement.\n",
      "----------------------------------------\n",
      "Summary for Section 15 - Title: 2.2\n",
      "Summary: 2 distinguish different test types fl-fl-fl-fl-fl-fl-fl-fl-fl-fl-fl.\n",
      "----------------------------------------\n",
      "Summary for Section 16 - Title: 2.3\n",
      "Summary: maintenance testing and its triggers. summarize maintenance testing and its triggers. summarize maintenance testing and its triggers. summarize maintenance testing and its triggers.\n",
      "----------------------------------------\n",
      "Summary for Section 17 - Title: 2.1.1\n",
      "Summary: choice of SDLC impacts on: Scope and timing of test activities Level of detail of test documentation Choice of test techniques and test approach Extent of test automation Role and responsibilities of a tester. dynamic testing cannot be performed early SDLC.\n",
      "----------------------------------------\n",
      "Summary for Section 18 - Title: 2.1.3\n",
      "Summary: TDD, ATDD and BDD are similar development approaches, where tests are defined as a means of directing development. each of these approaches implements principle of early testing and follows a shift-left approach. tests may persist as automated tests to ensure code quality future adaptions / refactoring.\n",
      "----------------------------------------\n",
      "Summary for Section 19 - Title: 2.1.5\n",
      "Summary: the principle of early testing is sometimes referred to as shift-left. shift-left normally suggests that testing should be done earlier SDLC. but it does not mean that testing later SDLC should be neglected.\n",
      "----------------------------------------\n",
      "Summary for Section 20 - Title: 2.2\n",
      "Summary: each test level is an instance of test process, performed relation to software at a given stage of development. sequential SDLC models, test levels are often defined such that exit criteria of one level are part of entry criteria for next level. test types are groups of test activities related to specific quality characteristics.\n",
      "----------------------------------------\n",
      "Summary for Section 21 - Title: 2.2.2\n",
      "Summary: functional testing evaluates functions that a component system should perform. non-functional testing evaluates attributes other than functional characteristics of a component system. black-box testing is specification-based and derives tests from documentation external to test object. white-box testing is structure-based and derives tests from system's implementation internal structure.\n",
      "----------------------------------------\n",
      "Summary for Section 22 - Title: 2.3\n",
      "Summary: maintenance can involve planned releases/deployments and unplanned releases/deployments. impact analysis may be done before a change is made. testing changes to a system production includes both evaluating success of implementation of change and checking for possible regressions parts of system that remain unchanged.\n",
      "----------------------------------------\n",
      "Summary for Section 23 - Title: 3.1\n",
      "Summary: explain value of static testing. explain value of static testing. explain value of static testing. explain value of static testing. explain value of dynamic testing.\n",
      "----------------------------------------\n",
      "Summary for Section 24 - Title: 3.2\n",
      "Summary: identifying benefits of early and frequent stakeholder feedback. identifying benefits of early and frequent stakeholder feedback. identifying benefits of frequent stakeholder feedback.\n",
      "----------------------------------------\n",
      "Summary for Section 25 - Title: 3.2\n",
      "Summary: responsibilities are assigned to principal roles when performing reviews. reviewers should be aware of which responsibilities are assigned to principal roles when performing reviews. reviewers should be aware of which responsibilities are assigned to principal roles when performing reviews.\n",
      "----------------------------------------\n",
      "Summary for Section 26 - Title: 3.2\n",
      "Summary: 5 review factors that contribute to a successful review. a reviewer's ability to remember a reviewer's name is one of the most important factors.\n",
      "----------------------------------------\n",
      "Summary for Section 27 - Title: 3.1.1\n",
      "Summary: Almost any work product that can be read and understood can be subject of a review. work products that are not appropriate for static testing include those that are difficult to interpret by human beings.\n",
      "----------------------------------------\n",
      "Summary for Section 28 - Title: 3.1.3\n",
      "Summary: static and dynamic testing can both lead to detection of defects work products. however there are some defect types that can only be found by either static dynamic testing. static testing may more easily detect defects that lay on paths through code that are rarely executed hard to reach using dynamic testing.\n",
      "----------------------------------------\n",
      "Summary for Section 29 - Title: 3.2.1\n",
      "Summary: early and frequent stakeholder feedback allows for early communication of potential quality problems. a failure to deliver what stakeholder wants can result costly rework, missed deadlines, blame games, and might even lead to complete project failure.\n",
      "----------------------------------------\n",
      "Summary for Section 30 - Title: 3.2.3\n",
      "Summary: reviews involve various stakeholders, who may take on several roles. a reviewer may be someone working on project, a subject matter expert, any other stakeholder. a review leader – takes overall responsibility for review.\n",
      "----------------------------------------\n",
      "Summary for Section 31 - Title: 3.2.5\n",
      "Summary: reviews should have clear objectives and measurable exit criteria. reviews should be conducted on small chunks, so reviewers do not lose concentration. reviews should be part of organization’s culture, to promote learning and process improvement.\n",
      "----------------------------------------\n",
      "Summary for Section 32 - Title: 4.2\n",
      "Summary: 1 Use equivalence partitioning to derive test cases. 1 use equivalence partitioning to derive test cases.\n",
      "----------------------------------------\n",
      "Summary for Section 33 - Title: 4.2\n",
      "Summary: 3 use decision table testing to derive test cases. use decision table testing to derive test cases. use decision table testing to derive test cases. use decision table testing to derive test cases.\n",
      "----------------------------------------\n",
      "Summary for Section 34 - Title: 4.3\n",
      "Summary: explains statement testing. explains statement testing. explains statement testing. explains statement testing. explains statement testing. explains statement testing.\n",
      "----------------------------------------\n",
      "Summary for Section 35 - Title: 4.3\n",
      "Summary: explain value of white-box testing 4.4 experience-based test techniques 4.4 white-box testing. explain value of white-box testing. explain value of white-box testing.\n",
      "----------------------------------------\n",
      "Summary for Section 36 - Title: 4.4\n",
      "Summary: exploratory testing is a form of exploratory testing. it involves a series of tests to find out if a hypothesis is true or false. the results are compared with those of a control group.\n",
      "----------------------------------------\n",
      "Summary for Section 37 - Title: 4.5\n",
      "Summary: collaboration-based test approaches - cbtas. collaboration-based test approaches - cbtas. cbtas.\n",
      "----------------------------------------\n",
      "Summary for Section 38 - Title: 4.5\n",
      "Summary: acceptance criteria can be written in a number of different ways. acceptance criteria can be written in a number of different ways. acceptance criteria can be written in a number of different ways. acceptance criteria can be written in a number of different ways.\n",
      "----------------------------------------\n",
      "Summary for Section 39 - Title: 4.1\n",
      "Summary: test techniques help to develop a relatively small, but sufficient, set of test cases a systematic way. they also help tester define test conditions, identify coverage items, and identify test data during test analysis and design. test techniques are classified as black-box, white-box, and experience-based.\n",
      "----------------------------------------\n",
      "Summary for Section 40 - Title: 4.2.1\n",
      "Summary: equivalence partitioning divides data into partitions based on expectation that all elements of a given partition are to be processed same way by test object. partitions may be continuous discrete, ordered unordered, finite infinite. to achieve 100% coverage with this technique, test cases must exercise all identified partitions by covering each partition at least once.\n",
      "----------------------------------------\n",
      "Summary for Section 41 - Title: 4.2.3\n",
      "Summary: decision tables are an effective way of recording complex logic, such as business rules. a full decision table has enough columns to cover every combination of conditions. table can be simplified by deleting columns containing infeasible combinations.\n",
      "----------------------------------------\n",
      "Summary for Section 42 - Title: 4.3\n",
      "Summary: this section focuses on two code-related white-box test techniques: statement testing Branch testing. there are also more rigorous techniques that are used some safety-critical, mission-critical, high-integrity environments. these techniques are not discussed this syllabus.\n",
      "----------------------------------------\n",
      "Summary for Section 43 - Title: 4.3.2\n",
      "Summary: branch testing and branch coverage aim is to design test cases to exercise branches code until an acceptable level of coverage is achieved. coverage measured as number of branches exercised by test cases divided by total number of branches. when 100% branch coverage is achieved, all branches code, unconditional and conditional are exercised by test cases.\n",
      "----------------------------------------\n",
      "Summary for Section 44 - Title: 4.4\n",
      "Summary: Error guessing Exploratory testing Checklist-based testing Checklist-based testing Checklist-based testing Error guessing Exploratory testing Checklist-based testing.\n",
      "----------------------------------------\n",
      "Summary for Section 45 - Title: 4.4.2\n",
      "Summary: testing is used to learn more about test object, to explore it more deeply with focused tests, and to create tests for untested areas. exploratory testing will be more effective if tester is experienced, has domain knowledge and has a high degree of essential skills.\n",
      "----------------------------------------\n",
      "Summary for Section 46 - Title: 4.5\n",
      "Summary: each of above-mentioned techniques has a particular objective with respect to defect detection. collaboration-based approaches focus also on defect avoidance by collaboration and communication.\n",
      "----------------------------------------\n",
      "Summary for Section 47 - Title: 4.5.2\n",
      "Summary: acceptance criteria are conditions that an implementation of user story must meet to be accepted by stakeholders. acceptance criteria may be viewed as test conditions that should be exercised by tests. there are several ways to write acceptance criteria for a user story.\n",
      "----------------------------------------\n",
      "Summary for Section 48 - Title: 5.1\n",
      "Summary: 1 Exemplify purpose and content of a test plan FL-fl-fl-fl-fl-fl-fl-fl-fl-fl-fl-fl-fl-fl-fl-fl-fl-fl-fl-fl-fl-fl-fl-fl-fl-fl-fl-fl-fl-fl-fl-fl-fl-f\n",
      "----------------------------------------\n",
      "Summary for Section 49 - Title: 5.1\n",
      "Summary: compare and contrast entry criteria and exit criteria. compare and contrast entry criteria and exit criteria. compare and contrast entry criteria and exit criteria. compare and contrast entry criteria and exit criteria.\n",
      "----------------------------------------\n",
      "Summary for Section 50 - Title: 5.1\n",
      "Summary: 5 apply test case prioritization. FL- 5 apply test case prioritization. FL- 5 apply test case prioritization.\n",
      "----------------------------------------\n",
      "Summary for Section 51 - Title: 5.1\n",
      "Summary: testing quadrants and their relationships with test levels and test types. risk management is a key part of a test manager's job. a test manager's job is to manage the risk of an incident.\n",
      "----------------------------------------\n",
      "Summary for Section 52 - Title: 5.2\n",
      "Summary: 2 distinguish between project risks and product risks. product risks are more likely to be a problem than a project risk. product risks more likely to be a problem than a project risk.\n",
      "----------------------------------------\n",
      "Summary for Section 53 - Title: 5.2\n",
      "Summary: 5.3 test monitoring, test control and test completion 5.3 test monitoring, test control and test completion 5.3 test monitoring, test monitoring, test control and test completion 5.3 test monitoring, test monitoring, test control and test completion 5.3 test monitoring, test monitoring, test control and test completion 5.3 test monitoring, test monitoring, test monitoring and test completion 5.4 test monitoring, test monitoring, test control and test completion 5.3 test monitoring, test monitoring, test monitoring, test control and test completion 5.4 test monitoring, test monitoring, test control and test\n",
      "----------------------------------------\n",
      "Summary for Section 54 - Title: 5.3\n",
      "Summary: 2 summarize purposes, content, and audiences for test reports. test reports can be used for a variety of purposes, including assessing the effectiveness of a product or service.\n",
      "----------------------------------------\n",
      "Summary for Section 55 - Title: 5.4\n",
      "Summary: configuration management supports testing 5.5 defect management 5.5 configuration management supports testing 5.5 defect management 5.5 configuration management supports defect management 5.5 defect management 5.5 configuration management supports testing.\n",
      "----------------------------------------\n",
      "Summary for Section 56 - Title: 5.1\n",
      "Summary: test planning.................\n",
      "----------------------------------------\n",
      "Summary for Section 57 - Title: 5.1.2\n",
      "Summary: release planning looks ahead to release of a product, defines and re-defines product backlog. it also serves as basis for test approach and test plan across all iterations. iteration planning looks ahead to end of a single iteration and is concerned with iteration backlog.\n",
      "----------------------------------------\n",
      "Summary for Section 58 - Title: 5.1.4\n",
      "Summary: test effort estimation involves predicting amount of test-related work needed to meet objectives of a test project. it is important to make it clear to stakeholders that estimate is based on a number of assumptions and is always subject to estimation error. following four estimation techniques are described in this syllabus.\n",
      "----------------------------------------\n",
      "Summary for Section 59 - Title: 5.1.6\n",
      "Summary: test pyramid model shows that different tests may have different granularity. test pyramid model supports team test automation and test effort allocation. pyramid layers represent groups of tests. higher layer, lower test granularity, test isolation and test execution time.\n",
      "----------------------------------------\n",
      "Summary for Section 60 - Title: 5.2\n",
      "Summary: organizations face many internal and external factors that make it uncertain whether or when they will achieve their objectives. risk management allows organizations to increase likelihood of achieving objectives, improve quality of their products and increase stakeholders’ confidence and trust. main risk management activities are: risk analysis Risk control test approach, which test activities are selected, prioritized, and managed based on risk control.\n",
      "----------------------------------------\n",
      "Summary for Section 61 - Title: 5.2.2\n",
      "Summary: software testing one is generally concerned with two types of risks: project risks and product risks. project risks are related to management and control of project. product risks include: missing wrong functionality, incorrect calculations, runtime errors, poor architecture, inefficient algorithms.\n",
      "----------------------------------------\n",
      "Summary for Section 62 - Title: 5.2.4\n",
      "Summary: product risk control consists of risk mitigation and risk monitoring. several response options to risk are possible, e.g., risk acceptance, risk transfer, contingency plan. testing can be used to mitigate product risks by testing.\n",
      "----------------------------------------\n",
      "Summary for Section 63 - Title: 5.3.1\n",
      "Summary: test monitoring gathers a variety of metrics to support test control and test completion. common test metrics include: project progress metrics Test progress metrics Product quality metrics Defect metrics Risk metrics Coverage metrics cost metrics. test metrics are gathered to show progress against planned schedule and budget.\n",
      "----------------------------------------\n",
      "Summary for Section 64 - Title: 5.3.3\n",
      "Summary: best means of communicating test status varies, depending on test management concerns, organizational test strategies, regulatory standards, or, case of self-organizing teams, on team itself. verbal communication with team members and other stakeholders Dashboards Electronic communication channels Online documentation Formal test reports One more option can be used.\n",
      "----------------------------------------\n",
      "Summary for Section 65 - Title: 5.5\n",
      "Summary: defect management process includes a workflow for handling individual anomalies from their discovery to their closure and rules for their classification. process must be followed by all involved stakeholders. a defect report logged during dynamic testing typically includes: Unique identifier Title with a short summary of anomaly being reported.\n",
      "----------------------------------------\n",
      "Extracted text for Dictionary saved to 'data/dictionary.txt'\n"
     ]
    }
   ],
   "source": [
    "#adding of summary to the dictionary as value to the key\n",
    "# Initialize an empty dictionary to store section content and summaries\n",
    "sections_dict = {}\n",
    "\n",
    "# Check if there are sections available before accessing them\n",
    "if len(sections) >= 2:\n",
    "    # Load the pre-trained T5 model and tokenizer\n",
    "    model_name = \"t5-large\"\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "    tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "    for section_index, (section_title, section_content) in enumerate(sections):\n",
    "        # Tokenize the input section\n",
    "        input_ids = tokenizer.encode(\"summarize: \" + section_content, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "\n",
    "        # Generate the summary\n",
    "        summary_ids = model.generate(input_ids, max_length=150, min_length=30, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "        summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "        # Store the summary in sections_dict using the section title as the key\n",
    "        sections_dict[section_title] = summary\n",
    "\n",
    "        # Print the summary for each section\n",
    "        print(f\"Summary for Section {section_index + 1} - Title: {section_title}\")\n",
    "        print(\"Summary:\", summary)\n",
    "        print(\"-\" * 40)\n",
    "else:\n",
    "    print(\"Not enough sections found in the list.\")\n",
    "\n",
    "output_file_path = 'data/dictionary.txt'\n",
    "with open(output_file_path, 'w', encoding='utf-8') as out_file:\n",
    "    # Writing the extracted text to the output file\n",
    "    out_file.write(extracted_text)\n",
    "# \n",
    "# Closing the stream\n",
    "output_string.close()\n",
    "# \n",
    "# Printing message to indicate that the text has been saved to the file\n",
    "print(f\"Extracted text for Dictionary saved to '{output_file_path}'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n",
      "1.1 : software testing is a set of activities to discover defects and evaluate quality of software artifacts. a common misconception about testing is that it only consists of executing tests. testing is not only a technical activity. it also needs to be properly planned, managed, estimated, monitored and controlled.\n",
      "1.1.2 : testing can trigger failures that are caused by defects software can directly find defects test object. debugging is concerned with finding causes of this failure, analyzing these causes, and eliminating them. when static testing identifies a defect, debugging is concerned with removing it.\n",
      "1.2.1 : testing provides a cost-effective means of detecting defects. testing indirectly contributes to higher quality test objects. testing may also be required to meet contractual legal requirements.\n",
      "1.2.3 : human beings make errors for various reasons, such as time pressure, complexity of work products, processes, infrastructure interactions, simply because they are tired lack adequate training. errors and defects are not only cause of failures. failures can also be caused by environmental conditions, such as when radiation electromagnetic field cause defects firmware.\n",
      "1.4 : testing is context dependent, but, at a high level, there are common sets of test activities without which testing is less likely to achieve test objectives. these sets of test activities form a test process. which test activities are included this test process, how they are implemented, and when they occur is normally decided as part of test planning for specific situation. which test activities are included this test process, how they are implemented, and when they occur is normally decided as part of test planning for specific situation.\n",
      "1.4.2 : test activities are integral part of development processes carried out within an organization. way testing is carried out will depend on a number of contextual factors. these factors will have an impact on many test-related issues.\n",
      "1.4.1 : test planning work products include: test plan, test schedule, risk register. test monitoring and control work products include: test progress reports. test execution work products include: test logs, and defect reports.\n",
      "1.4.5 : test management role takes overall responsibility for test process, test team and leadership of test activities. testing role is mainly focused on activities of test analysis, test design, test implementation and test execution. different people may take on roles of testing and test management at different times.\n",
      "1.5.1 : testing knowledge Thoroughness, carefulness, curiosity, attention to details, being methodical Good communication skills, active listening, being a team player Communication skills crucial. some people may perceive testing as a destructive activity, even though it contributes greatly to project success and product quality.\n",
      "1.5.3 : independent testers are likely to recognize different kinds of failures and defects compared to developers because of their different backgrounds, technical perspectives, and biases. independent testers may be isolated from development team, which may lead to a lack of collaboration, communication problems, an adversarial relationship with development team. independent testers may be seen as a bottleneck be blamed for delays release.\n",
      "2.1 : retrospectives can be used as a mechanism for process improvement. retrospectives can be used to identify areas for process improvement. retrospectives can be used to identify areas for process improvement.\n",
      "2.2 : each test level is an instance of test process, performed relation to software at a given stage of development. sequential SDLC models, test levels are often defined such that exit criteria of one level are part of entry criteria for next level. test types are groups of test activities related to specific quality characteristics.\n",
      "2.3 : maintenance can involve planned releases/deployments and unplanned releases/deployments. impact analysis may be done before a change is made. testing changes to a system production includes both evaluating success of implementation of change and checking for possible regressions parts of system that remain unchanged.\n",
      "2.1.1 : choice of SDLC impacts on: Scope and timing of test activities Level of detail of test documentation Choice of test techniques and test approach Extent of test automation Role and responsibilities of a tester. dynamic testing cannot be performed early SDLC.\n",
      "2.1.3 : TDD, ATDD and BDD are similar development approaches, where tests are defined as a means of directing development. each of these approaches implements principle of early testing and follows a shift-left approach. tests may persist as automated tests to ensure code quality future adaptions / refactoring.\n",
      "2.1.5 : the principle of early testing is sometimes referred to as shift-left. shift-left normally suggests that testing should be done earlier SDLC. but it does not mean that testing later SDLC should be neglected.\n",
      "2.2.2 : functional testing evaluates functions that a component system should perform. non-functional testing evaluates attributes other than functional characteristics of a component system. black-box testing is specification-based and derives tests from documentation external to test object. white-box testing is structure-based and derives tests from system's implementation internal structure.\n",
      "3.1 : explain value of static testing. explain value of static testing. explain value of static testing. explain value of static testing. explain value of dynamic testing.\n",
      "3.2 : 5 review factors that contribute to a successful review. a reviewer's ability to remember a reviewer's name is one of the most important factors.\n",
      "3.1.1 : Almost any work product that can be read and understood can be subject of a review. work products that are not appropriate for static testing include those that are difficult to interpret by human beings.\n",
      "3.1.3 : static and dynamic testing can both lead to detection of defects work products. however there are some defect types that can only be found by either static dynamic testing. static testing may more easily detect defects that lay on paths through code that are rarely executed hard to reach using dynamic testing.\n",
      "3.2.1 : early and frequent stakeholder feedback allows for early communication of potential quality problems. a failure to deliver what stakeholder wants can result costly rework, missed deadlines, blame games, and might even lead to complete project failure.\n",
      "3.2.3 : reviews involve various stakeholders, who may take on several roles. a reviewer may be someone working on project, a subject matter expert, any other stakeholder. a review leader – takes overall responsibility for review.\n",
      "3.2.5 : reviews should have clear objectives and measurable exit criteria. reviews should be conducted on small chunks, so reviewers do not lose concentration. reviews should be part of organization’s culture, to promote learning and process improvement.\n",
      "4.2 : 3 use decision table testing to derive test cases. use decision table testing to derive test cases. use decision table testing to derive test cases. use decision table testing to derive test cases.\n",
      "4.3 : this section focuses on two code-related white-box test techniques: statement testing Branch testing. there are also more rigorous techniques that are used some safety-critical, mission-critical, high-integrity environments. these techniques are not discussed this syllabus.\n",
      "4.4 : Error guessing Exploratory testing Checklist-based testing Checklist-based testing Checklist-based testing Error guessing Exploratory testing Checklist-based testing.\n",
      "4.5 : each of above-mentioned techniques has a particular objective with respect to defect detection. collaboration-based approaches focus also on defect avoidance by collaboration and communication.\n",
      "4.1 : test techniques help to develop a relatively small, but sufficient, set of test cases a systematic way. they also help tester define test conditions, identify coverage items, and identify test data during test analysis and design. test techniques are classified as black-box, white-box, and experience-based.\n",
      "4.2.1 : equivalence partitioning divides data into partitions based on expectation that all elements of a given partition are to be processed same way by test object. partitions may be continuous discrete, ordered unordered, finite infinite. to achieve 100% coverage with this technique, test cases must exercise all identified partitions by covering each partition at least once.\n",
      "4.2.3 : decision tables are an effective way of recording complex logic, such as business rules. a full decision table has enough columns to cover every combination of conditions. table can be simplified by deleting columns containing infeasible combinations.\n",
      "4.3.2 : branch testing and branch coverage aim is to design test cases to exercise branches code until an acceptable level of coverage is achieved. coverage measured as number of branches exercised by test cases divided by total number of branches. when 100% branch coverage is achieved, all branches code, unconditional and conditional are exercised by test cases.\n",
      "4.4.2 : testing is used to learn more about test object, to explore it more deeply with focused tests, and to create tests for untested areas. exploratory testing will be more effective if tester is experienced, has domain knowledge and has a high degree of essential skills.\n",
      "4.5.2 : acceptance criteria are conditions that an implementation of user story must meet to be accepted by stakeholders. acceptance criteria may be viewed as test conditions that should be exercised by tests. there are several ways to write acceptance criteria for a user story.\n",
      "5.2 : organizations face many internal and external factors that make it uncertain whether or when they will achieve their objectives. risk management allows organizations to increase likelihood of achieving objectives, improve quality of their products and increase stakeholders’ confidence and trust. main risk management activities are: risk analysis Risk control test approach, which test activities are selected, prioritized, and managed based on risk control.\n",
      "5.3 : 2 summarize purposes, content, and audiences for test reports. test reports can be used for a variety of purposes, including assessing the effectiveness of a product or service.\n",
      "5.4 : configuration management supports testing 5.5 defect management 5.5 configuration management supports testing 5.5 defect management 5.5 configuration management supports defect management 5.5 defect management 5.5 configuration management supports testing.\n",
      "5.1.2 : release planning looks ahead to release of a product, defines and re-defines product backlog. it also serves as basis for test approach and test plan across all iterations. iteration planning looks ahead to end of a single iteration and is concerned with iteration backlog.\n",
      "5.1.4 : test effort estimation involves predicting amount of test-related work needed to meet objectives of a test project. it is important to make it clear to stakeholders that estimate is based on a number of assumptions and is always subject to estimation error. following four estimation techniques are described in this syllabus.\n",
      "5.1.6 : test pyramid model shows that different tests may have different granularity. test pyramid model supports team test automation and test effort allocation. pyramid layers represent groups of tests. higher layer, lower test granularity, test isolation and test execution time.\n",
      "5.2.2 : software testing one is generally concerned with two types of risks: project risks and product risks. project risks are related to management and control of project. product risks include: missing wrong functionality, incorrect calculations, runtime errors, poor architecture, inefficient algorithms.\n",
      "5.2.4 : product risk control consists of risk mitigation and risk monitoring. several response options to risk are possible, e.g., risk acceptance, risk transfer, contingency plan. testing can be used to mitigate product risks by testing.\n",
      "5.3.1 : test monitoring gathers a variety of metrics to support test control and test completion. common test metrics include: project progress metrics Test progress metrics Product quality metrics Defect metrics Risk metrics Coverage metrics cost metrics. test metrics are gathered to show progress against planned schedule and budget.\n",
      "5.3.3 : best means of communicating test status varies, depending on test management concerns, organizational test strategies, regulatory standards, or, case of self-organizing teams, on team itself. verbal communication with team members and other stakeholders Dashboards Electronic communication channels Online documentation Formal test reports One more option can be used.\n",
      "5.5 : defect management process includes a workflow for handling individual anomalies from their discovery to their closure and rules for their classification. process must be followed by all involved stakeholders. a defect report logged during dynamic testing typically includes: Unique identifier Title with a short summary of anomaly being reported.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None]"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# update of key dictionary w full names of sections\n",
    "# fot section 5.1 no summary generated, so it is removed from the list of sections\n",
    "sections_dict.pop('5.1')\n",
    "\n",
    "\n",
    "#sections_dict[\"1.2.1. Testing’s Contributions to Success\"] = sections_dict.pop('1.2.1')\n",
    "\n",
    "#debug result\n",
    "print(len(sections_dict))\n",
    "[print(key,':',value) for key, value in sections_dict.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46\n",
      "{'1.1': 'software testing is a set of activities to discover defects and evaluate quality of software artifacts. a common misconception about testing is that it only consists of executing tests. testing is not only a technical activity. it also needs to be properly planned, managed, estimated, monitored and controlled.', '1.1.2': 'testing can trigger failures that are caused by defects software can directly find defects test object. debugging is concerned with finding causes of this failure, analyzing these causes, and eliminating them. when static testing identifies a defect, debugging is concerned with removing it.', '1.2.1': 'testing provides a cost-effective means of detecting defects. testing indirectly contributes to higher quality test objects. testing may also be required to meet contractual legal requirements.', '1.2.3': 'human beings make errors for various reasons, such as time pressure, complexity of work products, processes, infrastructure interactions, simply because they are tired lack adequate training. errors and defects are not only cause of failures. failures can also be caused by environmental conditions, such as when radiation electromagnetic field cause defects firmware.', '1.4': 'testing is context dependent, but, at a high level, there are common sets of test activities without which testing is less likely to achieve test objectives. these sets of test activities form a test process. which test activities are included this test process, how they are implemented, and when they occur is normally decided as part of test planning for specific situation. which test activities are included this test process, how they are implemented, and when they occur is normally decided as part of test planning for specific situation.', '5.1': 'test planning.................', '1.4.2': 'test activities are integral part of development processes carried out within an organization. way testing is carried out will depend on a number of contextual factors. these factors will have an impact on many test-related issues.', '1.4.1': 'test planning work products include: test plan, test schedule, risk register. test monitoring and control work products include: test progress reports. test execution work products include: test logs, and defect reports.', '1.4.5': 'test management role takes overall responsibility for test process, test team and leadership of test activities. testing role is mainly focused on activities of test analysis, test design, test implementation and test execution. different people may take on roles of testing and test management at different times.', '1.5.1': 'testing knowledge Thoroughness, carefulness, curiosity, attention to details, being methodical Good communication skills, active listening, being a team player Communication skills crucial. some people may perceive testing as a destructive activity, even though it contributes greatly to project success and product quality.', '1.5.3': 'independent testers are likely to recognize different kinds of failures and defects compared to developers because of their different backgrounds, technical perspectives, and biases. independent testers may be isolated from development team, which may lead to a lack of collaboration, communication problems, an adversarial relationship with development team. independent testers may be seen as a bottleneck be blamed for delays release.', '2.1': 'retrospectives can be used as a mechanism for process improvement. retrospectives can be used to identify areas for process improvement. retrospectives can be used to identify areas for process improvement.', '2.2': 'each test level is an instance of test process, performed relation to software at a given stage of development. sequential SDLC models, test levels are often defined such that exit criteria of one level are part of entry criteria for next level. test types are groups of test activities related to specific quality characteristics.', '2.3': 'maintenance can involve planned releases/deployments and unplanned releases/deployments. impact analysis may be done before a change is made. testing changes to a system production includes both evaluating success of implementation of change and checking for possible regressions parts of system that remain unchanged.', '2.1.1': 'choice of SDLC impacts on: Scope and timing of test activities Level of detail of test documentation Choice of test techniques and test approach Extent of test automation Role and responsibilities of a tester. dynamic testing cannot be performed early SDLC.', '2.1.3': 'TDD, ATDD and BDD are similar development approaches, where tests are defined as a means of directing development. each of these approaches implements principle of early testing and follows a shift-left approach. tests may persist as automated tests to ensure code quality future adaptions / refactoring.', '2.1.5': 'the principle of early testing is sometimes referred to as shift-left. shift-left normally suggests that testing should be done earlier SDLC. but it does not mean that testing later SDLC should be neglected.', '2.2.2': \"functional testing evaluates functions that a component system should perform. non-functional testing evaluates attributes other than functional characteristics of a component system. black-box testing is specification-based and derives tests from documentation external to test object. white-box testing is structure-based and derives tests from system's implementation internal structure.\", '3.1': 'explain value of static testing. explain value of static testing. explain value of static testing. explain value of static testing. explain value of dynamic testing.', '3.2': \"5 review factors that contribute to a successful review. a reviewer's ability to remember a reviewer's name is one of the most important factors.\", '3.1.1': 'Almost any work product that can be read and understood can be subject of a review. work products that are not appropriate for static testing include those that are difficult to interpret by human beings.', '3.1.3': 'static and dynamic testing can both lead to detection of defects work products. however there are some defect types that can only be found by either static dynamic testing. static testing may more easily detect defects that lay on paths through code that are rarely executed hard to reach using dynamic testing.', '3.2.1': 'early and frequent stakeholder feedback allows for early communication of potential quality problems. a failure to deliver what stakeholder wants can result costly rework, missed deadlines, blame games, and might even lead to complete project failure.', '3.2.3': 'reviews involve various stakeholders, who may take on several roles. a reviewer may be someone working on project, a subject matter expert, any other stakeholder. a review leader – takes overall responsibility for review.', '3.2.5': 'reviews should have clear objectives and measurable exit criteria. reviews should be conducted on small chunks, so reviewers do not lose concentration. reviews should be part of organization’s culture, to promote learning and process improvement.', '4.2': '3 use decision table testing to derive test cases. use decision table testing to derive test cases. use decision table testing to derive test cases. use decision table testing to derive test cases.', '4.3': 'this section focuses on two code-related white-box test techniques: statement testing Branch testing. there are also more rigorous techniques that are used some safety-critical, mission-critical, high-integrity environments. these techniques are not discussed this syllabus.', '4.4': 'Error guessing Exploratory testing Checklist-based testing Checklist-based testing Checklist-based testing Error guessing Exploratory testing Checklist-based testing.', '4.5': 'each of above-mentioned techniques has a particular objective with respect to defect detection. collaboration-based approaches focus also on defect avoidance by collaboration and communication.', '4.1': 'test techniques help to develop a relatively small, but sufficient, set of test cases a systematic way. they also help tester define test conditions, identify coverage items, and identify test data during test analysis and design. test techniques are classified as black-box, white-box, and experience-based.', '4.2.1': 'equivalence partitioning divides data into partitions based on expectation that all elements of a given partition are to be processed same way by test object. partitions may be continuous discrete, ordered unordered, finite infinite. to achieve 100% coverage with this technique, test cases must exercise all identified partitions by covering each partition at least once.', '4.2.3': 'decision tables are an effective way of recording complex logic, such as business rules. a full decision table has enough columns to cover every combination of conditions. table can be simplified by deleting columns containing infeasible combinations.', '4.3.2': 'branch testing and branch coverage aim is to design test cases to exercise branches code until an acceptable level of coverage is achieved. coverage measured as number of branches exercised by test cases divided by total number of branches. when 100% branch coverage is achieved, all branches code, unconditional and conditional are exercised by test cases.', '4.4.2': 'testing is used to learn more about test object, to explore it more deeply with focused tests, and to create tests for untested areas. exploratory testing will be more effective if tester is experienced, has domain knowledge and has a high degree of essential skills.', '4.5.2': 'acceptance criteria are conditions that an implementation of user story must meet to be accepted by stakeholders. acceptance criteria may be viewed as test conditions that should be exercised by tests. there are several ways to write acceptance criteria for a user story.', '5.2': 'organizations face many internal and external factors that make it uncertain whether or when they will achieve their objectives. risk management allows organizations to increase likelihood of achieving objectives, improve quality of their products and increase stakeholders’ confidence and trust. main risk management activities are: risk analysis Risk control test approach, which test activities are selected, prioritized, and managed based on risk control.', '5.3': '2 summarize purposes, content, and audiences for test reports. test reports can be used for a variety of purposes, including assessing the effectiveness of a product or service.', '5.4': 'configuration management supports testing 5.5 defect management 5.5 configuration management supports testing 5.5 defect management 5.5 configuration management supports defect management 5.5 defect management 5.5 configuration management supports testing.', '5.1.2': 'release planning looks ahead to release of a product, defines and re-defines product backlog. it also serves as basis for test approach and test plan across all iterations. iteration planning looks ahead to end of a single iteration and is concerned with iteration backlog.', '5.1.4': 'test effort estimation involves predicting amount of test-related work needed to meet objectives of a test project. it is important to make it clear to stakeholders that estimate is based on a number of assumptions and is always subject to estimation error. following four estimation techniques are described in this syllabus.', '5.1.6': 'test pyramid model shows that different tests may have different granularity. test pyramid model supports team test automation and test effort allocation. pyramid layers represent groups of tests. higher layer, lower test granularity, test isolation and test execution time.', '5.2.2': 'software testing one is generally concerned with two types of risks: project risks and product risks. project risks are related to management and control of project. product risks include: missing wrong functionality, incorrect calculations, runtime errors, poor architecture, inefficient algorithms.', '5.2.4': 'product risk control consists of risk mitigation and risk monitoring. several response options to risk are possible, e.g., risk acceptance, risk transfer, contingency plan. testing can be used to mitigate product risks by testing.', '5.3.1': 'test monitoring gathers a variety of metrics to support test control and test completion. common test metrics include: project progress metrics Test progress metrics Product quality metrics Defect metrics Risk metrics Coverage metrics cost metrics. test metrics are gathered to show progress against planned schedule and budget.', '5.3.3': 'best means of communicating test status varies, depending on test management concerns, organizational test strategies, regulatory standards, or, case of self-organizing teams, on team itself. verbal communication with team members and other stakeholders Dashboards Electronic communication channels Online documentation Formal test reports One more option can be used.', '5.5': 'defect management process includes a workflow for handling individual anomalies from their discovery to their closure and rules for their classification. process must be followed by all involved stakeholders. a defect report logged during dynamic testing typically includes: Unique identifier Title with a short summary of anomaly being reported.'}\n"
     ]
    }
   ],
   "source": [
    "print(len(sections_dict))\n",
    "print(sections_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary + keywords = T5 questions\n",
    "\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "# Load the T5 model and tokenizer\n",
    "model_name = \"t5-small\"\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Sample summary and keywords (replace with your own data)\n",
    "summary = \"Software testing is the process of evaluating software to identify defects.\"\n",
    "keywords = [\"Software testing\", \"process\", \"defects\"]\n",
    "\n",
    "# Prepare input for question generation\n",
    "input_text = f\"Summary: {summary} Keywords: {', '.join(keywords)}\"\n",
    "\n",
    "# Tokenize the input\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "\n",
    "# Generate questions\n",
    "output = model.generate(inputs[\"input_ids\"], max_length=512, num_return_sequences=1, no_repeat_ngram_size=2)\n",
    "\n",
    "# Decode and print the generated questions\n",
    "generated_questions = [tokenizer.decode(question, skip_special_tokens=True) for question in output]\n",
    "for i, question in enumerate(generated_questions, 1):\n",
    "    print(f\"Question {i}: {question}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# questions generation\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Load the pre-trained model and tokenizer for question generation\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"PrimeQA/mt5-base-tydi-question-generator\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"PrimeQA/mt5-base-tydi-question-generator\")\n",
    "\n",
    "# Function to generate a question for a given summary\n",
    "def generate_question(summary, max_length=64):\n",
    "    # Tokenize the input text and generate the question\n",
    "    features = tokenizer([summary], return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "    output = model.generate(input_ids=features['input_ids'], \n",
    "                            attention_mask=features['attention_mask'],\n",
    "                            max_length=max_length,\n",
    "                            num_return_sequences=1)\n",
    "    \n",
    "    return tokenizer.decode(output[0])\n",
    "\n",
    "# Example usage:\n",
    "summary = \"defect management process includes a workflow for handling individual anomalies from their discovery to their closure and rules for their classification. process must be followed by all involved stakeholders. a defect report logged during dynamic testing typically includes: Unique identifier Title with a short summary of anomaly being reported.\"\n",
    "question = generate_question(summary)\n",
    "print(\"Generated Question:\", question)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#experiment!!!\n",
    "\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "\n",
    "# Load the pre-trained BART model and tokenizer\n",
    "model_name = \"facebook/bart-large-cnn\"  # You can choose a different model size if needed\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Function to generate an answer for a given question and context\n",
    "def generate_answer(question, context, max_length=64):\n",
    "    # Prepare the input text by combining the question and context\n",
    "    input_text = f\"question: {question} context: {context}\"\n",
    "\n",
    "    # Tokenize the input text\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "\n",
    "    # Generate the answer\n",
    "    answer_ids = model.generate(input_ids, max_length=max_length, num_beams=4, early_stopping=True, num_return_sequences=1)\n",
    "\n",
    "    # Decode the generated answer\n",
    "    answer = tokenizer.decode(answer_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    return answer\n",
    "\n",
    "# Example usage:\n",
    "#context = \"defect management process includes a workflow for handling individual anomalies from their discovery to their closure and rules for their classification. process must be followed by all involved stakeholders. a defect report logged during dynamic testing typically includes: Unique identifier Title with a short summary of anomaly being reported.\"\n",
    "#question = \"What is the most common defect management process?\"\n",
    "\n",
    "# Generate the answer\n",
    "generated_answer = generate_answer(question, context)\n",
    "print(\"Generated Answer:\", generated_answer)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "\n",
    "# Load the pre-trained BART model and tokenizer\n",
    "model_name = \"facebook/bart-large-cnn\"\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Function to generate an answer for a given question and context\n",
    "def generate_answer(question, context, max_length=64):\n",
    "    # Prepare the input text by combining the question and context\n",
    "    input_text = f\"question: {question} context: {context}\"\n",
    "\n",
    "    # Tokenize the input text\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "\n",
    "    # Generate the answer\n",
    "    answer_ids = model.generate(input_ids, max_length=max_length, num_beams=4, early_stopping=True, num_return_sequences=1)\n",
    "\n",
    "    # Decode the generated answer\n",
    "    answer = tokenizer.decode(answer_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    return answer\n",
    "\n",
    "# Example usage:\n",
    "#context = \"defect management process includes a workflow for handling individual anomalies from their discovery to their closure and rules for their classification. process must be followed by all involved stakeholders. a defect report logged during dynamic testing typically includes: Unique identifier Title with a short summary of anomaly being reported.\"\n",
    "context = \"testing provides a cost-effective means of detecting defects.\"\n",
    "question = \"What provides a cost-effective means of detecting defects?\"\n",
    "\n",
    "# Generate the answer\n",
    "generated_answer = generate_answer(question, context)\n",
    "print(\"Generated Answer:\", generated_answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keywords extraction - missing so far, leads to HTTPSConnectionPool(host='cdn-lfs.huggingface.co', port=443): Read timed out. A lot of hours are required. \n",
    "# Talk to teachers\n",
    "\n",
    "# Load the pre-trained KeyBERT model\n",
    "#model = KeyBERT(\"distilbert-base-nli-mean-tokens\")\n",
    "\n",
    "# Input text (use sections[0][1] as the content of the first section)\n",
    "#section_content = sections[0][1]\n",
    "\n",
    "# Extract keywords\n",
    "#try:\n",
    "#    keywords = model.extract_keywords(section_content, keyphrase_ngram_range=(1, 2), stop_words='english', use_mmr=True, top_n=10, resume_download=True)\n",
    "    \n",
    "    # Print the extracted keywords\n",
    "#    for keyword in keywords:\n",
    "#        print(keyword)\n",
    "#except Exception as e:\n",
    "#    print(\"An error occurred:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modeling based on rules\n",
    "sentences = sent_tokenize(summary) \n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Function to generate a fixed number of questions from sentences using trigrams\n",
    "def generate_questions(text, num_questions=20):\n",
    "    questions = []\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Function to generate a fixed number of questions from sentences using trigrams\n",
    "def generate_questions(text, num_questions=30):\n",
    "    questions = []\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # Define question templates with different prefixes\n",
    "    question_templates = [\"What\", \"What is\", \"What can\", \"What does\"]\n",
    "    \n",
    "    for sentence in text:\n",
    "        # Tokenize each sentence into words\n",
    "        words = nltk.word_tokenize(sentence)\n",
    "        # Generate n-grams (trigrams) from the words\n",
    "        n_grams = list(ngrams(words, 3))\n",
    "\n",
    "        # Construct questions using the trigrams and different question templates\n",
    "        for n_gram in n_grams:\n",
    "            if (\n",
    "                n_gram[-1].lower() not in stop_words \n",
    "                and n_gram[-1].lower() != n_gram[-2].lower()\n",
    "                and \"can\" not in n_gram\n",
    "            ):\n",
    "                for template in question_templates:\n",
    "                    question = f\"{template} {n_gram[0]} {n_gram[1]} {n_gram[2]}?\"\n",
    "                    questions.append(question)\n",
    "\n",
    "            # Stop generating questions if we reach the desired number\n",
    "            if len(questions) >= num_questions:\n",
    "                return questions\n",
    "\n",
    "    return questions[:num_questions]  # Return only the specified number of questions\n",
    "\n",
    "# Generate 20 questions from the sentences using trigrams and different prefixes\n",
    "questions = generate_questions(sentences, num_questions=30)\n",
    "\n",
    "# Print the generated questions\n",
    "for i, question in enumerate(questions, start=1):\n",
    "    print(f\"Question {i}: {question}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#distractors generation\n",
    "\n",
    "import random\n",
    "\n",
    "def generate_distractors(correct_answer, num_distractors, word_embedding_model=None):\n",
    "    \"\"\"\n",
    "    Generate distractors for a given correct answer.\n",
    "\n",
    "    Args:\n",
    "    - correct_answer (str): The correct answer.\n",
    "    - num_distractors (int): The number of distractors to generate.\n",
    "    - word_embedding_model: A pre-trained word embedding model (e.g., Word2Vec or GloVe).\n",
    "\n",
    "    Returns:\n",
    "    - distractors (list): A list of generated distractors.\n",
    "    \"\"\"\n",
    "    distractors = []\n",
    "    \n",
    "    # You can start by making simple modifications to the correct answer, such as replacing words.\n",
    "    for _ in range(num_distractors):\n",
    "        # Randomly select a distractor generation strategy\n",
    "        strategy = random.choice([\"replace\", \"synonym\"])\n",
    "        \n",
    "        if strategy == \"replace\":\n",
    "            # Replace a random word in the correct answer with a random word\n",
    "            correct_answer_words = correct_answer.split()\n",
    "            word_to_replace = random.choice(correct_answer_words)\n",
    "            replacement_word = \"replacement_word\"  # Replace this with logic to select a random word\n",
    "            distractor = correct_answer.replace(word_to_replace, replacement_word)\n",
    "        elif strategy == \"synonym\" and word_embedding_model:\n",
    "            # Find a synonym for a random word in the correct answer using word embeddings\n",
    "            correct_answer_words = correct_answer.split()\n",
    "            word_to_replace = random.choice(correct_answer_words)\n",
    "            synonyms = find_synonyms(word_to_replace, word_embedding_model)\n",
    "            if synonyms:\n",
    "                replacement_word = random.choice(synonyms)\n",
    "                distractor = correct_answer.replace(word_to_replace, replacement_word)\n",
    "            else:\n",
    "                # If no synonyms are found, use a fallback strategy (e.g., random replacement)\n",
    "                distractor = generate_random_distractor(correct_answer)\n",
    "        else:\n",
    "            # Use a fallback strategy (e.g., random replacement) if no other strategy is chosen\n",
    "            distractor = generate_random_distractor(correct_answer)\n",
    "\n",
    "        distractors.append(distractor)\n",
    "    \n",
    "    return distractors\n",
    "\n",
    "def find_synonyms(word, word_embedding_model):\n",
    "    \"\"\"\n",
    "    Find synonyms for a word using a word embedding model.\n",
    "\n",
    "    Args:\n",
    "    - word (str): The target word.\n",
    "    - word_embedding_model: A pre-trained word embedding model (e.g., Word2Vec or GloVe).\n",
    "\n",
    "    Returns:\n",
    "    - synonyms (list): A list of synonyms for the target word.\n",
    "    \"\"\"\n",
    "    # Replace this with code to find synonyms using the word embedding model\n",
    "    # This may involve querying the model's word vectors to find similar words.\n",
    "    synonyms = []\n",
    "    return synonyms\n",
    "\n",
    "def generate_random_distractor(correct_answer):\n",
    "    \"\"\"\n",
    "    Generate a random distractor by replacing a random word in the correct answer.\n",
    "\n",
    "    Args:\n",
    "    - correct_answer (str): The correct answer.\n",
    "\n",
    "    Returns:\n",
    "    - distractor (str): The generated distractor.\n",
    "    \"\"\"\n",
    "    correct_answer_words = correct_answer.split()\n",
    "    word_to_replace = random.choice(correct_answer_words)\n",
    "    replacement_word = \"replacement_word\"  # Replace this with logic to select a random word\n",
    "    distractor = correct_answer.replace(word_to_replace, replacement_word)\n",
    "    return distractor\n",
    "\n",
    "# Example usage:\n",
    "#correct_answer = \"testing provides a Cost-effective Means of Detecting Defects.\"\n",
    "num_distractors = 3\n",
    "distractors = generate_distractors(correct_answer, num_distractors)\n",
    "\n",
    "# Print the correct answer and distractors\n",
    "print(\"Correct Answer:\", correct_answer)\n",
    "print(\"Distractors:\", distractors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here comes gradio + manual selection of correct questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained model and tokenizer\n",
    "#model_name = \"bert-large-uncased-whole-word-masking-finetuned-squad\"\n",
    "#tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "#model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "\n",
    "# Provide a passage and a question\n",
    "#passage = extracted_text\n",
    "#question = \"Which of the following statements describe a valid test objective?\"\n",
    "\n",
    "#Which of the following statements describe a valid test objective?\n",
    "#What does not work as expected?\n",
    "\n",
    "# Tokenize the passage and question\n",
    "#inputs = tokenizer(question, passage, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "# Get the answer from the model\n",
    "#start_scores, end_scores = model(**inputs, return_dict = False)\n",
    "#start_idx = torch.argmax(start_scores)\n",
    "#end_idx = torch.argmax(end_scores)\n",
    "\n",
    "# Decode the answer from the tokenized output\n",
    "#answer_tokens = inputs[\"input_ids\"][0][start_idx:end_idx + 1]\n",
    "#answer = tokenizer.decode(answer_tokens)\n",
    "\n",
    "#print(\"Answer:\", answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To process text and generate questions with answers, you can consider using pre-trained language models, such as GPT-3, GPT-4, BERT, T5, or similar models. Each of these models has its strengths and can be used for different aspects of question generation and answering:\n",
    "\n",
    "T5 (Text-to-Text Transfer Transformer): T5 is a versatile language model that can be fine-tuned for various natural language processing tasks, including question generation. You can fine-tune a pre-trained T5 model on your specific dataset to generate high-quality questions.\n",
    "\n",
    "GPT-3: OpenAI's GPT-3 is a powerful language model known for its natural language generation capabilities. You can prompt GPT-3 to generate questions based on your input text. It can produce contextually relevant questions, but it may require careful instruction and filtering of the generated output.\n",
    "\n",
    "BART (Bidirectional and Auto-Regressive Transformers): BART is another transformer-based model that can be fine-tuned for question generation tasks. It excels in text generation tasks and can produce coherent and meaningful questions.\n",
    "\n",
    "XLNet: XLNet is a transformer model that has achieved strong performance in various NLP tasks. It can be fine-tuned for question generation, and its bidirectional context modeling can lead to better question generation.\n",
    "\n",
    "BERT (Bidirectional Encoder Representations from Transformers): BERT can also be used for question generation by fine-tuning. While it was originally designed for understanding context, it can be adapted for question generation with appropriate training data.\n",
    "\n",
    "UniLM: UniLM is a model that can be used for various text generation tasks, including question generation. It combines unidirectional, bidirectional, and sequence-to-sequence learning, making it versatile for NLP tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "# from io import StringIO\n",
    "\n",
    "# # Load the pre-trained model and tokenizer\n",
    "# model_name = \"bert-large-uncased-whole-word-masking-finetuned-squad\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "\n",
    "# # Create a StringIO object with your text\n",
    "# text_io = StringIO()\n",
    "# text_io.write(\"Your text goes here.\")\n",
    "# text_io.seek(0)  # Reset the StringIO object to the beginning\n",
    "\n",
    "# # Read the text from the StringIO object and convert it to a regular string\n",
    "# text = text_io.read()\n",
    "\n",
    "# # Provide a question\n",
    "# question = \"What is the answer to my question?\"\n",
    "\n",
    "# # Tokenize the text and question\n",
    "# inputs = tokenizer(question, text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "# # Get the answer from the model\n",
    "# start_scores, end_scores = model(**inputs)\n",
    "# start_idx = torch.argmax(start_scores)\n",
    "# end_idx = torch.argmax(end_scores)\n",
    "\n",
    "# # Decode the answer from the tokenized output\n",
    "# answer_tokens = inputs[\"input_ids\"][0][start_idx:end_idx + 1]\n",
    "# answer = tokenizer.decode(answer_tokens)\n",
    "\n",
    "# print(\"Answer:\", answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Question Generation using Seq2Seq (T5)\n",
    "\n",
    "# Load the pre-trained Seq2Seq model for question generation\n",
    "# question_generation_model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "# question_generation_tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "# \n",
    "#ISTQB document (replace with your actual content)\n",
    "# istqb_document = \"\"\"\n",
    "# 1.1. What is Testing? \n",
    "# \n",
    "# Software systems are an integral part of our daily life. Most people have had experience with software \n",
    "# that did not work as expected. Software that does not work correctly can lead to many problems, \n",
    "# including loss of money, time or business reputation, and, in extreme cases, even injury or death. \n",
    "# Software testing assesses software quality and helps reducing the risk of software failure in operation. \n",
    "# \n",
    "# Software testing is a set of activities to discover defects and evaluate the quality of software artifacts. \n",
    "# These artifacts, when being tested, are known as test objects. A common misconception about testing is \n",
    "# that it only consists of executing tests (i.e., running the software and checking the test results). However, \n",
    "# software testing also includes other activities and must be aligned with the software development lifecycle \n",
    "# (see chapter 2). \n",
    "# \n",
    "# Another common misconception about testing is that testing focuses entirely on verifying the test object. \n",
    "# Whilst testing involves verification, i.e., checking whether the system meets specified requirements, it also \n",
    "# involves validation, which means checking whether the system meets users’ and other stakeholders’ \n",
    "# needs in its operational environment. \n",
    "# \"\"\"\n",
    "# \n",
    "#Generate questions from the ISTQB document\n",
    "# def generate_questions(document, max_length=64, num_questions=1):\n",
    "    # inputs = question_generation_tokenizer.encode(\"generate questions: \" + document, return_tensors=\"pt\", max_length=max_length, truncation=True)\n",
    "    # questions = question_generation_model.generate(inputs, max_length=max_length, num_return_sequences=num_questions)\n",
    "    # return [question_generation_tokenizer.decode(question, skip_special_tokens=True) for question in questions]\n",
    "# \n",
    "# generated_questions = generate_questions(istqb_document)\n",
    "# \n",
    "#Print generated questions\n",
    "# for question in generated_questions:\n",
    "    # print(\"Question:\", question)\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is template for Quize layout, needs to be re-worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to display sections\n",
    "def display_sections(section_title, sections):\n",
    "    # Find the selected section in the list based on the section title\n",
    "    selected_section = next((s for s in sections if s[0] == section_title), None)\n",
    "\n",
    "    if selected_section:\n",
    "        _, section_content = selected_section\n",
    "        return section_content\n",
    "    else:\n",
    "        return \"Section not found\"\n",
    "\n",
    "# Get a list of section titles from the sections list\n",
    "section_titles = [section for section in sections]\n",
    "\n",
    "# Create a Gradio interface with a dropdown list of section titles and a textbox for content\n",
    "iface = gr.Interface(\n",
    "    fn=display_sections,\n",
    "    inputs=[gr.inputs.Dropdown(section_titles, label=\"Select a Section\"), gr.inputs.Textbox(lines = 10, default=\"Selected Section Content\", label=\"Section Content\")],\n",
    "    outputs=\"text\",\n",
    "    flagging_options = ['yes', 'no'],\n",
    "    theme=gr.themes.Soft()\n",
    ")\n",
    "\n",
    "iface.launch(share=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JSON converter from CSVlogger() to JSON, this is when the previous step w questions selection is finished via Flag button"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set path to both files\n",
    "csv_file = 'flagged/log.csv'\n",
    "json_file = 'flagged/questions_answers.json'\n",
    "\n",
    "# empty list to store the JSON data\n",
    "json_data = []\n",
    "\n",
    "# Open the CSV file for reading\n",
    "with open(csv_file, 'r') as csvfile:\n",
    "    # Create a CSV reader object\n",
    "    csvreader = csv.DictReader(csvfile)\n",
    "    \n",
    "    # Iterate through each row in the CSV file\n",
    "    for row in csvreader:\n",
    "        # Append each row as a dictionary to the JSON data list\n",
    "        json_data.append(row)\n",
    "\n",
    "# Open the JSON file for writing and save the JSON data\n",
    "with open(json_file, 'w') as jsonfile:\n",
    "    json.dump(json_data, jsonfile, indent=4)\n",
    "\n",
    "print(f\"CSV data has been converted to JSON successfully and saved as {json_file}. The file contains final result of Quiz generator. \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draft metrics\n",
    "\n",
    "import nltk\n",
    "\n",
    "# Reference questions (human-generated)\n",
    "reference_questions = [\n",
    "    \"What is the test objective?\",\n",
    "    \"How do objectives vary?\",\n",
    "    \"What does the context include?\",\n",
    "    # Add more reference questions here\n",
    "]\n",
    "\n",
    "# Automatically generated questions\n",
    "generated_questions = [\n",
    "    \"What is test objectives?\",\n",
    "    \"What is objectives vary?\",\n",
    "    \"How do objectives depend?\",\n",
    "    # Add more generated questions here\n",
    "]\n",
    "\n",
    "# Initialize the NLTK BLEU scorer\n",
    "bleu_scorer = nltk.translate.bleu_score.SmoothingFunction()\n",
    "\n",
    "# Calculate BLEU score (a measure of similarity)\n",
    "bleu_scores = [nltk.translate.bleu_score.sentence_bleu([r.split()], g.split(), smoothing_function=bleu_scorer.method1) for r, g in zip(reference_questions, generated_questions)]\n",
    "\n",
    "# Calculate accuracy rate (percentage of questions that match reference questions)\n",
    "accuracy_rate = sum(score == 1.0 for score in bleu_scores) / len(bleu_scores) * 100\n",
    "\n",
    "print(\"Accuracy Rate: {:.2f}%\".format(accuracy_rate))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from selenium import webdriver\n",
    "\n",
    "#driver=webdriver.Chrome()\n",
    "\n",
    "#driver.get(\"https://www.facebook.com/\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip show selenium\n",
    "#!pip install selenium==4.12.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract api key\n",
    "\n",
    "#from pyChatGPT import ChatGPT\n",
    "#from selenium.webdriver.common.by import By\n",
    "#from selenium.webdriver.support.ui import WebDriverWait\n",
    "#from selenium.webdriver.support import expected_conditions as EC\n",
    "#from selenium.webdriver.common.action_chains import ActionChains\n",
    "#from selenium.webdriver.support.ui import WebDriverWait##\n",
    "\n",
    "#session_token = \"eyJhbGciOiJkaXIiLCJlbmMiOiJBMjU2R0NNIn0..RSoIbYuWt0dZzv_n.yzUd3McMevS948NpJcruJnsILZCqj30VJnxYzcJRnQ38WO9xCyF1LazX-6kARlOnQccDdoakpHjCQ_1NsMO-8MMLm1RKVFdWG3QmH5CCfFAFwZlGiVo-Fj04fVnxFZCvw3j4ouaqA2XwELxW0m9Q_fhCqy8ZLaiF4YpJCmjubt4A9HAJZ0pbrNknQq-OL62DJXJuOL92t_pE-jZpgMIZA3jTZHdvZUzZqfcqM26KKikwJg_WD5wAmLAqz_whTt2p6mNei2Yt6reJQ_uP_5Cwr6Ae9uEF3rX-h0ylmz_di8Ntexgk5nlN2dU4gHEWoNUo0Nf8tqXQMHfoQn6LS3AnFIcDEAAA5s7QTmJZ4hkyCAUk1TOXRG3afrEJD1snnXvrJkv7skXMQfDYhneBE8lUnTQpTJzRxW3KUfXbx89vRXCcboP-LvhTZ_q3adKGiQT5ZhJ6Gb5pCrxFVS34Y7996VfseEbf7duaSW58UNp8mG95YyQtQM1JwaegR9TdE37L3-oNVpoJtS2CzbH6UyS4Ddk6z6IDT-3-5EMLOzO2Bz16CV87DsB---SCsnOll62LXaRHYrdc7Y43u7JMCWgQtLeO0Zm-8H25HkfYhm5YvqBW7pUTeOo43cWZPFDTIcDulunZh68S065c3GzvokIVoAkMieZBzwZQ-KAFjisxntaO69cYVJ_OVjX7aJvXHvsCZcX0jcHwlkooIqn_dbaXgl7718We_QGgLVz6hsDvUlsT3FWfKOP77PSWY20Vvd3Vn3LMzBkxIhLIENpPLHx6kHa_BIBlLdUDJaZKTGCLKvm7GGzgv7kAaQe4bIC-uXGjZDifyP__Wj5M3xxkwY7rAF-voYEvGGoArczz5sQuX2qWBLyQ7FpvgC2lZwarkz9ZWoLBWMgv2jf7Ypm7cBuXnLhzK1TV9tGMinrkWXqjNA9cFLqU3oQ-212hVorUOBj2gG8MO5Z6vIi1GGtrUGN4zvKyDikL1WrBLFZPOPnp2pNX63i2F-6PjqMI9u1X1ZKMqN-viz8JOuFmW6KMayCUWGrKtKXn0_wk8SkB8bzOmZ4w6WbZavlsS23v8NOJkwm2sZOoovKEsHNOQEbnZGGckGZwXNKHnkK4nLJrW-00MfBs3Lq597UbDpGKGqPHh8kOCjGnaACg27CCWckmglgnNwsU6NNv360jIKgFAuT5mJFFr1h8fUFWB0T7V3QbdXOGV9J4g0NP2cIGE-tXOiQquI84kpxjjOVOE_jPAVNEVF0hrBC6Eg4xMYas0mVQ2K-fmZrRtmNKi1UMq2iQALbrPm0UA0LToAqURFhAdUdyYoXj4hGVbvmKBGN8P-0fjEi4lYKBnPsBiWkejTnogTJc_5T2RePoHkopy6_9q3lCD2JcNHkkUlJbCYfmAC7NJNsM-3N-SJ0Glb7KmYx8sY3MqRJHvLPLeo7I5QxAgKeKurgRr3lxxNJbcliL28A5hauqEjUMJnWf6BqhpplcQXLQYlsLn4u9DLuMXXdhbNUp-CvGM0SrydLpy1Y819YaZI7ds88bNB5Hq4e35AqRBo_uDuU2MY5infiyv4-2DvoWcJVl6Dcmag_KwXaGvZewm7lPbG8dxDNPfWmxMBsqDRk62rbzLe3o-EphkTkyEfTreWOMrQSTkrEwdGMzG8sXiV8hHcfbAT_qL4vzNIrs0gxsKqt0O0ecGUG1wbW6PXhArlA0Z_ot-NK7g7aP3ntgdsdaIrqw6FCnKzQnXT35oVD0InGO4EbZnIcg4YyTepjWs1LrUjtTVagWPagr3c-kICpJfjw4jERhdsvQFS1V_QFG_SjT00XW52FvU5r5IeMLjz0r7Khphq5B8XveUAvl65X36mvS-npVHGdfrCBsCIxSXpKVT8o_acGzM9vFpVtdALy32dQap92Lqv-YtQfyMCkyfYXaH4nlEYpMfxNLzD_M98nD35PTr3ovA85JtYMP7fWzV7QiQzx5sqgtsXlbBQeMTA0httZZj8qZ7VtWKGv0BbWkPTQRlh-9JbPYJS0KB791Djdw-nfdRS1sSi7972WykK91W6iCrRff_BCWXzn83-YCk1_F4rn420cDLVlB3hDvIO-SiQmCuhH9xCnX9a0PKGPT82YvCYVuO3Ets0mYHyLd4GLpqvF0KHjqGnIOAzuzU7QfECz6JNCkm1yLa8QVY978gyi68pxJAVVVAVs546FwP2pLjv3RQkt5cfzGpkP4wMt-tXVJR7SCy-Sntqfpn5L4heisOjo6EHkpgP4OTyv-kUPa5HOxkIAVrmG7g2Qwvn-SjfkEG_jZHqFkl4unsECiSKN1fZMaTTIC9qOJ8YFGPsBijhR-toUd5EmtYQJ_rW8aosszMkQmPy1MRKv0Iwab6QYm1ncWyqLzfJMTFtYRRDnyclfqr1oEwS-TbBFRJZADN8I677a7CNiq_JIphlx5w8kB7ADtk--zAHmCVlycAza6EgArS4WvgRJ18QYNjN1-e-3v1ISrxSNjXRK8PAIGqInHGJ7EpkmXDmS5I4HlbDakdtPeAiLTLuaXvctbbSWg5vSgLLIBVwskpcLBhCpotHuhou_ZLTU6q40bfee8u0Gt99mTswb5BkS9DCJlpGNAr99GVSYpvhyPSbQfnScN4EnBK-aoySeoaqnTVdLCqTlhgJqLKlVyNf6-nASXvBiEQ0pjcdk3AxQ_j_7aCGj4Eo0hhSndlhZoXaU22y_DV6CY8CxDqJrbwU2NZDKvJZjZmVza6RH4FAVHgnlwsgKCb-I7Ccpw93EhrOmgpObGD3fEIhr0TKTxVeqxkjWK9wz672kndN__FQEnOA9Q664rCQcro349u0yOs50XtNZZ6nnuV3WfQZ1pkPxlmMyfgp0T.FCUbJELvyDNKws3nIQugCg\"\n",
    "##session_token = \"eyJhbGciOiJkaXIiLCJlbmMiOiJBMjU2R0NNIn0..Tp3P5x2i7af5GTfN.y6_A8C625dFSW8oSdmTLRQvvjYS8QmcPFKEDpLl-PHD1KngeOZFs78Jmpcrx_AgAxNcK4aWi_EHRiRGHUcUx4fnpFH0artO90rhECxVFoVJ4iouZ0477aTbDpfC19-y7G9xnCdlmb3CHTbz1l7ifsq3N9HwThAuybG_uYzAkYH74HKtG28REuxEea5EfTSS-1gracdfxuFtGWb1DvXJ_xpczUvbI_7vrHeuD-wWyrnH9a1NU484r1YntPLG9hhwHmbQ8YbdQyDv-Jl7LqHM1GGSFx8Q0ebW1Y3nxYyQnZ13zEYSosudoQQ4pd80WD3a2VXbdDpSWlFIjDt5V9pXISzc85KUha45JLAEr0EPK5eJaaWRdHIgEYrg6vRWbMaGQ4kz7YWDgd_VzOyHYQeU11UMeFuqdlafQWBo-dv2FOb-GbDKy85vtwLYvIoTgRH3e7ixNWVzbnjWtR_bSBpMEBjQHJG0E7KWHFkamPoav1SyUy-SiOVJKBPbRgxnFC25LWfiJGmXgRV1L7wIYorMxJ3lJKjXH7in8cEl64dn2l7FJ5EH9eeZg1nIeIsSO5lSATkG1ueLY8cuQeEJPL6LZzVuVJV7v-K2C5vrgft0yoU9-LEyXry9bU5WUZf1PKOSr8VS37MRZwMlWmbBrLkQaqHSAB2l3HS5T9kO2ZafyGc6d8luoVvIYxeIXlPrI6j6k3PaPPclh9ACi8pIay17TmVfSbBVGgTZp2VbP37Yfri5tG0dYkZEjXDUkd-NDdxVfe1ChQLwLeN6NUHu-IgTOUK3sqPNU9HYptdz1yD3GfpJY5qOKtGt4D-OoECahIeSzxNYlo5JY4rXviSeGeW2PLvq3pkD4MN5jrgzjCZ9getdAua8yEGvT5_A-uzN8w9gFw8kdhgK3BwOhdfU1cY_OUDoDTIZ12MJbkAH9BzcMXU6P9CoNshyQM8dE1-D_WNxySwVcTSwfOuaZ9MQIhLWa8geqvhVqNDjLzYfKjaydyr6wPwDsBLh9dTqQmmp88aD3VJ-Is6aqRjUIUlSOTsA8eNL5vy-cNZufRX4OrXB9p_Az0PfFNvjSptzoMGmH-0YXiyP4yENYtNSTagawc_PlB66IuUlOmCuyXmVXlzRKTEiAbwNkVrfbBB2ezw6g4cQ6ROROEuCXfBCKh5GjQ-IJgxQRjf6OOaczo-G_YYUnZhuZK2NgEPeXfS18QT_dAKdJLB4PySxitP-MKjOGoWOWbm858Uod1NwQQUMmO7JYAzqxrfNxxFkEA60XTxM-QkZnssgsdjFy0uGEXuwBaoYuYX6_WX9k71ixZ47nNfhRVfTPvXb_8GWVuf_QmwKLYg6ZEcGgGyDiIjhdypr2bretXyR5AfYj1Cludvmc5frm2IXGEJTQIHUkx5sW70ycXtOWNqHfE5sXlt9zyM12FywccPoxUZjZM4YdSleaL-smbApxa-AKjxzTWIgPJHpd-nxputQJdRYjAHLCqPVCycNse5q1y5oVJUCkaDgd4maMA0kBRJf3ftQG1UnG_W2Ao_sf-Qqrflzu80xiv30AjTmoHwGih8u4rB-iMS6Arj2Bbcl3EO24svdqsVCCS-smAasHsDP2pVPCw2HWAXmNNv5IDTI4TlNYhabCnm1eG50rmOXWy_2Qhp9sI1trOrxFqyEmkMyK_iypLFIL-DlrpJ8KZc5KPW6s_prVv7rGRcvEvf_AZVfoJ9UWLxg0KtUH3xoFAFhpqpW1NO3CdjzeBbEbs9xKU7zLFeQwbZU3oZCzjJ3HAKc1enlU48D64aRRw2ZaTnlLHw5tJDSpchBUJX5jyAo9WXVhvjLCe3Coj-OfwqsUyPkkCal33h7O8twSIIFe7THN0rPux-Fjxk1s11GfB9QvppgibwV7L7m6ODTelMofI17eeMGxJcBCePZzVfSVIp3SbTsQhkkU-UErI27TdO3j0_r4d4ceBKSF2kGxRFVCqoKQ2O6ifv_vMdiOlDlCFHgiSs933MuAF5OrWavrGp59gWamUfJXMsX_CqeP8B8Fu-gI3nWR-qX5Xyu7gpphdkn60Dsnw_I174OY9p0RbS01kGiRGCA5wzv5OQaJYcvIOzLr9O9Pa_nWK6jIx4u7REN6Z6-B4Q6dK7Pc1nWPuUQNS1rPkxVYHq2gUDE7kLch5JvCLUImRFceTpiw1QeScTi_p3GHQXIzvrjHv0PlZ5c7fs2VTwM9wUcFEX24C-lJhUnKnPeyI3gK15hQSHe0EClUzrdf_159KKY6VgE4ig3lwAQYVU4zD_ZJTrs3clRRVNufg-XEDFEJ_MxCme-tjskTxUUglTDRKwD4Ij8Ok31yQiAOcflsF-CG6AY3HyVObVLqp3jCS9kPRzQ4gJRuhd4Zhh3QpmzVk4Q0iCjom4jfzVEUWAhtl8399LG6b3kNnbBWX_ON_1aj58JlU-QYeGHuZ7olbrKVkphljS9-lUjTMp5DJmG3D_L4yIVP25VDVceM4K1TGW4m_FaGjuKsmhLHgP4IMBRYgvHAeLoHqfwXORBnQ1M0mGAHsFt1i7kTOBVwJx1v8slmuFGdYgIJtRi7qPjOPvqaBmFg7_PT-tLC1R_8D0zZNBCkZAHYq7JQM09HV6qNj7s_zShfxN2crakn6J8VPDashRiYAysOq3V-ZB1Efai3bkWl90ae1WaWZtCH7ZVc3Z7ie3anyMSxYfLzGlaRm_dOgA0_SUws90cYbhWkzHBunGmUMCbf50PwC-InQ6-6Y1LIPEkH591wITeiqgI7SYcx30c12kA06-iRQ7yPZzD-bmWiEXd1_KQgb85vMf3wR3E-haRiVBYHVF_CHc-JM28GuVFmKrDilWt7VyEy5XwpVaZw0p4l0ukVUgkv.V0DZCdRFRZKFkglRGB0WIg\"\n",
    "#api = ChatGPT(session_token)#\n",
    "\n",
    "##textbox_xpath = '//*[@id=\"radix-:rs:\"]/div[2]/div/div[4]/button'\n",
    "##script = 'alert(\"Hello, from JavaScript!\");'\n",
    "##script = f'document.evaluate(\\'{textbox_xpath}\\', document, null, XPathResult.FIRST_ORDERED_NODE_TYPE, null).singleNodeValue.click();'\n",
    "##api.driver.execute_script(script)\n",
    "##button = WebDriverWait(driver, 10).until(\n",
    "##    EC.element_to_be_clickable((By.XPATH, '//*[@id=\"radix-:rs:\"]/div[2]/div/div[4]/button'))\n",
    "##)#\n",
    "\n",
    "## Click the button\n",
    "##button.click()#\n",
    "#\n",
    "\n",
    "## Click the button\n",
    "##button.click()\n",
    "#resp = api.send_message('Write an essay on Generative AI')#\n",
    "\n",
    "## Locate and scroll to the \"Okay, let's go\" button\n",
    "#button = api.driver.find_element(By.XPATH, '//*[@id=\"radix-:rl:\"]/div[2]/div/div[4]/button')\n",
    "##api.driver.execute_script(\"arguments[0].scrollIntoView();\", button)#\n",
    "\n",
    "## Click the button\n",
    "#button.click()#\n",
    "#\n",
    "#\n",
    "\n",
    "#resp = api.send_message(' AI')\n",
    "##button1 = api.driver.find_element(By.XPATH, '//*[@id=\"__next\"]/div[1]/div[2]/div/main/div[1]/div[2]/form/div/div[2]/div/button')\n",
    "##button.click()\n",
    "#api.driver.quit()#\n",
    "\n",
    "## Close the browser when done\n",
    "##api.driver.quit()#\n",
    "#\n",
    "\n",
    "##print(resp['message'])\n",
    "##api.refresh_auth()  # refresh the authorization token\n",
    "##api.reset_conversation()  # reset the conversation#\n",
    "\n",
    "##button_xpath = '//*[@id=\"radix-:rs:\"]/div[2]/div/div[4]/button/div'\n",
    "##button_element = driver.find_element(By.XPATH, button_xpath)\n",
    "##button_element.click()#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qgmc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
