1.1. What is Testing? Software systems are an integral part of our daily life. Most people have had experience with software that did not work as expected. Software that does not work correctly can lead to many problems including loss of money time or business reputation and in extreme cases even injury or death. Software testing assesses software quality and helps reducing risk of software failure in operation. Software testing is a set of activities to discover defects and evaluate quality of software artifacts. These artifacts when being tested are known as test objects. A common misconception about testing is that it only consists of executing tests (i.e. running software and checking test results). However software testing also includes other activities and must be aligned with software development lifecycle (see chapter 2). Another common misconception about testing is that testing focuses entirely on verifying test object. Whilst testing involves verification i.e. checking whether system meets specified requirements it also involves validation which means checking whether system meets users’ and other stakeholders’ needs in its operational environment. Testing may be dynamic or static. Dynamic testing involves execution of software while static testing does not. Static testing includes reviews (see chapter 3) and static analysis. Dynamic testing uses different types of test techniques and test approaches to derive test cases (see chapter 4). Testing is not only a technical activity. It also needs to be properly planned managed estimated monitored and controlled (see chapter 5). Testers use tools (see chapter 6) but it is important to remember that testing is largely an intellectual activity requiring testers to have specialized knowledge use analytical skills and apply critical thinking and systems thinking (Myers 2011 Roman 2018). ISO/IEC/IEEE 29119-1 standard provides further information about software testing concepts. 1.1.1. Test Objectives typical test objectives are: • Evaluating work products such as requirements user stories designs and code • Triggering failures and finding defects • Ensuring required coverage of a test object • Reducing level of risk of inadequate software quality • Verifying whether specified requirements have been fulfilled • Verifying that a test object complies with contractual legal and regulatory requirements • Providing information to stakeholders to allow them to make informed decisions • Building confidence in quality of test object • Validating whether test object is complete and works as expected by stakeholders Objectives of testing can vary depending upon context which includes work product being tested test level risks software development lifecycle (SDLC) being followed and factors related to business context e.g. corporate structure competitive considerations or time to market. v4.0 Page of 74 2023-04-21  1.1.2. Testing and Debugging Testing and debugging are separate activities. Testing can trigger failures that are caused by defects in software (dynamic testing) or can directly find defects in test object (static testing). When dynamic testing (see chapter 4) triggers a failure debugging is concerned with finding causes of this failure (defects) analyzing these causes and eliminating them. typical debugging process in this case involves: • Reproduction of a failure • Diagnosis (finding root cause) • Fixing cause Subsequent confirmation testing checks whether fixes resolved problem. Preferably confirmation testing is done by same person who performed initial test. Subsequent regression testing can also be performed to check whether fixes are causing failures in other parts of test object (see section 2.2.3 for more information on confirmation testing and regression testing). When static testing identifies a defect debugging is concerned with removing it. There is no need for reproduction or diagnosis since static testing directly finds defects and cannot cause failures (see chapter 3). 1.2. Why is Testing Necessary? Testing as a form of quality control helps in achieving agreed upon goals within set scope time quality and budget constraints. Testing’s contribution to success should not be restricted to test team activities. Any stakeholder can use their testing skills to bring project closer to success. Testing components systems and associated documentation helps to identify defects in software 1.2.1. Testing’s Contributions to Success Testing provides a cost-effective means of detecting defects. These defects can then be removed (by debugging – a non-testing activity) so testing indirectly contributes to higher quality test objects. Testing provides a means of directly evaluating quality of a test object at various stages in SDLC. These measures are used as part of a larger project management activity contributing to decisions to move to next stage of SDLC such as release decision. Testing provides users with indirect representation on development project. Testers ensure that their understanding of users’ needs are considered throughout development lifecycle. alternative is to involve a representative set of users as part of development project which is not usually possible due to high costs and lack of availability of suitable users. Testing may also be required to meet contractual or legal requirements or to comply with regulatory standards. 1.2.2. Testing and Quality Assurance (QA) While people often use terms “testing” and “quality assurance” (QA) interchangeably testing and QA are not same. Testing is a form of quality control (QC). v4.0 Page of 74 2023-04-21  QC is a product-oriented corrective approach that focuses on those activities supporting achievement of appropriate levels of quality. Testing is a major form of quality control while others include formal methods (model checking and proof of correctness) simulation and prototyping. QA is a process-oriented preventive approach that focuses on implementation and improvement of processes. It works on basis that if a good process is followed correctly then it will generate a good product. QA applies to both development and testing processes and is responsibility of everyone on a project. Test results are used by QA and QC. In QC they are used to fix defects while in QA they provide feedback on how well development and test processes are performing. 1.2.3. Errors Defects Failures and Root Causes Human beings make errors (mistakes) which produce defects (faults bugs) which in turn may result in failures. Humans make errors for various reasons such as time pressure complexity of work products processes infrastructure or interactions or simply because they are tired or lack adequate training. Defects can be found in documentation such as a requirements specification or a test script in source code or in a supporting artifact such as a build file. Defects in artifacts produced earlier in SDLC if undetected often lead to defective artifacts later in lifecycle. If a defect in code is executed system may fail to do what it should do or do something it shouldn’t causing a failure. Some defects will always result in a failure if executed while others will only result in a failure in specific circumstances and some may never result in a failure. Errors and defects are not only cause of failures. Failures can also be caused by environmental conditions such as when radiation or electromagnetic field cause defects in firmware. A root cause is a fundamental reason for occurrence of a problem (e.g. a situation that leads to an error). Root causes are identified through root cause analysis which is typically performed when a failure occurs or a defect is identified. It is believed that further similar failures or defects can be prevented or their frequency reduced by addressing root cause such as by removing it. 1.3. Testing Principles A number of testing principles offering general guidelines applicable to all testing have been suggested over years. This syllabus describes seven such principles. 1. Testing shows presence not absence of defects. Testing can show that defects are present in test object but cannot prove that there are no defects (Buxton 1970). Testing reduces probability of defects remaining undiscovered in test object but even if no defects are found testing cannot prove test object correctness. 2. Exhaustive testing is impossible. Testing everything is not feasible except in trivial cases (Manna 1978). Rather than attempting to test exhaustively test techniques (see chapter 4) test case prioritization (see section 5.1.5) and risk-based testing (see section 5.2) should be used to focus test efforts. 3. Early testing saves time and money. Defects that are removed early in process will not cause subsequent defects in derived work products. cost of quality will be reduced since fewer failures will occur later in SDLC (Boehm 1981). To find defects early both static testing (see chapter 3) and dynamic testing (see chapter 4) should be started as early as possible. 4. Defects cluster together. A small number of system components usually contain most of defects discovered or are responsible for most of operational failures (Enders 1975). This phenomenon is an v4.0 Page of 74 2023-04-21  illustration of Pareto principle. Predicted defect clusters and actual defect clusters observed during testing or in operation are an important input for risk-based testing (see section 5.2). 5. Tests wear out. If same tests are repeated many times they become increasingly ineffective in detecting new defects (Beizer 1990). To overcome this effect existing tests and test data may need to be modified and new tests may need to be written. However in some cases repeating same tests can have a beneficial outcome e.g. in automated regression testing (see section 2.2.3). 6. Testing is context dependent. There is no single universally applicable approach to testing. Testing is done differently in different contexts (Kaner 2011). 7. Absence-of-defects fallacy. It is a fallacy (i.e. a misconception) to expect that software verification will ensure success of a system. Thoroughly testing all specified requirements and fixing all defects found could still produce a system that does not fulfill users’ needs and expectations that does not help in achieving customer’s business goals and that is inferior compared to other competing systems. In addition to verification validation should also be carried out (Boehm 1981). 1.4. Test Activities Testware and Test Roles Testing is context dependent but at a high level there are common sets of test activities without which testing is less likely to achieve test objectives. These sets of test activities form a test process. test process can be tailored to a given situation based on various factors. Which test activities are included in this test process how they are implemented and when they occur is normally decided as part of test planning for specific situation (see section 5.1). following sections describe general aspects of this test process in terms of test activities and tasks impact of context testware traceability between test basis and testware and testing roles. ISO/IEC/IEEE 29119-2 standard provides further information about test processes. 1.4.1. Test Activities and Tasks A test process usually consists of main groups of activities described below. Although many of these activities may appear to follow a logical sequence they are often implemented iteratively or in parallel. These testing activities usually need to be tailored to system and project. Test planning consists of defining test objectives and then selecting an approach that best achieves objectives within constraints imposed by overall context. Test planning is further explained in section 5.1. Test monitoring and control. Test monitoring involves ongoing checking of all test activities and comparison of actual progress against plan. Test control involves taking actions necessary to meet objectives of testing. Test monitoring and control are further explained in section 5.3. Test analysis includes analyzing test basis to identify testable features and to define and prioritize associated test conditions together with related risks and risk levels (see section 5.2). test basis and test objects are also evaluated to identify defects they may contain and to assess their testability. Test analysis is often supported by use of test techniques (see chapter 4). Test analysis answers question “what to test?” in terms of measurable coverage criteria. Test design includes elaborating test conditions into test cases and other testware (e.g. test charters). This activity often involves identification of coverage items which serve as a guide to specify test case inputs. Test techniques (see chapter 4) can be used to support this activity. Test design v4.0 Page of 74 2023-04-21  also includes defining test data requirements designing test environment and identifying any other required infrastructure and tools. Test design answers question “how to test?”. Test implementation includes creating or acquiring testware necessary for test execution (e.g. test data). Test cases can be organized into test procedures and are often assembled into test suites. Manual and automated test scripts are created. Test procedures are prioritized and arranged within a test execution schedule for efficient test execution (see section 5.1.5). test environment is built and verified to be set up correctly. Test execution includes running tests in accordance with test execution schedule (test runs). Test execution may be manual or automated. Test execution can take many forms including continuous testing or pair testing sessions. Actual test results are compared with expected results. test results are logged. Anomalies are analyzed to identify their likely causes. This analysis allows us to report anomalies based on failures observed (see section 5.5). Test completion activities usually occur at project milestones (e.g. release end of iteration test level completion) for any unresolved defects change requests or product backlog items created. Any testware that may be useful in future is identified and archived or handed over to appropriate teams. test environment is shut down to an agreed state. test activities are analyzed to identify lessons learned and improvements for future iterations releases or projects (see section 2.1.6). A test completion report is created and communicated to stakeholders. 1.4.2. Test Process in Context Testing is not performed in isolation. Test activities are an integral part of development processes carried out within an organization. Testing is also funded by stakeholders and its final goal is to help fulfill stakeholders’ business needs. Therefore way testing is carried out will depend on a number of contextual factors including: • Stakeholders (needs expectations requirements willingness to cooperate etc.) • Team members (skills knowledge level of experience availability training needs etc.) • Business domain (criticality of test object identified risks needs specific legal regulations etc.) • Technical factors (type of software product architecture technology used etc.) • Project constraints (scope time budget resources etc.) • Organizational factors (organizational structure existing policies practices used etc.) • Software development lifecycle (engineering practices development methods etc.) • Tools (availability usability compliance etc.) These factors will have an impact on many test-related issues including: test strategy test techniques used degree of test automation required level of coverage level of detail of test documentation reporting etc. 1.4.3. Testware Testware is created as output work products from test activities described in section 1.4.1. There is a significant variation in how different organizations produce shape name organize and manage their v4.0 Page of 74 2023-04-21  work products. Proper configuration management (see section 5.4) ensures consistency and integrity of work products. following list of work products is not exhaustive: • Test planning work products include: test plan test schedule risk register and entry and exit criteria (see section 5.1). Risk register is a list of risks together with risk likelihood risk impact and information about risk mitigation (see section 5.2). Test schedule risk register and entry and exit criteria are often a part of test plan. • Test monitoring and control work products include: test progress reports (see section 5.3.2) documentation of control directives (see section 5.3) and risk information (see section 5.2). • Test analysis work products include: (prioritized) test conditions (e.g. acceptance criteria see section 4.5.2) and defect reports regarding defects in test basis (if not fixed directly). • Test design work products include: (prioritized) test cases test charters coverage items test data requirements and test environment requirements. • Test implementation work products include: test procedures automated test scripts test suites test data test execution schedule and test environment elements. Examples of test environment elements include: stubs drivers simulators and service virtualizations. • Test execution work products include: test logs and defect reports (see section 5.5). • Test completion work products include: test completion report (see section 5.3.2) action items for improvement of subsequent projects or iterations documented lessons learned and change requests (e.g. as product backlog items). 1.4.4. Traceability between Test Basis and Testware In order to implement effective test monitoring and control it is important to establish and maintain traceability throughout test process between test basis elements testware associated with these elements (e.g. test conditions risks test cases) test results and detected defects. Accurate traceability supports coverage evaluation so it is very useful if measurable coverage criteria are defined in test basis. coverage criteria can function as key performance indicators to drive activities that show to what extent test objectives have been achieved (see section 1.1.1). For example: • Traceability of test cases to requirements can verify that requirements are covered by test cases. • Traceability of test results to risks can be used to evaluate level of residual risk in a test object. In addition to evaluating coverage good traceability makes it possible to determine impact of changes facilitates test audits and helps meet IT governance criteria. Good traceability also makes test progress and completion reports more easily understandable by including status of test basis elements. This can also assist in communicating technical aspects of testing to stakeholders in an understandable manner. Traceability provides information to assess product quality process capability and project progress against business goals. v4.0 Page of 74 2023-04-21  1.4.5. Roles in Testing In this syllabus two principal roles in testing are covered: a test management role and a testing role. activities and tasks assigned to these two roles depend on factors such as project and product context skills of people in roles and organization. test management role takes overall responsibility for test process test team and leadership of test activities. test management role is mainly focused on activities of test planning test monitoring and control and test completion. way in which test management role is carried out varies depending on context. For example in Agile software development some of test management tasks may be handled by Agile team. Tasks that span multiple teams or entire organization may be performed by test managers outside of development team. testing role takes overall responsibility for engineering (technical) aspect of testing. testing role is mainly focused on activities of test analysis test design test implementation and test execution. Different people may take on these roles at different times. For example test management role can be performed by a team leader by a test manager by a development manager etc. It is also possible for one person to take on roles of testing and test management at same time. 1.5. Essential Skills and Good Practices in Testing Skill is ability to do something well that comes from one’s knowledge practice and aptitude. Good testers should possess some essential skills to do their job well. Good testers should be effective team players and should be able to perform testing on different levels of test independence. 1.5.1. Generic Skills Required for Testing While being generic following skills are particularly relevant for testers: • Testing knowledge (to increase effectiveness of testing e.g. by using test techniques) • Thoroughness carefulness curiosity attention to details being methodical (to identify defects especially ones that are difficult to find) • Good communication skills active listening being a team player (to interact effectively with all stakeholders to convey information to others to be understood and to report and discuss defects) • Analytical thinking critical thinking creativity (to increase effectiveness of testing) • Technical knowledge (to increase efficiency of testing e.g. by using appropriate test tools) • Domain knowledge (to be able to understand and to communicate with end users/business representatives) Testers are often bearers of bad news. It is a common human trait to blame bearer of bad news. This makes communication skills crucial for testers. Communicating test results may be perceived as criticism of product and of its author. Confirmation bias can make it difficult to accept information that disagrees with currently held beliefs. Some people may perceive testing as a destructive activity even though it contributes greatly to project success and product quality. To try to improve this view information about defects and failures should be communicated in a constructive way. v4.0 Page of 74 2023-04-21  1.5.2. Whole Team Approach One of important skills for a tester is ability to work effectively in a team context and to contribute positively to team goals. whole team approach – a practice coming from Extreme Programming (see section 2.1) – builds upon this skill. In whole-team approach any team member with necessary knowledge and skills can perform any task and everyone is responsible for quality. team members share same workspace (physical or virtual) as co-location facilitates communication and interaction. whole team approach improves team dynamics enhances communication and collaboration within team and creates synergy by allowing various skill sets within team to be leveraged for benefit of project. Testers work closely with other team members to ensure that desired quality levels are achieved. This includes collaborating with business representatives to help them create suitable acceptance tests and working with developers to agree on test strategy and decide on test automation approaches. Testers can thus transfer testing knowledge to other team members and influence development of product. Depending on context whole team approach may not always be appropriate. For instance in some situations such as safety-critical a high level of test independence may be needed. 1.5.3. Independence of Testing A certain degree of independence makes tester more effective at finding defects due to differences between author’s and tester’s cognitive biases (cf. Salman 1995). Independence is not however a replacement for familiarity e.g. developers can efficiently find many defects in their own code. Work products can be tested by their author (no independence) by author's peers from same team (some independence) by testers from outside author's team but within organization (high independence) or by testers from outside organization (very high independence). For most projects it is usually best to carry out testing with multiple levels of independence (e.g. developers performing component and component integration testing test team performing system and system integration testing and business representatives performing acceptance testing). main benefit of independence of testing is that independent testers are likely to recognize different kinds of failures and defects compared to developers because of their different backgrounds technical perspectives and biases. Moreover an independent tester can verify challenge or disprove assumptions made by stakeholders during specification and implementation of system. However there are also some drawbacks. Independent testers may be isolated from development team which may lead to a lack of collaboration communication problems or an adversarial relationship with development team. Developers may lose a sense of responsibility for quality. Independent testers may be seen as a bottleneck or be blamed for delays in release. v4.0 Page of 74 2023-04-21  2. Testing Throughout Software Development Lifecycle – 130 minutes Keywords acceptance testing black-box testing component integration testing component testing confirmation testing functional testing integration testing maintenance testing non-functional testing regression testing shift-left system integration testing system testing test level test object test type white-box testing Learning Objectives for Chapter 2: 2.1 Testing in Context of a Software Development Lifecycle FL-2.1.1 (K2) Explain impact of chosen software development lifecycle on testing FL-2.1.2 (K1) Recall good testing practices that apply to all software development lifecycles FL-2.1.3 (K1) Recall examples of test-first approaches to development FL-2.1.4 (K2) Summarize how DevOps might have an impact on testing FL-2.1.5 (K2) Explain shift-left approach FL-2.1.6 (K2) Explain how retrospectives can be used as a mechanism for process improvement 2.2 Test Levels and Test Types FL-2.2.1 (K2) Distinguish different test levels FL-2.2.2 (K2) Distinguish different test types FL-2.2.3 (K2) Distinguish confirmation testing from regression testing 2.3 Maintenance Testing FL-2.3.1 (K2) Summarize maintenance testing and its triggers v4.0 Page of 74 2023-04-21  2.1. Testing in Context of a Software Development Lifecycle A software development lifecycle (SDLC) model is an abstract high-level representation of software development process. A SDLC model defines how different development phases and types of activities performed within this process relate to each other both logically and chronologically. Examples of SDLC models include: sequential development models (e.g. waterfall model V-model) iterative development models (e.g. spiral model prototyping) and incremental development models (e.g. Unified Process). Some activities within software development processes can also be described by more detailed software development methods and Agile practices. Examples include: acceptance test-driven development (ATDD) behavior-driven development (BDD) domain-driven design (DDD) extreme programming (XP) feature-driven development (FDD) Kanban Lean IT Scrum and test-driven development (TDD). 2.1.1. Impact of Software Development Lifecycle on Testing Testing must be adapted to SDLC to succeed. choice of SDLC impacts on the: • Scope and timing of test activities (e.g. test levels and test types) • Level of detail of test documentation • Choice of test techniques and test approach • Extent of test automation • Role and responsibilities of a tester In sequential development models in initial phases testers typically participate in requirement reviews test analysis and test design. executable code is usually created in later phases so typically dynamic testing cannot be performed early in SDLC. In some iterative and incremental development models it is assumed that each iteration delivers a working prototype or product increment. This implies that in each iteration both static and dynamic testing may be performed at all test levels. Frequent delivery of increments requires fast feedback and extensive regression testing. Agile software development assumes that change may occur throughout project. Therefore lightweight work product documentation and extensive test automation to make regression testing easier are favored in agile projects. Also most of manual testing tends to be done using experience-based test techniques (see Section 4.4) that do not require extensive prior test analysis and design. 2.1.2. Software Development Lifecycle and Good Testing Practices Good testing practices independent of chosen SDLC model include following: • For every software development activity there is a corresponding test activity so that all development activities are subject to quality control • Different test levels (see chapter 2.2.1) have specific and different test objectives which allows for testing to be appropriately comprehensive while avoiding redundancy • Test analysis and design for a given test level begins during corresponding development phase of SDLC so that testing can adhere to principle of early testing (see section 1.3) v4.0 Page of 74 2023-04-21  • Testers are involved in reviewing work products as soon as drafts of this documentation are available so that this earlier testing and defect detection can support shift-left strategy (see section 2.1.5) 2.1.3. Testing as a Driver for Software Development TDD ATDD and BDD are similar development approaches where tests are defined as a means of directing development. Each of these approaches implements principle of early testing (see section 1.3) and follows a shift-left approach (see section 2.1.5) since tests are defined before code is written. They support an iterative development model. These approaches are characterized as follows: Test-Driven Development (TDD): • Directs coding through test cases (instead of extensive software design) (Beck 2003) • Tests are written first then code is written to satisfy tests and then tests and code are refactored Acceptance Test-Driven Development (ATDD) (see section 4.5.3): • Derives tests from acceptance criteria as part of system design process (Gärtner 2011) • Tests are written before part of application is developed to satisfy tests Behavior-Driven Development (BDD): • Expresses desired behavior of an application with test cases written in a simple form of natural language which is easy to understand by stakeholders – usually using Given/When/Then format. (Chelimsky 2010) • Test cases are then automatically translated into executable tests For all above approaches tests may persist as automated tests to ensure code quality in future adaptions / refactoring. 2.1.4. DevOps and Testing DevOps is an organizational approach aiming to create synergy by getting development (including testing) and operations to work together to achieve a set of common goals. DevOps requires a cultural shift within an organization to bridge gaps between development (including testing) and operations while treating their functions with equal value. DevOps promotes team autonomy fast feedback integrated toolchains and technical practices like continuous integration (CI) and continuous delivery (CD). This enables teams to build test and release high-quality code faster through a DevOps delivery pipeline (Kim 2016). From testing perspective some of benefits of DevOps are: • Fast feedback on code quality and whether changes adversely affect existing code • CI promotes a shift-left approach in testing (see section 2.1.5) by encouraging developers to submit high quality code accompanied by component tests and static analysis • Promotes automated processes like CI/CD that facilitate establishing stable test environments • Increases view on non-functional quality characteristics (e.g. performance reliability) v4.0 Page of 74 2023-04-21  • Automation through a delivery pipeline reduces need for repetitive manual testing • risk in regression is minimized due to scale and range of automated regression tests DevOps is not without its risks and challenges which include: • DevOps delivery pipeline must be defined and established • CI / CD tools must be introduced and maintained • Test automation requires additional resources and may be difficult to establish and maintain Although DevOps comes with a high level of automated testing manual testing – especially from user's perspective – will still be needed. 2.1.5. Shift-Left Approach principle of early testing (see section 1.3) is sometimes referred to as shift-left because it is an approach where testing is performed earlier in SDLC. Shift-left normally suggests that testing should be done earlier (e.g. not waiting for code to be implemented or for components to be integrated) but it does not mean that testing later in SDLC should be neglected. There are some good practices that illustrate how to achieve a “shift-left” in testing which include: • Reviewing specification from perspective of testing. These review activities on specifications often find potential defects such as ambiguities incompleteness and inconsistencies • Writing test cases before code is written and have code run in a test harness during code implementation • Using CI and even better CD as it comes with fast feedback and automated component tests to accompany source code when it is submitted to code repository • Completing static analysis of source code prior to dynamic testing or as part of an automated process • Performing non-functional testing starting at component test level where possible. This is a form of shift-left as these non-functional test types tend to be performed later in SDLC when a complete system and a representative test environment are available A shift-left approach might result in extra training effort and/or costs earlier in process but is expected to save efforts and/or costs later in process. For shift-left approach it is important that stakeholders are convinced and bought into this concept. 2.1.6. Retrospectives and Process Improvement Retrospectives (also known as “post-project meetings” and project retrospectives) are often held at end of a project or an iteration at a release milestone or can be held when needed. timing and organization of retrospectives depend on particular SDLC model being followed. In these meetings participants (not only testers but also e.g. developers architects product owner business analysts) discuss: • What was successful and should be retained? v4.0 Page of 74 2023-04-21  • What was not successful and could be improved? • How to incorporate improvements and retain successes in future? results should be recorded and are normally part of test completion report (see section 5.3.2). Retrospectives are critical for successful implementation of continuous improvement and it is important that any recommended improvements are followed up. Typical benefits for testing include: • Increased test effectiveness / efficiency (e.g. by implementing suggestions for process improvement) • Increased quality of testware (e.g. by jointly reviewing test processes) • Team bonding and learning (e.g. as a result of opportunity to raise issues and propose improvement points) • Improved quality of test basis (e.g. as deficiencies in extent and quality of requirements could be addressed and solved) • Better cooperation between development and testing (e.g. as collaboration is reviewed and optimized regularly) 2.2. Test Levels and Test Types Test levels are groups of test activities that are organized and managed together. Each test level is an instance of test process performed in relation to software at a given stage of development from individual components to complete systems or where applicable systems of systems. Test levels are related to other activities within SDLC. In sequential SDLC models test levels are often defined such that exit criteria of one level are part of entry criteria for next level. In some iterative models this may not apply. Development activities may span through multiple test levels. Test levels may overlap in time. Test types are groups of test activities related to specific quality characteristics and most of those test activities can be performed at every test level. 2.2.1. Test Levels In this syllabus following five test levels are described: • Component testing (also known as unit testing) focuses on testing components in isolation. It often requires specific support such as test harnesses or unit test frameworks. Component testing is normally performed by developers in their development environments. • Component integration testing (also known as unit integration testing) focuses on testing interfaces and interactions between components. Component integration testing is heavily dependent on integration strategy approaches like bottom-up top-down or big-bang. • System testing focuses on overall behavior and capabilities of an entire system or product often including functional testing of end-to-end tasks and non-functional testing of quality characteristics. For some non-functional quality characteristics it is preferable to test them on a complete system in a representative test environment (e.g. usability). Using simulations of sub- v4.0 Page of 74 2023-04-21  systems is also possible. System testing may be performed by an independent test team and is related to specifications for system. • System integration testing focuses on testing interfaces of system under test and other systems and external services . System integration testing requires suitable test environments preferably similar to operational environment. • Acceptance testing focuses on validation and on demonstrating readiness for deployment which means that system fulfills user’s business needs. Ideally acceptance testing should be performed by intended users. main forms of acceptance testing are: user acceptance testing (UAT) operational acceptance testing contractual and regulatory acceptance testing alpha testing and beta testing. Test levels are distinguished by following non-exhaustive list of attributes to avoid overlapping of test activities: • Test object • Test objectives • Test basis • Defects and failures • Approach and responsibilities 2.2.2. Test Types A lot of test types exist and can be applied in projects. In this syllabus following four test types are addressed: Functional testing evaluates functions that a component or system should perform. functions are “what” test object should do. main objective of functional testing is checking functional completeness functional correctness and functional appropriateness. Non-functional testing evaluates attributes other than functional characteristics of a component or system. Non-functional testing is testing of “how well system behaves”. main objective of non- functional testing is checking non-functional software quality characteristics. ISO/IEC 25010 standard provides following classification of non-functional software quality characteristics: • Performance efficiency • Compatibility • Usability • Reliability • Security • Maintainability • Portability It is sometimes appropriate for non-functional testing to start early in life cycle (e.g. as part of reviews and component testing or system testing). Many non-functional tests are derived from functional tests as v4.0 Page of 74 2023-04-21  they use same functional tests but check that while performing function a non-functional constraint is satisfied (e.g. checking that a function performs within a specified time or a function can be ported to a new platform). late discovery of non-functional defects can pose a serious threat to success of a project. Non-functional testing sometimes needs a very specific test environment such as a usability lab for usability testing. Black-box testing (see section 4.2) is specification-based and derives tests from documentation external to test object. main objective of black-box testing is checking system's behavior against its specifications. White-box testing (see section 4.3) is structure-based and derives tests from system's implementation or internal structure (e.g. code architecture work flows and data flows). main objective of white-box testing is to cover underlying structure by tests to acceptable level. All four above mentioned test types can be applied to all test levels although focus will be different at each level. Different test techniques can be used to derive test conditions and test cases for all mentioned test types. 2.2.3. Confirmation Testing and Regression Testing Changes are typically made to a component or system to either enhance it by adding a new feature or to fix it by removing a defect. Testing should then also include confirmation testing and regression testing. Confirmation testing confirms that an original defect has been successfully fixed. Depending on risk one can test fixed version of software in several ways including: • executing all test cases that previously have failed due to defect or also by • adding new tests to cover any changes that were needed to fix defect However when time or money is short when fixing defects confirmation testing might be restricted to simply exercising steps that should reproduce failure caused by defect and checking that failure does not occur. Regression testing confirms that no adverse consequences have been caused by a change including a fix that has already been confirmation tested. These adverse consequences could affect same component where change was made other components in same system or even other connected systems. Regression testing may not be restricted to test object itself but can also be related to environment. It is advisable first to perform an impact analysis to optimize extent of regression testing. Impact analysis shows which parts of software could be affected. Regression test suites are run many times and generally number of regression test cases will increase with each iteration or release so regression testing is a strong candidate for automation. Automation of these tests should start early in project. Where CI is used such as in DevOps (see section 2.1.4) it is good practice to also include automated regression tests. Depending on situation this may include regression tests on different levels. Confirmation testing and/or regression testing for test object are needed on all test levels if defects are fixed and/or changes are made on these test levels. v4.0 Page of 74 2023-04-21  2.3. Maintenance Testing There are different categories of maintenance it can be corrective adaptive to changes in environment or improve performance or maintainability (see ISO/IEC 14764 for details) so maintenance can involve planned releases/deployments and unplanned releases/deployments (hot fixes). Impact analysis may be done before a change is made to help decide if change should be made based on potential consequences in other areas of system. Testing changes to a system in production includes both evaluating success of implementation of change and checking for possible regressions in parts of system that remain unchanged (which is usually most of system). scope of maintenance testing typically depends on: • degree of risk of change • size of existing system • size of change triggers for maintenance and maintenance testing can be classified as follows: • Modifications such as planned enhancements (i.e. release-based) corrective changes or hot fixes. • Upgrades or migrations of operational environment such as from one platform to another which can require tests associated with new environment as well as of changed software or tests of data conversion when data from another application is migrated into system being maintained. • Retirement such as when an application reaches end of its life. When a system is retired this can require testing of data archiving if long data-retention periods are required. Testing of restore and retrieval procedures after archiving may also be needed in event that certain data is required during archiving period. v4.0 Page of 74 2023-04-21  3. Static Testing – 80 minutes Keywords anomaly dynamic testing formal review informal review inspection review static analysis static testing technical review walkthrough Learning Objectives for Chapter 3: 3.1 Static Testing Basics FL-3.1.1 (K1) Recognize types of products that can be examined by different static test techniques FL-3.1.2 (K2) Explain value of static testing FL-3.1.3 (K2) Compare and contrast static and dynamic testing 3.2 Feedback and Review Process FL-3.2.1 (K1) Identify benefits of early and frequent stakeholder feedback FL-3.2.2 (K2) Summarize activities of review process FL-3.2.3 (K1) Recall which responsibilities are assigned to principal roles when performing reviews FL-3.2.4 (K2) Compare and contrast different review types FL-3.2.5 (K1) Recall factors that contribute to a successful review v4.0 Page of 74 2023-04-21  3.1. Static Testing Basics In contrast to dynamic testing in static testing software under test does not need to be executed. Code process specification system architecture specification or other work products are evaluated through manual examination (e.g. reviews) or with help of a tool (e.g. static analysis). Test objectives include improving quality detecting defects and assessing characteristics like readability completeness correctness testability and consistency. Static testing can be applied for both verification and validation. Testers business representatives and developers work together during example mappings collaborative user story writing and backlog refinement sessions to ensure that user stories and related work products meet defined criteria e.g. Definition of Ready (see section 5.1.3). Review techniques can be applied to ensure user stories are complete and understandable and include testable acceptance criteria. By asking right questions testers explore challenge and help improve proposed user stories. Static analysis can identify problems prior to dynamic testing while often requiring less effort since no test cases are required and tools (see chapter 6) are typically used. Static analysis is often incorporated into CI frameworks (see section 2.1.4). While largely used to detect specific code defects static analysis is also used to evaluate maintainability and security. Spelling checkers and readability tools are other examples of static analysis tools. 3.1.1. Work Products Examinable by Static Testing Almost any work product can be examined using static testing. Examples include requirement specification documents source code test plans test cases product backlog items test charters project documentation contracts and models. Any work product that can be read and understood can be subject of a review. However for static analysis work products need a structure against which they can be checked (e.g. models code or text with a formal syntax). Work products that are not appropriate for static testing include those that are difficult to interpret by human beings and that should not be analyzed by tools (e.g. 3rd party executable code due to legal reasons). 3.1.2. Value of Static Testing Static testing can detect defects in earliest phases of SDLC fulfilling principle of early testing (see section 1.3). It can also identify defects which cannot be detected by dynamic testing (e.g. unreachable code design patterns not implemented as desired defects in non-executable work products). Static testing provides ability to evaluate quality of and to build confidence in work products. By verifying documented requirements stakeholders can also make sure that these requirements describe their actual needs. Since static testing can be performed early in SDLC a shared understanding can be created among involved stakeholders. Communication will also be improved between involved stakeholders. For this reason it is recommended to involve a wide variety of stakeholders in static testing. Even though reviews can be costly to implement overall project costs are usually much lower than when no reviews are performed because less time and effort needs to be spent on fixing defects later in project. v4.0 Page of 74 2023-04-21  Code defects can be detected using static analysis more efficiently than in dynamic testing usually resulting in both fewer code defects and a lower overall development effort. 3.1.3. Differences between Static Testing and Dynamic Testing Static testing and dynamic testing practices complement each other. They have similar objectives such as supporting detection of defects in work products (see section 1.1.1) but there are also some differences such as: • Static and dynamic testing (with analysis of failures) can both lead to detection of defects however there are some defect types that can only be found by either static or dynamic testing. • Static testing finds defects directly while dynamic testing causes failures from which associated defects are determined through subsequent analysis • Static testing may more easily detect defects that lay on paths through code that are rarely executed or hard to reach using dynamic testing • Static testing can be applied to non-executable work products while dynamic testing can only be applied to executable work products • Static testing can be used to measure quality characteristics that are not dependent on executing code (e.g. maintainability) while dynamic testing can be used to measure quality characteristics that are dependent on executing code (e.g. performance efficiency) Typical defects that are easier and/or cheaper to find through static testing include: • Defects in requirements (e.g. inconsistencies ambiguities contradictions omissions inaccuracies duplications) • Design defects (e.g. inefficient database structures poor modularization) • Certain types of coding defects (e.g. variables with undefined values undeclared variables unreachable or duplicated code excessive code complexity) • Deviations from standards (e.g. lack of adherence to naming conventions in coding standards) • Incorrect interface specifications (e.g. mismatched number type or order of parameters) • Specific types of security vulnerabilities (e.g. buffer overflows) • Gaps or inaccuracies in test basis coverage (e.g. missing tests for an acceptance criterion) 3.2. Feedback and Review Process 3.2.1. Benefits of Early and Frequent Stakeholder Feedback Early and frequent feedback allows for early communication of potential quality problems. If there is little stakeholder involvement during SDLC product being developed might not meet stakeholder’s original or current vision. A failure to deliver what stakeholder wants can result in costly rework missed deadlines blame games and might even lead to complete project failure. v4.0 Page of 74 2023-04-21  Frequent stakeholder feedback throughout SDLC can prevent misunderstandings about requirements and ensure that changes to requirements are understood and implemented earlier. This helps development team to improve their understanding of what they are building. It allows them to focus on those features that deliver most value to stakeholders and that have most positive impact on identified risks. 3.2.2. Review Process Activities ISO/IEC 20246 standard defines a generic review process that provides a structured but flexible framework from which a specific review process may be tailored to a particular situation. If required review is more formal then more of tasks described for different activities will be needed. size of many work products makes them too large to be covered by a single review. review process may be invoked a couple of times to complete review for entire work product. activities in review process are: • Planning. During planning phase scope of review which comprises purpose work product to be reviewed quality characteristics to be evaluated areas to focus on exit criteria supporting information such as standards effort and timeframes for review shall be defined. • Review initiation. During review initiation goal is to make sure that everyone and everything involved is prepared to start review. This includes making sure that every participant has access to work product under review understands their role and responsibilities and receives everything needed to perform review. • Individual review. Every reviewer performs an individual review to assess quality of work product under review and to identify anomalies recommendations and questions by applying one or more review techniques (e.g. checklist-based reviewing scenario-based reviewing). ISO/IEC 20246 standard provides more depth on different review techniques. reviewers log all their identified anomalies recommendations and questions. • Communication and analysis. Since anomalies identified during a review are not necessarily defects all these anomalies need to be analyzed and discussed. For every anomaly decision should be made on its status ownership and required actions. This is typically done in a review meeting during which participants also decide what quality level of reviewed work product is and what follow-up actions are required. A follow-up review may be required to complete actions. • Fixing and reporting. For every defect a defect report should be created so that corrective actions can be followed-up. Once exit criteria are reached work product can be accepted. review results are reported. 3.2.3. Roles and Responsibilities in Reviews Reviews involve various stakeholders who may take on several roles. principal roles and their responsibilities are: • Manager – decides what is to be reviewed and provides resources such as staff and time for review • Author – creates and fixes work product under review v4.0 Page of 74 2023-04-21  • Moderator (also known as facilitator) – ensures effective running of review meetings including mediation time management and a safe review environment in which everyone can speak freely • Scribe (also known as recorder) – collates anomalies from reviewers and records review information such as decisions and new anomalies found during review meeting • Reviewer – performs reviews. A reviewer may be someone working on project a subject matter expert or any other stakeholder • Review leader – takes overall responsibility for review such as deciding who will be involved and organizing when and where review will take place Other more detailed roles are possible as described in ISO/IEC 20246 standard. 3.2.4. Review Types There exist many review types ranging from informal reviews to formal reviews. required level of formality depends on factors such as SDLC being followed maturity of development process criticality and complexity of work product being reviewed legal or regulatory requirements and need for an audit trail. same work product can be reviewed with different review types e.g. first an informal one and later a more formal one. Selecting right review type is key to achieving required review objectives (see section 3.2.5). selection is not only based on objectives but also on factors such as project needs available resources work product type and risks business domain and company culture. Some commonly used review types are: • Informal review. Informal reviews do not follow a defined process and do not require a formal documented output. main objective is detecting anomalies. • Walkthrough. A walkthrough which is led by author can serve many objectives such as evaluating quality and building confidence in work product educating reviewers gaining consensus generating new ideas motivating and enabling authors to improve and detecting anomalies. Reviewers might perform an individual review before walkthrough but this is not required. • Technical Review. A technical review is performed by technically qualified reviewers and led by a moderator. objectives of a technical review are to gain consensus and make decisions regarding a technical problem but also to detect anomalies evaluate quality and build confidence in work product generate new ideas and to motivate and enable authors to improve. • Inspection. As inspections are most formal type of review they follow complete generic process (see section 3.2.2). main objective is to find maximum number of anomalies. Other objectives are to evaluate quality build confidence in work product and to motivate and enable authors to improve. Metrics are collected and used to improve SDLC including inspection process. In inspections author cannot act as review leader or scribe. 3.2.5. Success Factors for Reviews There are several factors that determine success of reviews which include: v4.0 Page of 74 2023-04-21  • Defining clear objectives and measurable exit criteria. Evaluation of participants should never be an objective • Choosing appropriate review type to achieve given objectives and to suit type of work product review participants project needs and context • Conducting reviews on small chunks so that reviewers do not lose concentration during an individual review and/or review meeting (when held) • Providing feedback from reviews to stakeholders and authors so they can improve product and their activities (see section 3.2.1) • Providing adequate time to participants to prepare for review • Support from management for review process • Making reviews part of organization’s culture to promote learning and process improvement • Providing adequate training for all participants so they know how to fulfil their role • Facilitating meetings v4.0 Page of 74 2023-04-21  4. Test Analysis and Design – 390 minutes Keywords acceptance criteria acceptance test-driven development black-box test technique boundary value analysis branch coverage checklist-based testing collaboration-based test approach coverage coverage item decision table testing equivalence partitioning error guessing experience-based test technique exploratory testing state transition testing statement coverage test technique white-box test technique  4.1 Test Techniques Overview FL-4.1.1 (K2) Distinguish black-box white-box and experience-based test techniques 4.2 Black-box Test Techniques FL-4.2.1 (K3) Use equivalence partitioning to derive test cases FL-4.2.2 (K3) Use boundary value analysis to derive test cases FL-4.2.3 (K3) Use decision table testing to derive test cases FL-4.2.4 (K3) Use state transition testing to derive test cases 4.3 White-box Test Techniques FL-4.3.1 (K2) Explain statement testing FL-4.3.2 (K2) Explain branch testing FL-4.3.3 (K2) Explain value of white-box testing 4.4 Experience-based Test Techniques FL-4.4.1 (K2) Explain error guessing FL-4.4.2 (K2) Explain exploratory testing FL-4.4.3 (K2) Explain checklist-based testing 4.5. Collaboration-based Test Approaches FL-4.5.1 (K2) Explain how to write user stories in collaboration with developers and business representatives FL-4.5.2 (K2) Classify different options for writing acceptance criteria FL-4.5.3 (K3) Use acceptance test-driven development (ATDD) to derive test cases v4.0 Page of 74 2023-04-21  4.1. Test Techniques Overview Test techniques support tester in test analysis (what to test) and in test design (how to test). Test techniques help to develop a relatively small but sufficient set of test cases in a systematic way. Test techniques also help tester to define test conditions identify coverage items and identify test data during test analysis and design. Further information on test techniques and their corresponding measures can be found in ISO/IEC/IEEE 29119-4 standard and in (Beizer 1990 Craig 2002 Copeland 2004 Koomen 2006 Jorgensen 2014 Ammann 2016 Forgács 2019). In this syllabus test techniques are classified as black-box white-box and experience-based. Black-box test techniques (also known as specification-based techniques) are based on an analysis of specified behavior of test object without reference to its internal structure. Therefore test cases are independent of how software is implemented. Consequently if implementation changes but required behavior stays same then test cases are still useful. White-box test techniques (also known as structure-based techniques) are based on an analysis of test object’s internal structure and processing. As test cases are dependent on how software is designed they can only be created after design or implementation of test object. Experience-based test techniques effectively use knowledge and experience of testers for design and implementation of test cases. effectiveness of these techniques depends heavily on tester’s skills. Experience-based test techniques can detect defects that may be missed using black- box and white-box test techniques. Hence experience-based test techniques are complementary to black-box and white-box test techniques. 4.2. Black-Box Test Techniques Commonly used black-box test techniques discussed in following sections are: • Equivalence Partitioning • Boundary Value Analysis • Decision Table Testing • State Transition Testing 4.2.1. Equivalence Partitioning Equivalence Partitioning (EP) divides data into partitions (known as equivalence partitions) based on expectation that all elements of a given partition are to be processed in same way by test object. theory behind this technique is that if a test case that tests one value from an equivalence partition detects a defect this defect should also be detected by test cases that test any other value from same partition. Therefore one test for each partition is sufficient. Equivalence partitions can be identified for any data element related to test object including inputs outputs configuration items internal values time-related values and interface parameters. partitions may be continuous or discrete ordered or unordered finite or infinite. partitions must not overlap and must be non-empty sets. For simple test objects EP can be easy but in practice understanding how test object will treat different values is often complicated. Therefore partitioning should be done with care. v4.0 Page of 74 2023-04-21  A partition containing valid values is called a valid partition. A partition containing invalid values is called an invalid partition. definitions of valid and invalid values may vary among teams and organizations. For example valid values may be interpreted as those that should be processed by test object or as those for which specification defines their processing. Invalid values may be interpreted as those that should be ignored or rejected by test object or as those for which no processing is defined in test object specification. In EP coverage items are equivalence partitions. To achieve 100% coverage with this technique test cases must exercise all identified partitions (including invalid partitions) by covering each partition at least once. Coverage is measured as number of partitions exercised by at least one test case divided by total number of identified partitions and is expressed as a percentage. Many test objects include multiple sets of partitions (e.g. test objects with more than one input parameter) which means that a test case will cover partitions from different sets of partitions. simplest coverage criterion in case of multiple sets of partitions is called Each Choice coverage (Ammann 2016). Each Choice coverage requires test cases to exercise each partition from each set of partitions at least once. Each Choice coverage does not take into account combinations of partitions. 4.2.2. Boundary Value Analysis Boundary Value Analysis (BVA) is a technique based on exercising boundaries of equivalence partitions. Therefore BVA can only be used for ordered partitions. minimum and maximum values of a partition are its boundary values. In case of BVA if two elements belong to same partition all elements between them must also belong to that partition. BVA focuses on boundary values of partitions because developers are more likely to make errors with these boundary values. Typical defects found by BVA are located where implemented boundaries are misplaced to positions above or below their intended positions or are omitted altogether. This syllabus covers two versions of BVA: 2-value and 3-value BVA. They differ in terms of coverage items per boundary that need to be exercised to achieve 100% coverage. In 2-value BVA (Craig 2002 Myers 2011) for each boundary value there are two coverage items: this boundary value and its closest neighbor belonging to adjacent partition. To achieve 100% coverage with 2-value BVA test cases must exercise all coverage items i.e. all identified boundary values. Coverage is measured as number of boundary values that were exercised divided by total number of identified boundary values and is expressed as a percentage. In 3-value BVA (Koomen 2006 O’Regan 2019) for each boundary value there are three coverage items: this boundary value and both its neighbors. Therefore in 3-value BVA some of coverage items may not be boundary values. To achieve 100% coverage with 3-value BVA test cases must exercise all coverage items i.e. identified boundary values and their neighbors. Coverage is measured as number of boundary values and their neighbors exercised divided by total number of identified boundary values and their neighbors and is expressed as a percentage. 3-value BVA is more rigorous than 2-value BVA as it may detect defects overlooked by 2-value BVA. For example if decision “if (x ≤ 10) …” is incorrectly implemented as “if (x = 10) …” no test data derived from 2-value BVA (x = 10 x = 11) can detect defect. However x = 9 derived from 3-value BVA is likely to detect it. v4.0 Page of 74 2023-04-21  4.2.3. Decision Table Testing Decision tables are used for testing implementation of system requirements that specify how different combinations of conditions result in different outcomes. Decision tables are an effective way of recording complex logic such as business rules. When creating decision tables conditions and resulting actions of system are defined. These form rows of table. Each column corresponds to a decision rule that defines a unique combination of conditions along with associated actions. In limited-entry decision tables all values of conditions and actions (except for irrelevant or infeasible ones; see below) are shown as Boolean values (true or false). Alternatively in extended-entry decision tables some or all conditions and actions may also take on multiple values (e.g. ranges of numbers equivalence partitions discrete values). notation for conditions is as follows: “T” (true) means that condition is satisfied. “F” (false) means that condition is not satisfied. “–” means that value of condition is irrelevant for action outcome. “N/A” means that condition is infeasible for a given rule. For actions: “X” means that action should occur. Blank means that action should not occur. Other notations may also be used. A full decision table has enough columns to cover every combination of conditions. table can be simplified by deleting columns containing infeasible combinations of conditions. table can also be minimized by merging columns in which some conditions do not affect outcome into a single column. Decision table minimization algorithms are out of scope of this syllabus. In decision table testing coverage items are columns containing feasible combinations of conditions. To achieve 100% coverage with this technique test cases must exercise all these columns. Coverage is measured as number of exercised columns divided by total number of feasible columns and is expressed as a percentage. strength of decision table testing is that it provides a systematic approach to identify all combinations of conditions some of which might otherwise be overlooked. It also helps to find any gaps or contradictions in requirements. If there are many conditions exercising all decision rules may be time consuming since number of rules grows exponentially with number of conditions. In such a case to reduce number of rules that need to be exercised a minimized decision table or a risk- based approach may be used. 4.2.4. State Transition Testing A state transition diagram models behavior of a system by showing its possible states and valid state transitions. A transition is initiated by an event which may be additionally qualified by a guard condition. transitions are assumed to be instantaneous and may sometimes result in software taking action. common transition labeling syntax is as follows: “event [guard condition] / action”. Guard conditions and actions can be omitted if they do not exist or are irrelevant for tester. A state table is a model equivalent to a state transition diagram. Its rows represent states and its columns represent events (together with guard conditions if they exist). Table entries (cells) represent transitions and contain target state as well as resulting actions if defined. In contrast to state transition diagram state table explicitly shows invalid transitions which are represented by empty cells. A test case based on a state transition diagram or state table is usually represented as a sequence of events which results in a sequence of state changes (and actions if needed). One test case may and usually will cover several transitions between states. There exist many coverage criteria for state transition testing. This syllabus discusses three of them. v4.0 Page of 74 2023-04-21  In all states coverage coverage items are states. To achieve 100% all states coverage test cases must ensure that all states are visited. Coverage is measured as number of visited states divided by total number of states and is expressed as a percentage. In valid transitions coverage (also called 0-switch coverage) coverage items are single valid transitions. To achieve 100% valid transitions coverage test cases must exercise all valid transitions. Coverage is measured as number of exercised valid transitions divided by total number of valid transitions and is expressed as a percentage. In all transitions coverage coverage items are all transitions shown in a state table. To achieve 100% all transitions coverage test cases must exercise all valid transitions and attempt to execute invalid transitions. Testing only one invalid transition in a single test case helps to avoid fault masking i.e. a situation in which one defect prevents detection of another. Coverage is measured as number of valid and invalid transitions exercised or attempted to be covered by executed test cases divided by total number of valid and invalid transitions and is expressed as a percentage. All states coverage is weaker than valid transitions coverage because it can typically be achieved without exercising all transitions. Valid transitions coverage is most widely used coverage criterion. Achieving full valid transitions coverage guarantees full all states coverage. Achieving full all transitions coverage guarantees both full all states coverage and full valid transitions coverage and should be a minimum requirement for mission and safety-critical software. 4.3. White-Box Test Techniques Because of their popularity and simplicity this section focuses on two code-related white-box test techniques: • Statement testing • Branch testing There are more rigorous techniques that are used in some safety-critical mission-critical or high-integrity environments to achieve more thorough code coverage. There are also white-box test techniques used in higher test levels (e.g. API testing) or using coverage not related to code (e.g. neuron coverage in neural network testing). These techniques are not discussed in this syllabus. 4.3.1. Statement Testing and Statement Coverage In statement testing coverage items are executable statements. aim is to design test cases that exercise statements in code until an acceptable level of coverage is achieved. Coverage is measured as number of statements exercised by test cases divided by total number of executable statements in code and is expressed as a percentage. When 100% statement coverage is achieved it ensures that all executable statements in code have been exercised at least once. In particular this means that each statement with a defect will be executed which may cause a failure demonstrating presence of defect. However exercising a statement with a test case will not detect defects in all cases. For example it may not detect defects that are data dependent (e.g. a division by zero that only fails when a denominator is set to zero). Also 100% statement coverage does not ensure that all decision logic has been tested as for instance it may not exercise all branches (see chapter 4.3.2) in code. v4.0 Page of 74 2023-04-21  4.3.2. Branch Testing and Branch Coverage A branch is a transfer of control between two nodes in control flow graph which shows possible sequences in which source code statements are executed in test object. Each transfer of control can be either unconditional (i.e. straight-line code) or conditional (i.e. a decision outcome). In branch testing coverage items are branches and aim is to design test cases to exercise branches in code until an acceptable level of coverage is achieved. Coverage is measured as number of branches exercised by test cases divided by total number of branches and is expressed as a percentage. When 100% branch coverage is achieved all branches in code unconditional and conditional are exercised by test cases. Conditional branches typically correspond to a true or false outcome from an “if...then” decision an outcome from a switch/case statement or a decision to exit or continue in a loop. However exercising a branch with a test case will not detect defects in all cases. For example it may not detect defects requiring execution of a specific path in a code. Branch coverage subsumes statement coverage. This means that any set of test cases achieving 100% branch coverage also achieves 100% statement coverage (but not vice versa). 4.3.3. Value of White-box Testing A fundamental strength that all white-box techniques share is that entire software implementation is taken into account during testing which facilitates defect detection even when software specification is vague outdated or incomplete. A corresponding weakness is that if software does not implement one or more requirements white box testing may not detect resulting defects of omission (Watson 1996). White-box techniques can be used in static testing (e.g. during dry runs of code). They are well suited to reviewing code that is not yet ready for execution (Hetzel 1988) as well as pseudocode and other high- level or top-down logic which can be modeled with a control flow graph. Performing only black-box testing does not provide a measure of actual code coverage. White-box coverage measures provide an objective measurement of coverage and provide necessary information to allow additional tests to be generated to increase this coverage and subsequently increase confidence in code. 4.4. Experience-based Test Techniques Commonly used experience-based test techniques discussed in following sections are: • Error guessing • Exploratory testing • Checklist-based testing 4.4.1. Error Guessing Error guessing is a technique used to anticipate occurrence of errors defects and failures based on tester’s knowledge including: • How application has worked in past v4.0 Page of 74 2023-04-21  • types of errors developers tend to make and types of defects that result from these errors • types of failures that have occurred in other similar applications In general errors defects and failures may be related to: input (e.g. correct input not accepted parameters wrong or missing) output (e.g. wrong format wrong result) logic (e.g. missing cases wrong operator) computation (e.g. incorrect operand wrong computation) interfaces (e.g. parameter mismatch incompatible types) or data (e.g. incorrect initialization wrong type). Fault attacks are a methodical approach to implementation of error guessing. This technique requires tester to create or acquire a list of possible errors defects and failures and to design tests that will identify defects associated with errors expose defects or cause failures. These lists can be built based on experience defect and failure data or from common knowledge about why software fails. See (Whittaker 2002 Whittaker 2003 Andrews 2006) for more information on error guessing and fault attacks. 4.4.2. Exploratory Testing In exploratory testing tests are simultaneously designed executed and evaluated while tester learns about test object. testing is used to learn more about test object to explore it more deeply with focused tests and to create tests for untested areas. Exploratory testing is sometimes conducted using session-based testing to structure testing. In a session-based approach exploratory testing is conducted within a defined time-box. tester uses a test charter containing test objectives to guide testing. test session is usually followed by a debriefing that involves a discussion between tester and stakeholders interested in test results of test session. In this approach test objectives may be treated as high-level test conditions. Coverage items are identified and exercised during test session. tester may use test session sheets to document steps followed and discoveries made. Exploratory testing is useful when there are few or inadequate specifications or there is significant time pressure on testing. Exploratory testing is also useful to complement other more formal test techniques. Exploratory testing will be more effective if tester is experienced has domain knowledge and has a high degree of essential skills like analytical skills curiosity and creativeness (see section 1.5.1). Exploratory testing can incorporate use of other test techniques (e.g. equivalence partitioning). More information about exploratory testing can be found in (Kaner 1999 Whittaker 2009 Hendrickson 2013). 4.4.3. Checklist-Based Testing In checklist-based testing a tester designs implements and executes tests to cover test conditions from a checklist. Checklists can be built based on experience knowledge about what is important for user or an understanding of why and how software fails. Checklists should not contain items that can be checked automatically items better suited as entry/exit criteria or items that are too general (Brykczynski 1999). Checklist items are often phrased in form of a question. It should be possible to check each item separately and directly. These items may refer to requirements graphical interface properties quality characteristics or other forms of test conditions. Checklists can be created to support various test types including functional and non-functional testing (e.g. 10 heuristics for usability testing (Nielsen 1994)). v4.0 Page of 74 2023-04-21  Some checklist entries may gradually become less effective over time because developers will learn to avoid making same errors. New entries may also need to be added to reflect newly found high severity defects. Therefore checklists should be regularly updated based on defect analysis. However care should be taken to avoid letting checklist become too long (Gawande 2009). In absence of detailed test cases checklist-based testing can provide guidelines and some degree of consistency for testing. If checklists are high-level some variability in actual testing is likely to occur resulting in potentially greater coverage but less repeatability. 4.5. Collaboration-based Test Approaches Each of above-mentioned techniques (see sections 4.2 4.3 4.4) has a particular objective with respect to defect detection. Collaboration-based approaches on other hand focus also on defect avoidance by collaboration and communication. 4.5.1. Collaborative User Story Writing A user story represents a feature that will be valuable to either a user or purchaser of a system or software. User stories have three critical aspects (Jeffries 2000) called together “3 C’s”: • Card – medium describing a user story (e.g. an index card an entry in an electronic board) • Conversation – explains how software will be used (can be documented or verbal) • Confirmation – acceptance criteria (see section 4.5.2) most common format for a user story is “As a [role] I want [goal to be accomplished] so that I can [resulting business value for role]” followed by acceptance criteria. Collaborative authorship of user story can use techniques such as brainstorming and mind mapping. collaboration allows team to obtain a shared vision of what should be delivered by taking into account three perspectives: business development and testing. Good user stories should be: Independent Negotiable Valuable Estimable Small and Testable (INVEST). If a stakeholder does not know how to test a user story this may indicate that user story is not clear enough or that it does not reflect something valuable to them or that stakeholder just needs help in testing (Wake 2003). 4.5.2. Acceptance Criteria Acceptance criteria for a user story are conditions that an implementation of user story must meet to be accepted by stakeholders. From this perspective acceptance criteria may be viewed as test conditions that should be exercised by tests. Acceptance criteria are usually a result of Conversation (see section 4.5.1). Acceptance criteria are used to: • Define scope of user story • Reach consensus among stakeholders • Describe both positive and negative scenarios • Serve as a basis for user story acceptance testing (see section 4.5.3) v4.0 Page of 74 2023-04-21  • Allow accurate planning and estimation There are several ways to write acceptance criteria for a user story. two most common formats are: • Scenario-oriented (e.g. Given/When/Then format used in BDD see section 2.1.3) • Rule-oriented (e.g. bullet point verification list or tabulated form of input-output mapping) Most acceptance criteria can be documented in one of these two formats. However team may use another custom format as long as acceptance criteria are well-defined and unambiguous. 4.5.3. Acceptance Test-driven Development (ATDD) ATDD is a test-first approach (see section 2.1.3). Test cases are created prior to implementing user story. test cases are created by team members with different perspectives e.g. customers developers and testers (Adzic 2009). Test cases may be executed manually or automated. first step is a specification workshop where user story and (if not yet defined) its acceptance criteria are analyzed discussed and written by team members. Incompleteness ambiguities or defects in user story are resolved during this process. next step is to create test cases. This can be done by team as a whole or by tester individually. test cases are based on acceptance criteria and can be seen as examples of how software works. This will help team implement user story correctly. Since examples and tests are same these terms are often used interchangeably. During test design test techniques described in sections 4.2 4.3 and 4.4 may be applied. Typically first test cases are positive confirming correct behavior without exceptions or error conditions and comprising sequence of activities executed if everything goes as expected. After positive test cases are done team should perform negative testing. Finally team should cover non-functional quality characteristics as well (e.g. performance efficiency usability). Test cases should be expressed in a way that is understandable for stakeholders. Typically test cases contain sentences in natural language involving necessary preconditions (if any) inputs and postconditions. test cases must cover all characteristics of user story and should not go beyond story. However acceptance criteria may detail some of issues described in user story. In addition no two test cases should describe same characteristics of user story. When captured in a format supported by a test automation framework developers can automate test cases by writing supporting code as they implement feature described by a user story. acceptance tests then become executable requirements. v4.0 Page of 74 2023-04-21  5. Managing Test Activities – 335 minutes Keywords defect management defect report entry criteria exit criteria product risk project risk risk risk analysis risk assessment risk control risk identification risk level risk management risk mitigation risk monitoring risk-based testing test approach test completion report test control test monitoring test plan test planning test progress report test pyramid testing quadrants Learning Objectives for Chapter 5: 5.1 Test Planning FL-5.1.1 (K2) Exemplify purpose and content of a test plan FL-5.1.2 (K1) Recognize how a tester adds value to iteration and release planning FL-5.1.3 (K2) Compare and contrast entry criteria and exit criteria FL-5.1.4 (K3) Use estimation techniques to calculate required test effort FL-5.1.5 (K3) Apply test case prioritization FL-5.1.6 (K1) Recall concepts of test pyramid FL-5.1.7 (K2) Summarize testing quadrants and their relationships with test levels and test types 5.2 Risk Management FL-5.2.1 (K1) Identify risk level by using risk likelihood and risk impact FL-5.2.2 (K2) Distinguish between project risks and product risks FL-5.2.3 (K2) Explain how product risk analysis may influence thoroughness and scope of testing FL-5.2.4 (K2) Explain what measures can be taken in response to analyzed product risks 5.3 Test Monitoring Test Control and Test Completion FL-5.3.1 (K1) Recall metrics used for testing FL-5.3.2 (K2) Summarize purposes content and audiences for test reports FL-5.3.3 (K2) Exemplify how to communicate status of testing 5.4 Configuration Management FL-5.4.1 (K2) Summarize how configuration management supports testing 5.5 Defect Management FL-5.5.1 (K3) Prepare a defect report v4.0 Page of 74 2023-04-21  5.1. Test Planning 5.1.1. Purpose and Content of a Test Plan A test plan describes objectives resources and processes for a test project. A test plan: • Documents means and schedule for achieving test objectives • Helps to ensure that performed test activities will meet established criteria • Serves as a means of communication with team members and other stakeholders • Demonstrates that testing will adhere to existing test policy and test strategy (or explains why testing will deviate from them) Test planning guides testers’ thinking and forces testers to confront future challenges related to risks schedules people tools costs effort etc. process of preparing a test plan is a useful way to think through efforts needed to achieve test project objectives. typical content of a test plan includes: • Context of testing (e.g. scope test objectives constraints test basis) • Assumptions and constraints of test project • Stakeholders (e.g. roles responsibilities relevance to testing hiring and training needs) • Communication (e.g. forms and frequency of communication documentation templates) • Risk register (e.g. product risks project risks) • Test approach (e.g. test levels test types test techniques test deliverables entry criteria and exit criteria independence of testing metrics to be collected test data requirements test environment requirements deviations from organizational test policy and test strategy) • Budget and schedule More details about test plan and its content can be found in ISO/IEC/IEEE 29119-3 standard. 5.1.2. Tester's Contribution to Iteration and Release Planning In iterative SDLCs typically two kinds of planning occur: release planning and iteration planning. Release planning looks ahead to release of a product defines and re-defines product backlog and may involve refining larger user stories into a set of smaller user stories. It also serves as basis for test approach and test plan across all iterations. Testers involved in release planning participate in writing testable user stories and acceptance criteria (see section 4.5) participate in project and quality risk analyses (see section 5.2) estimate test effort associated with user stories (see section 5.1.4) determine test approach and plan testing for release. Iteration planning looks ahead to end of a single iteration and is concerned with iteration backlog. Testers involved in iteration planning participate in detailed risk analysis of user stories determine testability of user stories break down user stories into tasks (particularly testing tasks) estimate test effort for all testing tasks and identify and refine functional and non-functional aspects of test object. v4.0 Page of 74 2023-04-21  5.1.3. Entry Criteria and Exit Criteria Entry criteria define preconditions for undertaking a given activity. If entry criteria are not met it is likely that activity will prove to be more difficult time-consuming costly and riskier. Exit criteria define what must be achieved in order to declare an activity completed. Entry criteria and exit criteria should be defined for each test level and will differ based on test objectives. Typical entry criteria include: availability of resources (e.g. people tools environments test data budget time) availability of testware (e.g. test basis testable requirements user stories test cases) and initial quality level of a test object (e.g. all smoke tests have passed). Typical exit criteria include: measures of thoroughness (e.g. achieved level of coverage number of unresolved defects defect density number of failed test cases) and completion criteria (e.g. planned tests have been executed static testing has been performed all defects found are reported all regression tests are automated). Running out of time or budget can also be viewed as valid exit criteria. Even without other exit criteria being satisfied it can be acceptable to end testing under such circumstances if stakeholders have reviewed and accepted risk to go live without further testing. In Agile software development exit criteria are often called Definition of Done defining team’s objective metrics for a releasable item. Entry criteria that a user story must fulfill to start development and/or testing activities are called Definition of Ready. 5.1.4. Estimation Techniques Test effort estimation involves predicting amount of test-related work needed to meet objectives of a test project. It is important to make it clear to stakeholders that estimate is based on a number of assumptions and is always subject to estimation error. Estimation for small tasks is usually more accurate than for large ones. Therefore when estimating a large task it can be decomposed into a set of smaller tasks which then in turn can be estimated. In this syllabus following four estimation techniques are described. Estimation based on ratios. In this metrics-based technique figures are collected from previous projects within organization which makes it possible to derive “standard” ratios for similar projects. ratios of an organization’s own projects (e.g. taken from historical data) are generally best source to use in estimation process. These standard ratios can then be used to estimate test effort for new project. For example if in previous project development-to-test effort ratio was 3:2 and in current project development effort is expected to be 600 person-days test effort can be estimated to be 400 person-days. Extrapolation. In this metrics-based technique measurements are made as early as possible in current project to gather data. Having enough observations effort required for remaining work can be approximated by extrapolating this data (usually by applying a mathematical model). This method is very suitable in iterative SDLCs. For example team may extrapolate test effort in forthcoming iteration as averaged effort from last three iterations. Wideband Delphi. In this iterative expert-based technique experts make experience-based estimations. Each expert in isolation estimates effort. results are collected and if there are deviations that are out of range of agreed upon boundaries experts discuss their current estimates. Each expert is then asked to make a new estimation based on that feedback again in isolation. This process is repeated until a consensus is reached. Planning Poker is a variant of Wideband Delphi commonly used in Agile v4.0 Page of 74 2023-04-21  software development. In Planning Poker estimates are usually made using cards with numbers that represent effort size. Three-point estimation. In this expert-based technique three estimations are made by experts: most optimistic estimation (a) most likely estimation (m) and most pessimistic estimation (b). final estimate (E) is their weighted arithmetic mean. In most popular version of this technique estimate is calculated as E = (a + 4*m + b) / 6. advantage of this technique is that it allows experts to calculate measurement error: SD = (b – a) / 6. For example if estimates (in person- hours) are: a=6 m=9 and b=18 then final estimation is 10±2 person-hours (i.e. between 8 and 12 person-hours) because E = (6 + 4*9 + 18) / 6 = 10 and SD = (18 – 6) / 6 = 2. See (Kan 2003 Koomen 2006 Westfall 2009) for these and many other test estimation techniques. 5.1.5. Test Case Prioritization Once test cases and test procedures are specified and assembled into test suites these test suites can be arranged in a test execution schedule that defines order in which they are to be run. When prioritizing test cases different factors can be taken into account. most commonly used test case prioritization strategies are as follows: • Risk-based prioritization where order of test execution is based on results of risk analysis (see section 5.2.3). Test cases covering most important risks are executed first. • Coverage-based prioritization where order of test execution is based on coverage (e.g. statement coverage). Test cases achieving highest coverage are executed first. In another variant called additional coverage prioritization test case achieving highest coverage is executed first; each subsequent test case is one that achieves highest additional coverage. • Requirements-based prioritization where order of test execution is based on priorities of requirements traced back to corresponding test cases. Requirement priorities are defined by stakeholders. Test cases related to most important requirements are executed first. Ideally test cases would be ordered to run based on their priority levels using for example one of above-mentioned prioritization strategies. However this practice may not work if test cases or features being tested have dependencies. If a test case with a higher priority is dependent on a test case with a lower priority lower priority test case must be executed first. order of test execution must also take into account availability of resources. For example required test tools test environments or people that may only be available for a specific time window. 5.1.6. Test Pyramid test pyramid is a model showing that different tests may have different granularity. test pyramid model supports team in test automation and in test effort allocation by showing that different goals are supported by different levels of test automation. pyramid layers represent groups of tests. higher layer lower test granularity test isolation and test execution time. Tests in bottom layer are small isolated fast and check a small piece of functionality so usually a lot of them are needed to achieve a reasonable coverage. top layer represents complex high-level end-to-end tests. These high-level tests are generally slower than tests from lower layers and they typically check a large piece of functionality so usually just a few of them are needed to achieve a reasonable coverage. number and naming of layers may differ. For example original test pyramid model (Cohn 2009) defines three layers: “unit tests” “service tests” and “UI tests”. Another popular model defines unit v4.0 Page of 74 2023-04-21  (component) tests integration (component integration) tests and end-to-end tests. Other test levels (see section 2.2.1) can also be used. 5.1.7. Testing Quadrants testing quadrants defined by Brian Marick (Marick 2003 Crispin 2008) group test levels with appropriate test types activities test techniques and work products in Agile software development. model supports test management in visualizing these to ensure that all appropriate test types and test levels are included in SDLC and in understanding that some test types are more relevant to certain test levels than others. This model also provides a way to differentiate and describe types of tests to all stakeholders including developers testers and business representatives. In this model tests can be business facing or technology facing. Tests can also support team (i.e. guide development) or critique product (i.e. measure its behavior against expectations). combination of these two viewpoints determines four quadrants: • Quadrant Q1 (technology facing support team). This quadrant contains component and component integration tests. These tests should be automated and included in CI process. • Quadrant Q2 (business facing support team). This quadrant contains functional tests examples user story tests user experience prototypes API testing and simulations. These tests check acceptance criteria and can be manual or automated. • Quadrant Q3 (business facing critique product). This quadrant contains exploratory testing usability testing user acceptance testing. These tests are user-oriented and often manual. • Quadrant Q4 (technology facing critique product). This quadrant contains smoke tests and non-functional tests (except usability tests). These tests are often automated. 5.2. Risk Management Organizations face many internal and external factors that make it uncertain whether and when they will achieve their objectives (ISO 31000). Risk management allows organizations to increase likelihood of achieving objectives improve quality of their products and increase stakeholders’ confidence and trust. main risk management activities are: • Risk analysis (consisting of risk identification and risk assessment; see section 5.2.3) • Risk control (consisting of risk mitigation and risk monitoring; see section 5.2.4) test approach in which test activities are selected prioritized and managed based on risk analysis and risk control is called risk-based testing. 5.2.1. Risk Definition and Risk Attributes Risk is a potential event hazard threat or situation whose occurrence causes an adverse effect. A risk can be characterized by two factors: • Risk likelihood – probability of risk occurrence (greater than zero and less than one) • Risk impact (harm) – consequences of this occurrence v4.0 Page of 74 2023-04-21  These two factors express risk level which is a measure for risk. higher risk level more important is its treatment. 5.2.2. Project Risks and Product Risks In software testing one is generally concerned with two types of risks: project risks and product risks. Project risks are related to management and control of project. Project risks include: • Organizational issues (e.g. delays in work products deliveries inaccurate estimates cost-cutting) • People issues (e.g. insufficient skills conflicts communication problems shortage of staff) • Technical issues (e.g. scope creep poor tool support) • Supplier issues (e.g. third-party delivery failure bankruptcy of supporting company) Project risks when they occur may have an impact on project schedule budget or scope which affects project's ability to achieve its objectives. Product risks are related to product quality characteristics (e.g. described in ISO 25010 quality model). Examples of product risks include: missing or wrong functionality incorrect calculations runtime errors poor architecture inefficient algorithms inadequate response time poor user experience security vulnerabilities. Product risks when they occur may result in various negative consequences including: • User dissatisfaction • Loss of revenue trust reputation • Damage to third parties • High maintenance costs overload of helpdesk • Criminal penalties • In extreme cases physical damage injuries or even death 5.2.3. Product Risk Analysis From a testing perspective goal of product risk analysis is to provide an awareness of product risk in order to focus testing effort in a way that minimizes residual level of product risk. Ideally product risk analysis begins early in SDLC. Product risk analysis consists of risk identification and risk assessment. Risk identification is about generating a comprehensive list of risks. Stakeholders can identify risks by using various techniques and tools e.g. brainstorming workshops interviews or cause-effect diagrams. Risk assessment involves: categorization of identified risks determining their risk likelihood risk impact and level prioritizing and proposing ways to handle them. Categorization helps in assigning mitigation actions because usually risks falling into same category can be mitigated using a similar approach. Risk assessment can use a quantitative or qualitative approach or a mix of them. In quantitative approach risk level is calculated as multiplication of risk likelihood and risk impact. In qualitative approach risk level can be determined using a risk matrix. Product risk analysis may influence thoroughness and scope of testing. Its results are used to: v4.0 Page of 74 2023-04-21  • Determine scope of testing to be carried out • Determine particular test levels and propose test types to be performed • Determine test techniques to be employed and coverage to be achieved • Estimate test effort required for each task • Prioritize testing in an attempt to find critical defects as early as possible • Determine whether any activities in addition to testing could be employed to reduce risk 5.2.4. Product Risk Control Product risk control comprises all measures that are taken in response to identified and assessed product risks. Product risk control consists of risk mitigation and risk monitoring. Risk mitigation involves implementing actions proposed in risk assessment to reduce risk level. aim of risk monitoring is to ensure that mitigation actions are effective to obtain further information to improve risk assessment and to identify emerging risks. With respect to product risk control once a risk has been analyzed several response options to risk are possible e.g. risk mitigation by testing risk acceptance risk transfer or contingency plan (Veenendaal 2012). Actions that can be taken to mitigate product risks by testing are as follows: • Select testers with right level of experience and skills suitable for a given risk type • Apply an appropriate level of independence of testing • Conduct reviews and perform static analysis • Apply appropriate test techniques and coverage levels • Apply appropriate test types addressing affected quality characteristics • Perform dynamic testing including regression testing 5.3. Test Monitoring Test Control and Test Completion Test monitoring is concerned with gathering information about testing. This information is used to assess test progress and to measure whether test exit criteria or test tasks associated with exit criteria are satisfied such as meeting targets for coverage of product risks requirements or acceptance criteria. Test control uses information from test monitoring to provide in a form of control directives guidance and necessary corrective actions to achieve most effective and efficient testing. Examples of control directives include: • Reprioritizing tests when an identified risk becomes an issue • Re-evaluating whether a test item meets entry criteria or exit criteria due to rework • Adjusting test schedule to address a delay in delivery of test environment • Adding new resources when and where needed v4.0 Page of 74 2023-04-21  Test completion collects data from completed test activities to consolidate experience testware and any other relevant information. Test completion activities occur at project milestones such as when a test level is completed an agile iteration is finished a test project is completed (or cancelled) a software system is released or a maintenance release is completed. 5.3.1. Metrics used in Testing Test metrics are gathered to show progress against planned schedule and budget current quality of test object and effectiveness of test activities with respect to objectives or an iteration goal. Test monitoring gathers a variety of metrics to support test control and test completion. Common test metrics include: • Project progress metrics (e.g. task completion resource usage test effort) • Test progress metrics (e.g. test case implementation progress test environment preparation progress number of test cases run/not run passed/failed test execution time) • Product quality metrics (e.g. availability response time mean time to failure) • Defect metrics (e.g. number and priorities of defects found/fixed defect density defect detection percentage) • Risk metrics (e.g. residual risk level) • Coverage metrics (e.g. requirements coverage code coverage) • Cost metrics (e.g. cost of testing organizational cost of quality) 5.3.2. Purpose Content and Audience for Test Reports Test reporting summarizes and communicates test information during and after testing. Test progress reports support ongoing control of testing and must provide enough information to make modifications to test schedule resources or test plan when such changes are needed due to deviation from plan or changed circumstances. Test completion reports summarize a specific stage of testing (e.g. test level test cycle iteration) and can give information for subsequent testing. During test monitoring and control test team generates test progress reports for stakeholders to keep them informed. Test progress reports are usually generated on a regular basis (e.g. daily weekly etc.) and include: • Test period • Test progress (e.g. ahead or behind schedule) including any notable deviations • Impediments for testing and their workarounds • Test metrics (see section 5.3.1 for examples) • New and changed risks within testing period • Testing planned for next period v4.0 Page of 74 2023-04-21  A test completion report is prepared during test completion when a project test level or test type is complete and when ideally its exit criteria have been met. This report uses test progress reports and other data. Typical test completion reports include: • Test summary • Testing and product quality evaluation based on original test plan (i.e. test objectives and exit criteria) • Deviations from test plan (e.g. differences from planned schedule duration and effort). • Testing impediments and workarounds • Test metrics based on test progress reports • Unmitigated risks defects not fixed • Lessons learned that are relevant to testing Different audiences require different information in reports and influence degree of formality and frequency of reporting. Reporting on test progress to others in same team is often frequent and informal while reporting on testing for a completed project follows a set template and occurs only once. ISO/IEC/IEEE 29119-3 standard includes templates and examples for test progress reports (called test status reports) and test completion reports. 5.3.3. Communicating Status of Testing best means of communicating test status varies depending on test management concerns organizational test strategies regulatory standards or in case of self-organizing teams (see section 1.5.2) on team itself. options include: • Verbal communication with team members and other stakeholders • Dashboards (e.g. CI/CD dashboards task boards and burn-down charts) • Electronic communication channels (e.g. email chat) • Online documentation • Formal test reports (see section 5.3.2) One or more of these options can be used. More formal communication may be more appropriate for distributed teams where direct face-to-face communication is not always possible due to geographical distance or time differences. Typically different stakeholders are interested in different types of information so communication should be tailored accordingly. 5.4. Configuration Management In testing configuration management (CM) provides a discipline for identifying controlling and tracking work products such as test plans test strategies test conditions test cases test scripts test results test logs and test reports as configuration items. v4.0 Page of 74 2023-04-21  For a complex configuration item (e.g. a test environment) CM records items it consists of their relationships and versions. If configuration item is approved for testing it becomes a baseline and can only be changed through a formal change control process. Configuration management keeps a record of changed configuration items when a new baseline is created. It is possible to revert to a previous baseline to reproduce previous test results. To properly support testing CM ensures following: • All configuration items including test items (individual parts of test object) are uniquely identified version controlled tracked for changes and related to other configuration items so that traceability can be maintained throughout test process • All identified documentation and software items are referenced unambiguously in test documentation Continuous integration continuous delivery continuous deployment and associated testing are typically implemented as part of an automated DevOps pipeline (see section 2.1.4) in which automated CM is normally included. 5.5. Defect Management Since one of major test objectives is to find defects an established defect management process is essential. Although we refer to "defects" here reported anomalies may turn out to be real defects or something else (e.g. false positive change request) - this is resolved during process of dealing with defect reports. Anomalies may be reported during any phase of SDLC and form depends on SDLC. At a minimum defect management process includes a workflow for handling individual anomalies from their discovery to their closure and rules for their classification. workflow typically comprises activities to log reported anomalies analyze and classify them decide on a suitable response such as to fix or keep it as it is and finally to close defect report. process must be followed by all involved stakeholders. It is advisable to handle defects from static testing (especially static analysis) in a similar way. Typical defect reports have following objectives: • Provide those responsible for handling and resolving reported defects with sufficient information to resolve issue • Provide a means of tracking quality of work product • Provide ideas for improvement of development and test process A defect report logged during dynamic testing typically includes: • Unique identifier • Title with a short summary of anomaly being reported • Date when anomaly was observed issuing organization and author including their role • Identification of test object and test environment • Context of defect (e.g. test case being run test activity being performed SDLC phase and other relevant information such as test technique checklist or test data being used) v4.0 Page 55 of 74 2023-04-21  • Description of failure to enable reproduction and resolution including steps that detected anomaly and any relevant test logs database dumps screenshots or recordings • Expected results and actual results • Severity of defect (degree of impact) on interests of stakeholders or requirements • Priority to fix • Status of defect (e.g. open deferred duplicate waiting to be fixed awaiting confirmation testing re-opened closed rejected) • References (e.g. to test case) Some of this data may be automatically included when using defect management tools (e.g. identifier date author and initial status). Document templates for a defect report and example defect reports can be found in ISO/IEC/IEEE 29119-3 standard which refers to defect reports as incident reports. v4.0 