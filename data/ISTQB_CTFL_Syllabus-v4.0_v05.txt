1.1 testing software systems integral part our daily life people had experience software did expected software correctly lead problems loss money time business reputation extreme cases injury death software testing assesses software quality helps reducing risk software failure operation software testing set activities discover defects evaluate quality software artifacts artifacts tested known test objects misconception testing only consists executing tests running software checking test results however software testing also includes other activities must aligned software development lifecycle chapter 2 misconception testing testing focuses entirely verifying test object whilst testing involves verification checking whether system meets specified requirements also involves validation means checking whether system meets users other stakeholders needs operational environment testing dynamic static dynamic testing involves execution software while static testing static testing includes reviews chapter 3 static analysis dynamic testing uses different types test techniques test approaches derive test cases chapter 4 testing only technical activity also needs properly planned managed estimated monitored controlled chapter 5 testers use tools chapter 6 but important remember testing largely intellectual activity requiring testers specialized knowledge use analytical skills apply critical thinking systems thinking myers 2011 roman 2018 iso iec ieee 29119 1 standard provides further information software testing concepts 1.1.1 test objectives typical test objectives evaluating products requirements user stories designs code triggering failures finding defects ensuring required coverage test object reducing risk inadequate software quality verifying whether specified requirements been fulfilled verifying test object complies contractual legal regulatory requirements providing information stakeholders allow them make informed decisions building confidence quality test object validating whether test object complete works expected stakeholders objectives testing vary depending upon context includes product tested test risks software development lifecycle sdlc followed factors related business context corporate structure competitive considerations time market 15 software testing 1.1.2 testing debugging testing debugging separate activities testing trigger failures caused defects software dynamic testing directly find defects test object static testing dynamic testing chapter 4 triggers failure debugging concerned finding causes failure defects analyzing causes eliminating them typical debugging process case involves reproduction failure diagnosis finding root cause fixing cause subsequent confirmation testing checks whether fixes resolved problem preferably confirmation testing done same person performed initial test subsequent regression testing also performed check whether fixes causing failures other parts test object 2.2.3 information confirmation testing regression testing static testing identifies defect debugging concerned removing no need reproduction diagnosis since static testing directly finds defects cause failures chapter 3 1.2 testing necessary testing form quality control helps achieving agreed upon goals set scope time quality budget constraints testing ’s contribution success should restricted test team activities any stakeholder use their testing skills bring project closer success testing components systems associated documentation helps identify defects software 1.2.1 testing ’s contributions success testing provides cost effective means detecting defects defects then removed debugging non testing activity so testing indirectly contributes higher quality test objects testing provides means directly evaluating quality test object at various stages sdlc measures used part larger project management activity contributing decisions move next stage sdlc release decision testing provides users indirect representation development project testers ensure their understanding users needs considered throughout development lifecycle alternative involve representative set users part development project usually possible due high costs lack availability suitable users testing also required meet contractual legal requirements comply regulatory standards 1.2.2 testing quality assurance qa while people use terms testing quality assurance qa interchangeably testing qa same testing form quality control qc 16 software testing qc product oriented corrective approach focuses those activities supporting achievement appropriate levels quality testing major form quality control while others include formal methods model checking proof correctness simulation prototyping qa process oriented preventive approach focuses implementation improvement processes works basis if good process followed correctly then generate good product qa applies both development testing processes responsibility everyone project test results used qa qc qc they used fix defects while qa they provide feedback how well development test processes performing 1.2.3 errors defects failures root causes human beings make errors mistakes produce defects faults bugs turn result failures humans make errors various reasons time pressure complexity products processes infrastructure interactions simply because they tired lack adequate training defects found documentation requirements specification test script source code supporting artifact build file defects artifacts produced earlier sdlc if undetected lead defective artifacts later lifecycle if defect code executed system fail should something should n’t causing failure some defects always result failure if executed while others only result failure specific circumstances some never result failure errors defects only cause failures failures also caused environmental conditions radiation electromagnetic field cause defects firmware root cause fundamental reason occurrence problem situation leads error root causes identified through root cause analysis typically performed failure occurs defect identified believed further similar failures defects prevented their frequency reduced addressing root cause removing 1.3 testing principles number testing principles offering general guidelines applicable all testing been suggested years syllabus describes seven principles 1 testing shows presence absence defects testing show defects present test object but prove no defects 1970 testing reduces probability defects remaining undiscovered test object but if no defects found testing prove test object correctness 2 exhaustive testing impossible testing everything feasible except trivial cases manna 1978 rather attempting test exhaustively test techniques chapter 4 test case prioritization 5.1.5 risk based testing 5.2 should used focus test efforts 3 early testing saves time money defects removed early process cause subsequent defects derived products cost quality reduced since fewer failures occur later sdlc boehm 1981 find defects early both static testing chapter 3 dynamic testing chapter 4 should started early possible 4 defects cluster together small number system components usually contain defects discovered responsible operational failures enders 1975 phenomenon 17 software testing illustration pareto principle predicted defect clusters actual defect clusters observed during testing operation important input risk based testing 5.2 5 tests wear out if same tests repeated times they become increasingly ineffective detecting new defects beizer 1990 overcome effect existing tests test data need modified new tests need written however some cases repeating same tests beneficial outcome automated regression testing 2.2.3 6 testing context dependent no single universally applicable approach testing testing done differently different contexts kaner 2011 7 absence defects fallacy fallacy misconception expect software verification ensure success system thoroughly testing all specified requirements fixing all defects found could still produce system fulfill users needs expectations help achieving customer ’s business goals inferior compared other competing systems addition verification validation should also carried out boehm 1981 1.4 test activities testware test roles testing context dependent but at high sets test activities without testing less likely achieve test objectives sets test activities form test process test process tailored given situation based various factors test activities included test process how they implemented they occur normally decided part test planning specific situation 5.1 following sections describe general aspects test process terms test activities tasks impact context testware traceability between test basis testware testing roles iso iec ieee 29119 2 standard provides further information test processes 1.4.1 test activities tasks test process usually consists main groups activities described below although activities appear follow logical sequence they implemented iteratively parallel testing activities usually need tailored system project test planning consists defining test objectives then selecting approach best achieves objectives constraints imposed overall context test planning further explained 5.1 test monitoring control test monitoring involves ongoing checking all test activities comparison actual progress against plan test control involves taking actions necessary meet objectives testing test monitoring control further explained 5.3 test analysis includes analyzing test basis identify testable features define prioritize associated test conditions together related risks risk levels 5.2 test basis test objects also evaluated identify defects they contain assess their testability test analysis supported use test techniques chapter 4 test analysis answers question test terms measurable coverage criteria test design includes elaborating test conditions into test cases other testware test charters activity involves identification coverage items serve guide specify test case inputs test techniques chapter 4 used support activity test design 18 software testing also includes defining test data requirements designing test environment identifying any other required infrastructure tools test design answers question how test test implementation includes creating acquiring testware necessary test execution test data test cases organized into test procedures assembled into test suites manual automated test scripts created test procedures prioritized arranged test execution schedule efficient test execution 5.1.5 test environment built verified set up correctly test execution includes running tests accordance test execution schedule test runs test execution manual automated test execution take forms continuous testing pair testing sessions actual test results compared expected results test results logged anomalies analyzed identify their likely causes analysis allows us report anomalies based failures observed 5.5 test completion activities usually occur at project milestones release end iteration test completion any unresolved defects change requests product backlog items created any testware useful future identified archived handed appropriate teams test environment shut down agreed state test activities analyzed identify lessons learned improvements future iterations releases projects 2.1.6 test completion report created communicated stakeholders 1.4.2 test process context testing performed isolation test activities integral part development processes carried out organization testing also funded stakeholders final goal help fulfill stakeholders business needs therefore way testing carried out depend number contextual factors stakeholders needs expectations requirements willingness cooperate etc team members skills knowledge experience availability training needs etc business domain criticality test object identified risks market needs specific legal regulations etc technical factors type software product architecture technology used etc project constraints scope time budget resources etc organizational factors organizational structure existing policies practices used etc software development lifecycle engineering practices development methods etc tools availability usability compliance etc factors impact test related issues test strategy test techniques used degree test automation required coverage detail test documentation reporting etc 1.4.3 testware testware created output products test activities described 1.4.1 significant variation how different organizations produce shape name organize manage their 19 software testing products proper configuration management 5.4 ensures consistency integrity products following list products exhaustive test planning products include test plan test schedule risk register entry exit criteria 5.1 risk register list risks together risk likelihood risk impact information risk mitigation 5.2 test schedule risk register entry exit criteria part test plan test monitoring control products include test progress reports 5.3.2 documentation control directives 5.3 risk information 5.2 test analysis products include prioritized test conditions acceptance criteria 4.5.2 defect reports regarding defects test basis if fixed directly test design products include prioritized test cases test charters coverage items test data requirements test environment requirements test implementation products include test procedures automated test scripts test suites test data test execution schedule test environment elements examples test environment elements include stubs drivers simulators service virtualizations test execution products include test logs defect reports 5.5 test completion products include test completion report 5.3.2 action items improvement subsequent projects iterations documented lessons learned change requests product backlog items 1.4.4 traceability between test basis testware order implement effective test monitoring control important establish maintain traceability throughout test process between test basis elements testware associated elements test conditions risks test cases test results detected defects accurate traceability supports coverage evaluation so very useful if measurable coverage criteria defined test basis coverage criteria function key performance indicators drive activities show extent test objectives been achieved 1.1.1 example traceability test cases requirements verify requirements covered test cases traceability test results risks used evaluate residual risk test object addition evaluating coverage good traceability makes possible determine impact changes facilitates test audits helps meet governance criteria good traceability also makes test progress completion reports easily understandable status test basis elements also assist communicating technical aspects testing stakeholders understandable manner traceability provides information assess product quality process capability project progress against business goals 20 software testing 1.4.5 roles testing syllabus two principal roles testing covered test management role testing role activities tasks assigned two roles depend factors project product context skills people roles organization test management role takes overall responsibility test process test team leadership test activities test management role mainly focused activities test planning test monitoring control test completion way test management role carried out varies depending context example agile software development some test management tasks handled agile team tasks span multiple teams entire organization performed test managers outside development team testing role takes overall responsibility engineering technical aspect testing testing role mainly focused activities test analysis test design test implementation test execution different people take roles at different times example test management role performed team leader test manager development manager etc also possible one person take roles testing test management at same time 1.5 essential skills good practices testing skill ability something well comes one ’s knowledge practice aptitude good testers should possess some essential skills their job well good testers should effective team players should able perform testing different levels test independence 1.5.1 generic skills required testing while generic following skills particularly relevant testers testing knowledge increase effectiveness testing using test techniques thoroughness carefulness curiosity attention details methodical identify defects especially ones difficult find good communication skills active listening team player interact effectively all stakeholders convey information others understood report discuss defects analytical thinking critical thinking creativity increase effectiveness testing technical knowledge increase efficiency testing using appropriate test tools domain knowledge able understand communicate end users business representatives testers bearers bad news human trait blame bearer bad news makes communication skills crucial testers communicating test results perceived criticism product author confirmation bias make difficult accept information disagrees currently held beliefs some people perceive testing destructive activity though contributes greatly project success product quality try improve view information defects failures should communicated constructive way software testing 1.5.2 whole team approach one important skills ability effectively team context contribute positively team goals whole team approach practice coming extreme programming 2.1 builds upon skill whole team approach any team member necessary knowledge skills perform any task everyone responsible quality team members share same workspace physical virtual co location facilitates communication interaction whole team approach improves team dynamics enhances communication collaboration team creates synergy allowing various skill sets team leveraged benefit project testers closely other team members ensure desired quality levels achieved includes collaborating business representatives help them create suitable acceptance tests working developers agree test strategy decide test automation approaches testers thus transfer testing knowledge other team members influence development product depending context whole team approach always appropriate instance some situations safety critical high test independence needed 1.5.3 independence testing certain degree independence makes effective at finding defects due differences between author ’s ’s cognitive biases cf salman 1995 independence however replacement familiarity developers efficiently find defects their own code products tested their author no independence author 's peers same team some independence testers outside author 's team but organization high independence testers outside organization very high independence projects usually best carry out testing multiple levels independence developers performing component component integration testing test team performing system system integration testing business representatives performing acceptance testing main benefit independence testing independent testers likely recognize different kinds failures defects compared developers because their different backgrounds technical perspectives biases moreover independent verify challenge disprove assumptions made stakeholders during specification implementation system however also some drawbacks independent testers isolated development team lead lack collaboration communication problems adversarial relationship development team developers lose sense responsibility quality independent testers seen bottleneck blamed delays release 22 software testing 2 testing throughout software development lifecycle 130 minutes keywords acceptance testing black box testing component integration testing component testing confirmation testing functional testing integration testing maintenance testing non functional testing regression testing shift left system integration testing system testing test test object test type white box testing learning objectives chapter 2 2.1 testing context software development lifecycle fl-2.1.1 k2 explain impact chosen software development lifecycle testing fl-2.1.2 k1 recall good testing practices apply all software development lifecycles fl-2.1.3 k1 recall examples test first approaches development fl-2.1.4 k2 summarize how devops might impact testing fl-2.1.5 k2 explain shift left approach fl-2.1.6 k2 explain how retrospectives used mechanism process improvement 2.2 test levels test types fl-2.2.1 k2 distinguish different test levels fl-2.2.2 k2 distinguish different test types fl-2.2.3 k2 distinguish confirmation testing regression testing 2.3 maintenance testing fl-2.3.1 k2 summarize maintenance testing triggers 23 software testing 2.1 testing context software development lifecycle software development lifecycle sdlc model abstract high representation software development process sdlc model defines how different development phases types activities performed process relate other both logically chronologically examples sdlc models include sequential development models waterfall model v model iterative development models spiral model prototyping incremental development models unified process some activities software development processes also described detailed software development methods agile practices examples include acceptance test driven development atdd behavior driven development bdd domain driven design ddd extreme programming xp feature driven development fdd kanban lean scrum test driven development tdd 2.1.1 impact software development lifecycle testing testing must adapted sdlc succeed choice sdlc impacts scope timing test activities test levels test types detail test documentation choice test techniques test approach extent test automation role responsibilities sequential development models initial phases testers typically participate requirement reviews test analysis test design executable code usually created later phases so typically dynamic testing performed early sdlc some iterative incremental development models assumed iteration delivers working prototype product increment implies iteration both static dynamic testing performed at all test levels frequent delivery increments requires fast feedback extensive regression testing agile software development assumes change occur throughout project therefore lightweight product documentation extensive test automation make regression testing easier favored agile projects also manual testing tends done using experience based test techniques 4.4 require extensive prior test analysis design 2.1.2 software development lifecycle good testing practices good testing practices independent chosen sdlc model include following every software development activity corresponding test activity so all development activities subject quality control different test levels chapter 2.2.1 specific different test objectives allows testing appropriately comprehensive while avoiding redundancy test analysis design given test begins during corresponding development phase sdlc so testing adhere principle early testing 1.3 24 software testing testers involved reviewing products soon drafts documentation available so earlier testing defect detection support shift left strategy 2.1.5 2.1.3 testing driver software development tdd atdd bdd similar development approaches where tests defined means directing development approaches implements principle early testing 1.3 follows shift left approach 2.1.5 since tests defined before code written they support iterative development model approaches characterized follows test driven development tdd directs coding through test cases instead extensive software design beck 2003 tests written first then code written satisfy tests then tests code refactored acceptance test driven development atdd 4.5.3 derives tests acceptance criteria part system design process gärtner 2011 tests written before part application developed satisfy tests behavior driven development bdd expresses desired behavior application test cases written simple form natural language easy understand stakeholders usually using given then format chelimsky 2010 test cases then automatically translated into executable tests all approaches tests persist automated tests ensure code quality future adaptions refactoring 2.1.4 devops testing devops organizational approach aiming create synergy getting development testing operations together achieve set goals devops requires cultural shift organization bridge gaps between development testing operations while treating their functions equal value devops promotes team autonomy fast feedback integrated toolchains technical practices like continuous integration ci continuous delivery cd enables teams build test release high quality code faster through devops delivery pipeline kim 2016 testing perspective some benefits devops fast feedback code quality whether changes adversely affect existing code ci promotes shift left approach testing 2.1.5 encouraging developers submit high quality code accompanied component tests static analysis promotes automated processes like ci cd facilitate establishing stable test environments increases view non functional quality characteristics performance reliability 25 software testing automation through delivery pipeline reduces need repetitive manual testing risk regression minimized due scale range automated regression tests devops without risks challenges include devops delivery pipeline must defined established ci cd tools must introduced maintained test automation requires resources difficult establish maintain although devops comes high automated testing manual testing especially user 's perspective still needed 2.1.5 shift left approach principle early testing 1.3 sometimes referred shift left because approach where testing performed earlier sdlc shift left normally suggests testing should done earlier waiting code implemented components integrated but mean testing later sdlc should neglected some good practices illustrate how achieve shift left testing include reviewing specification perspective testing review activities specifications find potential defects ambiguities incompleteness inconsistencies writing test cases before code written code run test harness during code implementation using ci better cd comes fast feedback automated component tests accompany source code submitted code repository completing static analysis source code prior dynamic testing part automated process performing non functional testing starting at component test where possible form shift left non functional test types tend performed later sdlc complete system representative test environment available shift left approach might result extra training effort and/or costs earlier process but expected save efforts and/or costs later process shift left approach important stakeholders convinced bought into concept 2.1.6 retrospectives process improvement retrospectives also known post project meetings project retrospectives held at end project iteration at release milestone held needed timing organization retrospectives depend particular sdlc model followed meetings participants only testers but also developers architects product owner business analysts discuss successful should retained 26 software testing successful could improved how incorporate improvements retain successes future results should recorded normally part test completion report 5.3.2 retrospectives critical successful implementation continuous improvement important any recommended improvements followed up typical benefits testing include increased test effectiveness efficiency implementing suggestions process improvement increased quality testware jointly reviewing test processes team bonding learning result opportunity raise issues propose improvement points improved quality test basis deficiencies extent quality requirements could addressed solved better cooperation between development testing collaboration reviewed optimized regularly 2.2 test levels test types test levels groups test activities organized managed together test instance test process performed relation software at given stage development individual components complete systems where applicable systems systems test levels related other activities sdlc sequential sdlc models test levels defined exit criteria one part entry criteria next some iterative models apply development activities span through multiple test levels test levels overlap time test types groups test activities related specific quality characteristics those test activities performed at every test 2.2.1 test levels syllabus following five test levels described component testing also known unit testing focuses testing components isolation requires specific support test harnesses unit test frameworks component testing normally performed developers their development environments component integration testing also known unit integration testing focuses testing interfaces interactions between components component integration testing heavily dependent integration strategy approaches like bottom up top down big bang system testing focuses overall behavior capabilities entire system product functional testing end end tasks non functional testing quality characteristics some non functional quality characteristics preferable test them complete system representative test environment usability using simulations sub- 27 software testing systems also possible system testing performed independent test team related specifications system system integration testing focuses testing interfaces system test other systems external services system integration testing requires suitable test environments preferably similar operational environment acceptance testing focuses validation demonstrating readiness deployment means system fulfills user ’s business needs ideally acceptance testing should performed intended users main forms acceptance testing user acceptance testing uat operational acceptance testing contractual regulatory acceptance testing alpha testing beta testing test levels distinguished following non exhaustive list attributes avoid overlapping test activities test object test objectives test basis defects failures approach responsibilities 2.2.2 test types lot test types exist applied projects syllabus following four test types addressed functional testing evaluates functions component system should perform functions test object should main objective functional testing checking functional completeness functional correctness functional appropriateness non functional testing evaluates attributes other functional characteristics component system non functional testing testing how well system behaves main objective non- functional testing checking non functional software quality characteristics iso iec 25010 standard provides following classification non functional software quality characteristics performance efficiency compatibility usability reliability security maintainability portability sometimes appropriate non functional testing start early life cycle part reviews component testing system testing non functional tests derived functional tests 28 software testing they use same functional tests but check while performing function non functional constraint satisfied checking function performs specified time function ported new platform late discovery non functional defects pose serious threat success project non functional testing sometimes needs very specific test environment usability lab usability testing black box testing 4.2 specification based derives tests documentation external test object main objective black box testing checking system 's behavior against specifications white box testing 4.3 structure based derives tests system 's implementation internal structure code architecture flows data flows main objective white box testing cover underlying structure tests acceptable all four mentioned test types applied all test levels although focus different at different test techniques used derive test conditions test cases all mentioned test types 2.2.3 confirmation testing regression testing changes typically made component system either enhance adding new feature fix removing defect testing should then also include confirmation testing regression testing confirmation testing confirms original defect been successfully fixed depending risk one test fixed version software several ways executing all test cases previously failed due defect also adding new tests cover any changes were needed fix defect however time money short fixing defects confirmation testing might restricted simply exercising steps should reproduce failure caused defect checking failure occur regression testing confirms no adverse consequences been caused change fix already been confirmation tested adverse consequences could affect same component where change made other components same system other connected systems regression testing restricted test object itself but also related environment advisable first perform impact analysis optimize extent regression testing impact analysis shows parts software could affected regression test suites run times generally number regression test cases increase iteration release so regression testing strong candidate automation automation tests should start early project where ci used devops 2.1.4 good practice also include automated regression tests depending situation include regression tests different levels confirmation testing and/or regression testing test object needed all test levels if defects fixed and/or changes made test levels 29 software testing 2.3 maintenance testing different categories maintenance corrective adaptive changes environment improve performance maintainability iso iec 14764 details so maintenance involve planned releases deployments unplanned releases deployments hot fixes impact analysis done before change made help decide if change should made based potential consequences other areas system testing changes system production includes both evaluating success implementation change checking possible regressions parts system remain unchanged usually system scope maintenance testing typically depends degree risk change size existing system size change triggers maintenance maintenance testing classified follows modifications planned enhancements release based corrective changes hot fixes upgrades migrations operational environment one platform require tests associated new environment well changed software tests data conversion data application migrated into system maintained retirement application reaches end life system retired require testing data archiving if long data retention periods required testing restore retrieval procedures after archiving also needed event certain data required during archiving period 30 software testing 3 static testing 80 minutes keywords anomaly dynamic testing formal review informal review inspection review static analysis static testing technical review walkthrough learning objectives chapter 3 3.1 static testing basics fl-3.1.1 k1 recognize types products examined different static test techniques fl-3.1.2 k2 explain value static testing fl-3.1.3 k2 compare contrast static dynamic testing 3.2 feedback review process fl-3.2.1 k1 identify benefits early frequent stakeholder feedback fl-3.2.2 k2 summarize activities review process fl-3.2.3 k1 recall responsibilities assigned principal roles performing reviews fl-3.2.4 k2 compare contrast different review types fl-3.2.5 k1 recall factors contribute successful review 31 software testing 3.1 static testing basics contrast dynamic testing static testing software test need executed code process specification system architecture specification other products evaluated through manual examination reviews help tool static analysis test objectives include improving quality detecting defects assessing characteristics like readability completeness correctness testability consistency static testing applied both verification validation testers business representatives developers together during example mappings collaborative user story writing backlog refinement sessions ensure user stories related products meet defined criteria definition ready 5.1.3 review techniques applied ensure user stories complete understandable include testable acceptance criteria asking right questions testers explore challenge help improve proposed user stories static analysis identify problems prior dynamic testing while requiring less effort since no test cases required tools chapter 6 typically used static analysis incorporated into ci frameworks 2.1.4 while largely used detect specific code defects static analysis also used evaluate maintainability security spelling checkers readability tools other examples static analysis tools 3.1.1 products examinable static testing almost any product examined using static testing examples include requirement specification documents source code test plans test cases product backlog items test charters project documentation contracts models any product read understood subject review however static analysis products need structure against they checked models code text formal syntax products appropriate static testing include those difficult interpret human beings should analyzed tools 3rd party executable code due legal reasons 3.1.2 value static testing static testing detect defects earliest phases sdlc fulfilling principle early testing 1.3 also identify defects detected dynamic testing unreachable code design patterns implemented desired defects non executable products static testing provides ability evaluate quality build confidence products verifying documented requirements stakeholders also make sure requirements describe their actual needs since static testing performed early sdlc shared understanding created among involved stakeholders communication also improved between involved stakeholders reason recommended involve wide variety stakeholders static testing though reviews costly implement overall project costs usually much lower no reviews performed because less time effort needs spent fixing defects later project 32 software testing code defects detected using static analysis efficiently dynamic testing usually resulting both fewer code defects lower overall development effort 3.1.3 differences between static testing dynamic testing static testing dynamic testing practices complement other they similar objectives supporting detection defects products 1.1.1 but also some differences static dynamic testing analysis failures both lead detection defects however some defect types only found either static dynamic testing static testing finds defects directly while dynamic testing causes failures associated defects determined through subsequent analysis static testing easily detect defects lay paths through code rarely executed hard reach using dynamic testing static testing applied non executable products while dynamic testing only applied executable products static testing used measure quality characteristics dependent executing code maintainability while dynamic testing used measure quality characteristics dependent executing code performance efficiency typical defects easier and/or cheaper find through static testing include defects requirements inconsistencies ambiguities contradictions omissions inaccuracies duplications design defects inefficient database structures poor modularization certain types coding defects variables undefined values undeclared variables unreachable duplicated code excessive code complexity deviations standards lack adherence naming conventions coding standards incorrect interface specifications mismatched number type order parameters specific types security vulnerabilities buffer overflows gaps inaccuracies test basis coverage missing tests acceptance criterion 3.2 feedback review process 3.2.1 benefits early frequent stakeholder feedback early frequent feedback allows early communication potential quality problems if little stakeholder involvement during sdlc product developed might meet stakeholder ’s original current vision failure deliver stakeholder wants result costly rework missed deadlines blame games might lead complete project failure 33 software testing frequent stakeholder feedback throughout sdlc prevent misunderstandings requirements ensure changes requirements understood implemented earlier helps development team improve their understanding they building allows them focus those features deliver value stakeholders positive impact identified risks 3.2.2 review process activities iso iec 20246 standard defines generic review process provides structured but flexible framework specific review process tailored particular situation if required review formal then tasks described different activities needed size products makes them too large covered single review review process invoked couple times complete review entire product activities review process planning during planning phase scope review comprises purpose product reviewed quality characteristics evaluated areas focus exit criteria supporting information standards effort timeframes review shall defined review initiation during review initiation goal make sure everyone everything involved prepared start review includes making sure every participant access product review understands their role responsibilities receives everything needed perform review individual review every reviewer performs individual review assess quality product review identify anomalies recommendations questions applying one review techniques checklist based reviewing scenario based reviewing iso iec 20246 standard provides depth different review techniques reviewers log all their identified anomalies recommendations questions communication analysis since anomalies identified during review necessarily defects all anomalies need analyzed discussed every anomaly decision should made status ownership required actions typically done review meeting during participants also decide quality reviewed product follow up actions required follow up review required complete actions fixing reporting every defect defect report should created so corrective actions followed up once exit criteria reached product accepted review results reported 3.2.3 roles responsibilities reviews reviews involve various stakeholders take several roles principal roles their responsibilities manager decides reviewed provides resources staff time review author creates fixes product review 34 software testing moderator also known facilitator ensures effective running review meetings mediation time management safe review environment everyone speak freely scribe also known recorder collates anomalies reviewers records review information decisions new anomalies found during review meeting reviewer performs reviews reviewer someone working project subject matter expert any other stakeholder review leader takes overall responsibility review deciding involved organizing where review take place other detailed roles possible described iso iec 20246 standard 3.2.4 review types exist review types ranging informal reviews formal reviews required formality depends factors sdlc followed maturity development process criticality complexity product reviewed legal regulatory requirements need audit trail same product reviewed different review types first informal one later formal one selecting right review type key achieving required review objectives 3.2.5 selection only based objectives but also factors project needs available resources product type risks business domain company culture some used review types informal review informal reviews follow defined process require formal documented output main objective detecting anomalies walkthrough walkthrough led author serve objectives evaluating quality building confidence product educating reviewers gaining consensus generating new ideas motivating enabling authors improve detecting anomalies reviewers might perform individual review before walkthrough but required technical review technical review performed technically qualified reviewers led moderator objectives technical review gain consensus make decisions regarding technical problem but also detect anomalies evaluate quality build confidence product generate new ideas motivate enable authors improve inspection inspections formal type review they follow complete generic process 3.2.2 main objective find maximum number anomalies other objectives evaluate quality build confidence product motivate enable authors improve metrics collected used improve sdlc inspection process inspections author act review leader scribe 3.2.5 success factors reviews several factors determine success reviews include 35 software testing defining clear objectives measurable exit criteria evaluation participants should never objective choosing appropriate review type achieve given objectives suit type product review participants project needs context conducting reviews small chunks so reviewers lose concentration during individual review and/or review meeting held providing feedback reviews stakeholders authors so they improve product their activities 3.2.1 providing adequate time participants prepare review support management review process making reviews part organization ’s culture promote learning process improvement providing adequate training all participants so they know how fulfil their role facilitating meetings 36 software testing 4 test analysis design 390 minutes keywords acceptance criteria acceptance test driven development black box test technique boundary value analysis branch coverage checklist based testing collaboration based test approach coverage coverage item decision table testing equivalence partitioning error guessing experience based test technique exploratory testing state transition testing statement coverage test technique white box test technique learning objectives chapter 4 4.1 test techniques overview fl-4.1.1 k2 distinguish black box white box experience based test techniques 4.2 black box test techniques fl-4.2.1 k3 use equivalence partitioning derive test cases fl-4.2.2 k3 use boundary value analysis derive test cases fl-4.2.3 k3 use decision table testing derive test cases fl-4.2.4 k3 use state transition testing derive test cases 4.3 white box test techniques fl-4.3.1 k2 explain statement testing fl-4.3.2 k2 explain branch testing fl-4.3.3 k2 explain value white box testing 4.4 experience based test techniques fl-4.4.1 k2 explain error guessing fl-4.4.2 k2 explain exploratory testing fl-4.4.3 k2 explain checklist based testing 4.5 collaboration based test approaches fl-4.5.1 k2 explain how write user stories collaboration developers business representatives fl-4.5.2 k2 classify different options writing acceptance criteria fl-4.5.3 k3 use acceptance test driven development atdd derive test cases 37 software testing 4.1 test techniques overview test techniques support test analysis test test design how test test techniques help develop relatively small but sufficient set test cases systematic way test techniques also help define test conditions identify coverage items identify test data during test analysis design further information test techniques their corresponding measures found iso iec ieee 29119 4 standard beizer 1990 craig 2002 copeland 2004 koomen 2006 jorgensen 2014 ammann 2016 forgács 2019 syllabus test techniques classified black box white box experience based black box test techniques also known specification based techniques based analysis specified behavior test object without reference internal structure therefore test cases independent how software implemented consequently if implementation changes but required behavior stays same then test cases still useful white box test techniques also known structure based techniques based analysis test object ’s internal structure processing test cases dependent how software designed they only created after design implementation test object experience based test techniques effectively use knowledge experience testers design implementation test cases effectiveness techniques depends heavily ’s skills experience based test techniques detect defects missed using black- box white box test techniques hence experience based test techniques complementary black box white box test techniques 4.2 black box test techniques used black box test techniques discussed following sections equivalence partitioning boundary value analysis decision table testing state transition testing 4.2.1 equivalence partitioning equivalence partitioning ep divides data into partitions known equivalence partitions based expectation all elements given partition processed same way test object theory behind technique if test case tests one value equivalence partition detects defect defect should also detected test cases test any other value same partition therefore one test partition sufficient equivalence partitions identified any data element related test object inputs outputs configuration items internal values time related values interface parameters partitions continuous discrete ordered unordered finite infinite partitions must overlap must non empty sets simple test objects ep easy but practice understanding how test object treat different values complicated therefore partitioning should done care 38 software testing partition containing valid values called valid partition partition containing invalid values called invalid partition definitions valid invalid values vary among teams organizations example valid values interpreted those should processed test object those specification defines their processing invalid values interpreted those should ignored rejected test object those no processing defined test object specification ep coverage items equivalence partitions achieve 100 coverage technique test cases must exercise all identified partitions invalid partitions covering partition at least once coverage measured number partitions exercised at least one test case divided total number identified partitions expressed percentage test objects include multiple sets partitions test objects one input parameter means test case cover partitions different sets partitions simplest coverage criterion case multiple sets partitions called choice coverage ammann 2016 choice coverage requires test cases exercise partition set partitions at least once choice coverage take into account combinations partitions 4.2.2 boundary value analysis boundary value analysis bva technique based exercising boundaries equivalence partitions therefore bva only used ordered partitions minimum maximum values partition boundary values case bva if two elements belong same partition all elements between them must also belong partition bva focuses boundary values partitions because developers likely make errors boundary values typical defects found bva located where implemented boundaries misplaced positions below their intended positions omitted altogether syllabus covers two versions bva 2 value 3 value bva they differ terms coverage items per boundary need exercised achieve 100 coverage 2 value bva craig 2002 myers 2011 boundary value two coverage items boundary value closest neighbor belonging adjacent partition achieve 100 coverage 2 value bva test cases must exercise all coverage items all identified boundary values coverage measured number boundary values were exercised divided total number identified boundary values expressed percentage 3 value bva koomen 2006 o’regan 2019 boundary value three coverage items boundary value both neighbors therefore 3 value bva some coverage items boundary values achieve 100 coverage 3 value bva test cases must exercise all coverage items identified boundary values their neighbors coverage measured number boundary values their neighbors exercised divided total number identified boundary values their neighbors expressed percentage 3 value bva rigorous 2 value bva detect defects overlooked 2 value bva example if decision if x ≤ 10 incorrectly implemented if x = 10 no test data derived 2 value bva x = 10 x = 11 detect defect however x = 9 derived 3 value bva likely detect 39 software testing 4.2.3 decision table testing decision tables used testing implementation system requirements specify how different combinations conditions result different outcomes decision tables effective way recording complex logic business rules creating decision tables conditions resulting actions system defined form rows table column corresponds decision rule defines unique combination conditions along associated actions limited entry decision tables all values conditions actions except irrelevant infeasible ones below shown boolean values true false alternatively extended entry decision tables some all conditions actions also take multiple values ranges numbers equivalence partitions discrete values notation conditions follows t true means condition satisfied f false means condition satisfied means value condition irrelevant action outcome n means condition infeasible given rule actions x means action should occur blank means action should occur other notations also used full decision table enough columns cover every combination conditions table simplified deleting columns containing infeasible combinations conditions table also minimized merging columns some conditions affect outcome into single column decision table minimization algorithms out scope syllabus decision table testing coverage items columns containing feasible combinations conditions achieve 100 coverage technique test cases must exercise all columns coverage measured number exercised columns divided total number feasible columns expressed percentage strength decision table testing provides systematic approach identify all combinations conditions some might otherwise overlooked also helps find any gaps contradictions requirements if conditions exercising all decision rules time consuming since number rules grows exponentially number conditions case reduce number rules need exercised minimized decision table risk- based approach used 4.2.4 state transition testing state transition diagram models behavior system showing possible states valid state transitions transition initiated event additionally qualified guard condition transitions assumed instantaneous sometimes result software taking action transition labeling syntax follows event guard condition action guard conditions actions omitted if they exist irrelevant state table model equivalent state transition diagram rows represent states columns represent events together guard conditions if they exist table entries cells represent transitions contain target state well resulting actions if defined contrast state transition diagram state table explicitly shows invalid transitions represented empty cells test case based state transition diagram state table usually represented sequence events results sequence state changes actions if needed one test case usually cover several transitions between states exist coverage criteria state transition testing syllabus discusses three them 40 software testing all states coverage coverage items states achieve 100 all states coverage test cases must ensure all states visited coverage measured number visited states divided total number states expressed percentage valid transitions coverage also called 0 switch coverage coverage items single valid transitions achieve 100 valid transitions coverage test cases must exercise all valid transitions coverage measured number exercised valid transitions divided total number valid transitions expressed percentage all transitions coverage coverage items all transitions shown state table achieve 100 all transitions coverage test cases must exercise all valid transitions attempt execute invalid transitions testing only one invalid transition single test case helps avoid fault masking situation one defect prevents detection coverage measured number valid invalid transitions exercised attempted covered executed test cases divided total number valid invalid transitions expressed percentage all states coverage weaker valid transitions coverage because typically achieved without exercising all transitions valid transitions coverage widely used coverage criterion achieving full valid transitions coverage guarantees full all states coverage achieving full all transitions coverage guarantees both full all states coverage full valid transitions coverage should minimum requirement mission safety critical software 4.3 white box test techniques because their popularity simplicity focuses two code related white box test techniques statement testing branch testing rigorous techniques used some safety critical mission critical high integrity environments achieve thorough code coverage also white box test techniques used higher test levels api testing using coverage related code neuron coverage neural network testing techniques discussed syllabus 4.3.1 statement testing statement coverage statement testing coverage items executable statements aim design test cases exercise statements code until acceptable coverage achieved coverage measured number statements exercised test cases divided total number executable statements code expressed percentage 100 statement coverage achieved ensures all executable statements code been exercised at least once particular means statement defect executed cause failure demonstrating presence defect however exercising statement test case detect defects all cases example detect defects data dependent division zero only fails denominator set zero also 100 statement coverage ensure all decision logic been tested instance exercise all branches chapter 4.3.2 code 41 software testing 4.3.2 branch testing branch coverage branch transfer control between two nodes control flow graph shows possible sequences source code statements executed test object transfer control either unconditional straight line code conditional decision outcome branch testing coverage items branches aim design test cases exercise branches code until acceptable coverage achieved coverage measured number branches exercised test cases divided total number branches expressed percentage 100 branch coverage achieved all branches code unconditional conditional exercised test cases conditional branches typically correspond true false outcome if then decision outcome switch case statement decision exit continue loop however exercising branch test case detect defects all cases example detect defects requiring execution specific path code branch coverage subsumes statement coverage means any set test cases achieving 100 branch coverage also achieves 100 statement coverage but vice versa 4.3.3 value white box testing fundamental strength all white box techniques share entire software implementation taken into account during testing facilitates defect detection software specification vague outdated incomplete corresponding weakness if software implement one requirements white box testing detect resulting defects omission watson 1996 white box techniques used static testing during dry runs code they well suited reviewing code yet ready execution hetzel 1988 well pseudocode other high- top down logic modeled control flow graph performing only black box testing provide measure actual code coverage white box coverage measures provide objective measurement coverage provide necessary information allow tests generated increase coverage subsequently increase confidence code 4.4 experience based test techniques used experience based test techniques discussed following sections error guessing exploratory testing checklist based testing 4.4.1 error guessing error guessing technique used anticipate occurrence errors defects failures based ’s knowledge how application worked past 42 software testing types errors developers tend make types defects result errors types failures occurred other similar applications general errors defects failures related input correct input accepted parameters wrong missing output wrong format wrong result logic missing cases wrong operator computation incorrect operand wrong computation interfaces parameter mismatch incompatible types data incorrect initialization wrong type fault attacks methodical approach implementation error guessing technique requires create acquire list possible errors defects failures design tests identify defects associated errors expose defects cause failures lists built based experience defect failure data knowledge software fails whittaker 2002 whittaker 2003 andrews 2006 information error guessing fault attacks 4.4.2 exploratory testing exploratory testing tests simultaneously designed executed evaluated while learns test object testing used learn test object explore deeply focused tests create tests untested areas exploratory testing sometimes conducted using session based testing structure testing session based approach exploratory testing conducted defined time box uses test charter containing test objectives guide testing test session usually followed debriefing involves discussion between stakeholders interested test results test session approach test objectives treated high test conditions coverage items identified exercised during test session use test session sheets document steps followed discoveries made exploratory testing useful few inadequate specifications significant time pressure testing exploratory testing also useful complement other formal test techniques exploratory testing effective if experienced domain knowledge high degree essential skills like analytical skills curiosity creativeness 1.5.1 exploratory testing incorporate use other test techniques equivalence partitioning information exploratory testing found kaner 1999 whittaker 2009 hendrickson 2013 4.4.3 checklist based testing checklist based testing designs implements executes tests cover test conditions checklist checklists built based experience knowledge important user understanding how software fails checklists should contain items checked automatically items better suited entry exit criteria items too general brykczynski 1999 checklist items phrased form question should possible check item separately directly items refer requirements graphical interface properties quality characteristics other forms test conditions checklists created support various test types functional non functional testing 10 heuristics usability testing nielsen 1994 43 software testing some checklist entries gradually become less effective time because developers learn avoid making same errors new entries also need added reflect newly found high severity defects therefore checklists should regularly updated based defect analysis however care should taken avoid letting checklist become too long gawande 2009 absence detailed test cases checklist based testing provide guidelines some degree consistency testing if checklists high some variability actual testing likely occur resulting potentially greater coverage but less repeatability 4.5 collaboration based test approaches mentioned techniques sections 4.2 4.3 4.4 particular objective respect defect detection collaboration based approaches other hand focus also defect avoidance collaboration communication 4.5.1 collaborative user story writing user story represents feature valuable either user purchaser system software user stories three critical aspects jeffries 2000 called together 3 c ’s card medium describing user story index card entry electronic conversation explains how software used documented verbal confirmation acceptance criteria 4.5.2 format user story role i want goal accomplished so i resulting business value role followed acceptance criteria collaborative authorship user story use techniques brainstorming mind mapping collaboration allows team obtain shared vision should delivered taking into account three perspectives business development testing good user stories should independent negotiable valuable estimable small testable invest if stakeholder know how test user story indicate user story clear enough reflect something valuable them stakeholder just needs help testing wake 2003 4.5.2 acceptance criteria acceptance criteria user story conditions implementation user story must meet accepted stakeholders perspective acceptance criteria viewed test conditions should exercised tests acceptance criteria usually result conversation 4.5.1 acceptance criteria used define scope user story reach consensus among stakeholders describe both positive negative scenarios serve basis user story acceptance testing 4.5.3 44 software testing allow accurate planning estimation several ways write acceptance criteria user story two formats scenario oriented given then format used bdd 2.1.3 rule oriented bullet point verification list tabulated form input output mapping acceptance criteria documented one two formats however team use custom format long acceptance criteria well defined unambiguous 4.5.3 acceptance test driven development atdd atdd test first approach 2.1.3 test cases created prior implementing user story test cases created team members different perspectives customers developers testers adzic 2009 test cases executed manually automated first step specification workshop where user story if yet defined acceptance criteria analyzed discussed written team members incompleteness ambiguities defects user story resolved during process next step create test cases done team whole individually test cases based acceptance criteria seen examples how software works help team implement user story correctly since examples tests same terms used interchangeably during test design test techniques described sections 4.2 4.3 4.4 applied typically first test cases positive confirming correct behavior without exceptions error conditions comprising sequence activities executed if everything goes expected after positive test cases done team should perform negative testing finally team should cover non functional quality characteristics well performance efficiency usability test cases should expressed way understandable stakeholders typically test cases contain sentences natural language involving necessary preconditions if any inputs postconditions test cases must cover all characteristics user story should go beyond story however acceptance criteria detail some issues described user story addition no two test cases should describe same characteristics user story captured format supported test automation framework developers automate test cases writing supporting code they implement feature described user story acceptance tests then become executable requirements 45 software testing 5 managing test activities 335 minutes keywords defect management defect report entry criteria exit criteria product risk project risk risk risk analysis risk assessment risk control risk identification risk risk management risk mitigation risk monitoring risk based testing test approach test completion report test control test monitoring test plan test planning test progress report test pyramid testing quadrants learning objectives chapter 5 5.1 test planning fl-5.1.1 k2 exemplify purpose content test plan fl-5.1.2 k1 recognize how adds value iteration release planning fl-5.1.3 k2 compare contrast entry criteria exit criteria fl-5.1.4 k3 use estimation techniques calculate required test effort fl-5.1.5 k3 apply test case prioritization fl-5.1.6 k1 recall concepts test pyramid fl-5.1.7 k2 summarize testing quadrants their relationships test levels test types 5.2 risk management fl-5.2.1 k1 identify risk using risk likelihood risk impact fl-5.2.2 k2 distinguish between project risks product risks fl-5.2.3 k2 explain how product risk analysis influence thoroughness scope testing fl-5.2.4 k2 explain measures taken response analyzed product risks 5.3 test monitoring test control test completion fl-5.3.1 k1 recall metrics used testing fl-5.3.2 k2 summarize purposes content audiences test reports fl-5.3.3 k2 exemplify how communicate status testing 5.4 configuration management fl-5.4.1 k2 summarize how configuration management supports testing 5.5 defect management fl-5.5.1 k3 prepare defect report 46 software testing 5.1 test planning 5.1.1 purpose content test plan test plan describes objectives resources processes test project test plan documents means schedule achieving test objectives helps ensure performed test activities meet established criteria serves means communication team members other stakeholders demonstrates testing adhere existing test policy test strategy explains testing deviate them test planning guides testers thinking forces testers confront future challenges related risks schedules people tools costs effort etc process preparing test plan useful way think through efforts needed achieve test project objectives typical content test plan includes context testing scope test objectives constraints test basis assumptions constraints test project stakeholders roles responsibilities relevance testing hiring training needs communication forms frequency communication documentation templates risk register product risks project risks test approach test levels test types test techniques test deliverables entry criteria exit criteria independence testing metrics collected test data requirements test environment requirements deviations organizational test policy test strategy budget schedule details test plan content found iso iec ieee 29119 3 standard 5.1.2 's contribution iteration release planning iterative sdlcs typically two kinds planning occur release planning iteration planning release planning looks ahead release product defines re defines product backlog involve refining larger user stories into set smaller user stories also serves basis test approach test plan across all iterations testers involved release planning participate writing testable user stories acceptance criteria 4.5 participate project quality risk analyses 5.2 estimate test effort associated user stories 5.1.4 determine test approach plan testing release iteration planning looks ahead end single iteration concerned iteration backlog testers involved iteration planning participate detailed risk analysis user stories determine testability user stories break down user stories into tasks particularly testing tasks estimate test effort all testing tasks identify refine functional non functional aspects test object 47 software testing 5.1.3 entry criteria exit criteria entry criteria define preconditions undertaking given activity if entry criteria met likely activity prove difficult time consuming costly riskier exit criteria define must achieved order declare activity completed entry criteria exit criteria should defined test differ based test objectives typical entry criteria include availability resources people tools environments test data budget time availability testware test basis testable requirements user stories test cases initial quality test object all smoke tests passed typical exit criteria include measures thoroughness achieved coverage number unresolved defects defect density number failed test cases completion criteria planned tests been executed static testing been performed all defects found reported all regression tests automated running out time budget also viewed valid exit criteria without other exit criteria satisfied acceptable end testing circumstances if stakeholders reviewed accepted risk go live without further testing agile software development exit criteria called definition done defining team ’s objective metrics releasable item entry criteria user story must fulfill start development and/or testing activities called definition ready 5.1.4 estimation techniques test effort estimation involves predicting amount test related needed meet objectives test project important make clear stakeholders estimate based number assumptions always subject estimation error estimation small tasks usually accurate large ones therefore estimating large task decomposed into set smaller tasks then turn estimated syllabus following four estimation techniques described estimation based ratios metrics based technique figures collected previous projects organization makes possible derive standard ratios similar projects ratios organization ’s own projects taken historical data generally best source use estimation process standard ratios then used estimate test effort new project example if previous project development test effort ratio 3:2 current project development effort expected 600 person days test effort estimated 400 person days extrapolation metrics based technique measurements made early possible current project gather data having enough observations effort required remaining approximated extrapolating data usually applying mathematical model method very suitable iterative sdlcs example team extrapolate test effort forthcoming iteration averaged effort last three iterations wideband delphi iterative expert based technique experts make experience based estimations expert isolation estimates effort results collected if deviations out range agreed upon boundaries experts discuss their current estimates expert then asked make new estimation based feedback again isolation process repeated until consensus reached planning poker variant wideband delphi used agile 48 software testing software development planning poker estimates usually made using cards numbers represent effort size three point estimation expert based technique three estimations made experts optimistic estimation likely estimation m pessimistic estimation b final estimate e their weighted arithmetic mean popular version technique estimate calculated e = + 4*m + b 6 advantage technique allows experts calculate measurement error sd = b 6 example if estimates person- hours a=6 m=9 b=18 then final estimation 10±2 person hours between 8 12 person hours because e = 6 + 4 9 + 18 6 = 10 sd = 18 6 6 = 2 kan 2003 koomen 2006 westfall 2009 other test estimation techniques 5.1.5 test case prioritization once test cases test procedures specified assembled into test suites test suites arranged test execution schedule defines order they run prioritizing test cases different factors taken into account used test case prioritization strategies follows risk based prioritization where order test execution based results risk analysis 5.2.3 test cases covering important risks executed first coverage based prioritization where order test execution based coverage statement coverage test cases achieving highest coverage executed first variant called coverage prioritization test case achieving highest coverage executed first subsequent test case one achieves highest coverage requirements based prioritization where order test execution based priorities requirements traced back corresponding test cases requirement priorities defined stakeholders test cases related important requirements executed first ideally test cases would ordered run based their priority levels using example one mentioned prioritization strategies however practice if test cases features tested dependencies if test case higher priority dependent test case lower priority lower priority test case must executed first order test execution must also take into account availability resources example required test tools test environments people only available specific time window 5.1.6 test pyramid test pyramid model showing different tests different granularity test pyramid model supports team test automation test effort allocation showing different goals supported different levels test automation pyramid layers represent groups tests higher layer lower test granularity test isolation test execution time tests bottom layer small isolated fast check small piece functionality so usually lot them needed achieve reasonable coverage top layer represents complex high end end tests high tests generally slower tests lower layers they typically check large piece functionality so usually just few them needed achieve reasonable coverage number naming layers differ example original test pyramid model cohn 2009 defines three layers unit tests service tests ui tests popular model defines unit 49 software testing component tests integration component integration tests end end tests other test levels 2.2.1 also used 5.1.7 testing quadrants testing quadrants defined brian marick marick 2003 crispin 2008 group test levels appropriate test types activities test techniques products agile software development model supports test management visualizing ensure all appropriate test types test levels included sdlc understanding some test types relevant certain test levels others model also provides way differentiate describe types tests all stakeholders developers testers business representatives model tests business facing technology facing tests also support team guide development critique product measure behavior against expectations combination two viewpoints determines four quadrants quadrant q1 technology facing support team quadrant contains component component integration tests tests should automated included ci process quadrant q2 business facing support team quadrant contains functional tests examples user story tests user experience prototypes api testing simulations tests check acceptance criteria manual automated quadrant q3 business facing critique product quadrant contains exploratory testing usability testing user acceptance testing tests user oriented manual quadrant q4 technology facing critique product quadrant contains smoke tests non functional tests except usability tests tests automated 5.2 risk management organizations face internal external factors make uncertain whether they achieve their objectives iso 31000 risk management allows organizations increase likelihood achieving objectives improve quality their products increase stakeholders confidence trust main risk management activities risk analysis consisting risk identification risk assessment 5.2.3 risk control consisting risk mitigation risk monitoring 5.2.4 test approach test activities selected prioritized managed based risk analysis risk control called risk based testing 5.2.1 risk definition risk attributes risk potential event hazard threat situation whose occurrence causes adverse effect risk characterized two factors risk likelihood probability risk occurrence greater zero less one risk impact harm consequences occurrence 50 software testing two factors express risk measure risk higher risk important treatment 5.2.2 project risks product risks software testing one generally concerned two types risks project risks product risks project risks related management control project project risks include organizational issues delays products deliveries inaccurate estimates cost cutting people issues insufficient skills conflicts communication problems shortage staff technical issues scope creep poor tool support supplier issues third party delivery failure bankruptcy supporting company project risks they occur impact project schedule budget scope affects project 's ability achieve objectives product risks related product quality characteristics described iso 25010 quality model examples product risks include missing wrong functionality incorrect calculations runtime errors poor architecture inefficient algorithms inadequate response time poor user experience security vulnerabilities product risks they occur result various negative consequences user dissatisfaction loss revenue trust reputation damage third parties high maintenance costs overload helpdesk criminal penalties extreme cases physical damage injuries death 5.2.3 product risk analysis testing perspective goal product risk analysis provide awareness product risk order focus testing effort way minimizes residual product risk ideally product risk analysis begins early sdlc product risk analysis consists risk identification risk assessment risk identification generating comprehensive list risks stakeholders identify risks using various techniques tools brainstorming workshops interviews cause effect diagrams risk assessment involves categorization identified risks determining their risk likelihood risk impact prioritizing proposing ways handle them categorization helps assigning mitigation actions because usually risks falling into same category mitigated using similar approach risk assessment use quantitative qualitative approach mix them quantitative approach risk calculated multiplication risk likelihood risk impact qualitative approach risk determined using risk matrix product risk analysis influence thoroughness scope testing results used 51 software testing determine scope testing carried out determine particular test levels propose test types performed determine test techniques employed coverage achieved estimate test effort required task prioritize testing attempt find critical defects early possible determine whether any activities addition testing could employed reduce risk 5.2.4 product risk control product risk control comprises all measures taken response identified assessed product risks product risk control consists risk mitigation risk monitoring risk mitigation involves implementing actions proposed risk assessment reduce risk aim risk monitoring ensure mitigation actions effective obtain further information improve risk assessment identify emerging risks respect product risk control once risk been analyzed several response options risk possible risk mitigation testing risk acceptance risk transfer contingency plan veenendaal 2012 actions taken mitigate product risks testing follows select testers right experience skills suitable given risk type apply appropriate independence testing conduct reviews perform static analysis apply appropriate test techniques coverage levels apply appropriate test types addressing affected quality characteristics perform dynamic testing regression testing 5.3 test monitoring test control test completion test monitoring concerned gathering information testing information used assess test progress measure whether test exit criteria test tasks associated exit criteria satisfied meeting targets coverage product risks requirements acceptance criteria test control uses information test monitoring provide form control directives guidance necessary corrective actions achieve effective efficient testing examples control directives include reprioritizing tests identified risk becomes issue re evaluating whether test item meets entry criteria exit criteria due rework adjusting test schedule address delay delivery test environment adding new resources where needed 52 software testing test completion collects data completed test activities consolidate experience testware any other relevant information test completion activities occur at project milestones test completed agile iteration finished test project completed cancelled software system released maintenance release completed 5.3.1 metrics used testing test metrics gathered show progress against planned schedule budget current quality test object effectiveness test activities respect objectives iteration goal test monitoring gathers variety metrics support test control test completion test metrics include project progress metrics task completion resource usage test effort test progress metrics test case implementation progress test environment preparation progress number test cases run run passed failed test execution time product quality metrics availability response time mean time failure defect metrics number priorities defects found fixed defect density defect detection percentage risk metrics residual risk coverage metrics requirements coverage code coverage cost metrics cost testing organizational cost quality 5.3.2 purpose content audience test reports test reporting summarizes communicates test information during after testing test progress reports support ongoing control testing must provide enough information make modifications test schedule resources test plan changes needed due deviation plan changed circumstances test completion reports summarize specific stage testing test test cycle iteration give information subsequent testing during test monitoring control test team generates test progress reports stakeholders keep them informed test progress reports usually generated regular basis daily weekly etc include test period test progress ahead behind schedule any notable deviations impediments testing their workarounds test metrics 5.3.1 examples new changed risks testing period testing planned next period 53 software testing test completion report prepared during test completion project test test type complete ideally exit criteria been met report uses test progress reports other data typical test completion reports include test summary testing product quality evaluation based original test plan test objectives exit criteria deviations test plan differences planned schedule duration effort testing impediments workarounds test metrics based test progress reports unmitigated risks defects fixed lessons learned relevant testing different audiences require different information reports influence degree formality frequency reporting reporting test progress others same team frequent informal while reporting testing completed project follows set template occurs only once iso iec ieee 29119 3 standard includes templates examples test progress reports called test status reports test completion reports 5.3.3 communicating status testing best means communicating test status varies depending test management concerns organizational test strategies regulatory standards case self organizing teams 1.5.2 team itself options include verbal communication team members other stakeholders dashboards ci cd dashboards task boards burn down charts electronic communication channels email chat online documentation formal test reports 5.3.2 one options used formal communication appropriate distributed teams where direct face face communication always possible due geographical distance time differences typically different stakeholders interested different types information so communication should tailored accordingly 5.4 configuration management testing configuration management cm provides discipline identifying controlling tracking products test plans test strategies test conditions test cases test scripts test results test logs test reports configuration items 54 software testing complex configuration item test environment cm records items consists their relationships versions if configuration item approved testing becomes baseline only changed through formal change control process configuration management keeps record changed configuration items new baseline created possible revert previous baseline reproduce previous test results properly support testing cm ensures following all configuration items test items individual parts test object uniquely identified version controlled tracked changes related other configuration items so traceability maintained throughout test process all identified documentation software items referenced unambiguously test documentation continuous integration continuous delivery continuous deployment associated testing typically implemented part automated devops pipeline 2.1.4 automated cm normally included 5.5 defect management since one major test objectives find defects established defect management process essential although refer defects here reported anomalies turn out real defects something else false positive change request resolved during process dealing defect reports anomalies reported during any phase sdlc form depends sdlc at minimum defect management process includes workflow handling individual anomalies their discovery their closure rules their classification workflow typically comprises activities log reported anomalies analyze classify them decide suitable response fix keep finally close defect report process must followed all involved stakeholders advisable handle defects static testing especially static analysis similar way typical defect reports following objectives provide those responsible handling resolving reported defects sufficient information resolve issue provide means tracking quality product provide ideas improvement development test process defect report logged during dynamic testing typically includes unique identifier title short summary anomaly reported date anomaly observed issuing organization author their role identification test object test environment context defect test case run test activity performed sdlc phase other relevant information test technique checklist test data used 55 software testing description failure enable reproduction resolution steps detected anomaly any relevant test logs database dumps screenshots recordings expected results actual results severity defect degree impact interests stakeholders requirements priority fix status defect open deferred duplicate waiting fixed awaiting confirmation testing re opened closed rejected references test case some data automatically included using defect management tools identifier date author initial status document templates defect report example defect reports found iso iec ieee 29119 3 standard refers defect reports incident reports