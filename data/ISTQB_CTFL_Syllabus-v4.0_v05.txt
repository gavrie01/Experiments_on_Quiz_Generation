1.1. What is Testing? Software systems are an integral part of our daily life. Most people have had experience with software that did not work as expected. Software that does not work correctly can lead to many problems including loss of money time business reputation and extreme cases even injury death. Software testing assesses software quality and helps reducing risk of software failure operation. Software testing is a set of activities to discover defects and evaluate quality of software artifacts. These artifacts when being tested are known as test objects. A common misconception about testing is that it only consists of executing tests . However software testing also includes other activities and must be aligned with software development lifecycle . Another common misconception about testing is that testing focuses entirely on verifying test object. Whilst testing involves verification i.e. checking whether system meets specified requirements it also involves validation which means checking whether system meets users’ and other stakeholders’ needs its operational environment. Testing may be dynamic static. Dynamic testing involves execution of software while static testing does not. Static testing includes reviews and static analysis. Dynamic testing uses different types of test techniques and test approaches to derive test cases . Testing is not only a technical activity. It also needs to be properly planned managed estimated monitored and controlled . Testers use tools but it is important to remember that testing is largely an intellectual activity requiring testers to have specialized knowledge use analytical skills and apply critical thinking and systems thinking . ISO/IEC/IEEE 29119-1 standard provides further information about software testing concepts. 1.1.1. Test Objectives typical test objectives are: Evaluating work products such as requirements user stories designs and code Triggering failures and finding defects Ensuring required coverage of a test object Reducing level of risk of inadequate software quality Verifying whether specified requirements have been fulfilled Verifying that a test object complies with contractual legal and regulatory requirements Providing information to stakeholders to allow them to make informed decisions Building confidence quality of test object Validating whether test object is complete and works as expected by stakeholders Objectives of testing can vary depending upon context which includes work product being tested test level risks software development lifecycle being followed and factors related to business context e.g. corporate structure competitive considerations time to market. v4.0 Page of 74 2023-04-21 1.1.2. Testing and Debugging Testing and debugging are separate activities. Testing can trigger failures that are caused by defects software can directly find defects test object . When dynamic testing triggers a failure debugging is concerned with finding causes of this failure analyzing these causes and eliminating them. typical debugging process this case involves: Reproduction of a failure Diagnosis Fixing cause Subsequent confirmation testing checks whether fixes resolved problem. Preferably confirmation testing is done by same person who performed initial test. Subsequent regression testing can also be performed to check whether fixes are causing failures other parts of test object . When static testing identifies a defect debugging is concerned with removing it. There is no need for reproduction diagnosis since static testing directly finds defects and cannot cause failures . 1.2. Why is Testing Necessary? Testing as a form of quality control helps achieving agreed upon goals within set scope time quality and budget constraints. Testing’s contribution to success should not be restricted to test team activities. Any stakeholder can use their testing skills to bring project closer to success. Testing components systems and associated documentation helps to identify defects software 1.2.1. Testing’s Contributions to Success Testing provides a cost-effective means of detecting defects. These defects can then be removed so testing indirectly contributes to higher quality test objects. Testing provides a means of directly evaluating quality of a test object at various stages SDLC. These measures are used as part of a larger project management activity contributing to decisions to move to next stage of SDLC such as release decision. Testing provides users with indirect representation on development project. Testers ensure that their understanding of users’ needs are considered throughout development lifecycle. alternative is to involve a representative set of users as part of development project which is not usually possible due to high costs and lack of availability of suitable users. Testing may also be required to meet contractual legal requirements to comply with regulatory standards. 1.2.2. Testing and Quality Assurance While people often use terms “testing” and “quality assurance” interchangeably testing and QA are not same. Testing is a form of quality control . v4.0 Page of 74 2023-04-21 QC is a product-oriented corrective approach that focuses on those activities supporting achievement of appropriate levels of quality. Testing is a major form of quality control while others include formal methods simulation and prototyping. QA is a process-oriented preventive approach that focuses on implementation and improvement of processes. It works on basis that if a good process is followed correctly then it will generate a good product. QA applies to both development and testing processes and is responsibility of everyone on a project. Test results are used by QA and QC. QC they are used to fix defects while QA they provide feedback on how well development and test processes are performing. 1.2.3. Errors Defects Failures and Root Causes Human beings make errors which produce defects which turn may result failures. Humans make errors for various reasons such as time pressure complexity of work products processes infrastructure interactions simply because they are tired lack adequate training. Defects can be found documentation such as a requirements specification a test script source code a supporting artifact such as a build file. Defects artifacts produced earlier SDLC if undetected often lead to defective artifacts later lifecycle. If a defect code is executed system may fail to do what it should do do something it shouldn’t causing a failure. Some defects will always result a failure if executed while others will only result a failure specific circumstances and some may never result a failure. Errors and defects are not only cause of failures. Failures can also be caused by environmental conditions such as when radiation electromagnetic field cause defects firmware. A root cause is a fundamental reason for occurrence of a problem . Root causes are identified through root cause analysis which is typically performed when a failure occurs a defect is identified. It is believed that further similar failures defects can be prevented their frequency reduced by addressing root cause such as by removing it. 1.3. Testing Principles A number of testing principles offering general guidelines applicable to all testing have been suggested over years. This syllabus describes seven such principles. 1. Testing shows presence not absence of defects. Testing can show that defects are present test object but cannot prove that there are no defects . Testing reduces probability of defects remaining undiscovered test object but even if no defects are found testing cannot prove test object correctness. 2. Exhaustive testing is impossible. Testing everything is not feasible except trivial cases . Rather than attempting to test exhaustively test techniques test case prioritization and risk-based testing should be used to focus test efforts. 3. Early testing saves time and money. Defects that are removed early process will not cause subsequent defects derived work products. cost of quality will be reduced since fewer failures will occur later SDLC . To find defects early both static testing and dynamic testing should be started as early as possible. 4. Defects cluster together. A small number of system components usually contain most of defects discovered are responsible for most of operational failures . This phenomenon is an v4.0 Page of 74 2023-04-21 illustration of Pareto principle. Predicted defect clusters and actual defect clusters observed during testing operation are an important input for risk-based testing . 5. Tests wear out. If same tests are repeated many times they become increasingly ineffective detecting new defects . To overcome this effect existing tests and test data may need to be modified and new tests may need to be written. However some cases repeating same tests can have a beneficial outcome e.g. automated regression testing . 6. Testing is context dependent. There is no single universally applicable approach to testing. Testing is done differently different contexts . 7. Absence-of-defects fallacy. It is a fallacy to expect that software verification will ensure success of a system. Thoroughly testing all specified requirements and fixing all defects found could still produce a system that does not fulfill users’ needs and expectations that does not help achieving customer’s business goals and that is inferior compared to other competing systems. addition to verification validation should also be carried out . 1.4. Test Activities Testware and Test Roles Testing is context dependent but at a high level there are common sets of test activities without which testing is less likely to achieve test objectives. These sets of test activities form a test process. test process can be tailored to a given situation based on various factors. Which test activities are included this test process how they are implemented and when they occur is normally decided as part of test planning for specific situation . following sections describe general aspects of this test process terms of test activities and tasks impact of context testware traceability between test basis and testware and testing roles. ISO/IEC/IEEE 29119-2 standard provides further information about test processes. 1.4.1. Test Activities and Tasks A test process usually consists of main groups of activities described below. Although many of these activities may appear to follow a logical sequence they are often implemented iteratively parallel. These testing activities usually need to be tailored to system and project. Test planning consists of defining test objectives and then selecting an approach that best achieves objectives within constraints imposed by overall context. Test planning is further explained section 5.1. Test monitoring and control. Test monitoring involves ongoing checking of all test activities and comparison of actual progress against plan. Test control involves taking actions necessary to meet objectives of testing. Test monitoring and control are further explained section 5.3. Test analysis includes analyzing test basis to identify testable features and to define and prioritize associated test conditions together with related risks and risk levels . test basis and test objects are also evaluated to identify defects they may contain and to assess their testability. Test analysis is often supported by use of test techniques . Test analysis answers question “what to test?” terms of measurable coverage criteria. Test design includes elaborating test conditions into test cases and other testware . This activity often involves identification of coverage items which serve as a guide to specify test case inputs. Test techniques can be used to support this activity. Test design v4.0 Page of 74 2023-04-21 also includes defining test data requirements designing test environment and identifying any other required infrastructure and tools. Test design answers question “how to test?”. Test implementation includes creating acquiring testware necessary for test execution . Test cases can be organized into test procedures and are often assembled into test suites. Manual and automated test scripts are created. Test procedures are prioritized and arranged within a test execution schedule for efficient test execution . test environment is built and verified to be set up correctly. Test execution includes running tests accordance with test execution schedule . Test execution may be manual automated. Test execution can take many forms including continuous testing pair testing sessions. Actual test results are compared with expected results. test results are logged. Anomalies are analyzed to identify their likely causes. This analysis allows us to report anomalies based on failures observed . Test completion activities usually occur at project milestones for any unresolved defects change requests product backlog items created. Any testware that may be useful future is identified and archived handed over to appropriate teams. test environment is shut down to an agreed state. test activities are analyzed to identify lessons learned and improvements for future iterations releases projects . A test completion report is created and communicated to stakeholders. 1.4.2. Test Process Context Testing is not performed isolation. Test activities are an integral part of development processes carried out within an organization. Testing is also funded by stakeholders and its final goal is to help fulfill stakeholders’ business needs. Therefore way testing is carried out will depend on a number of contextual factors including: Stakeholders Team members Business domain Technical factors Project constraints Organizational factors Software development lifecycle Tools These factors will have an impact on many test-related issues including: test strategy test techniques used degree of test automation required level of coverage level of detail of test documentation reporting etc. 1.4.3. Testware Testware is created as output work products from test activities described section 1.4.1. There is a significant variation how different organizations produce shape name organize and manage their v4.0 Page of 74 2023-04-21 work products. Proper configuration management ensures consistency and integrity of work products. following list of work products is not exhaustive: Test planning work products include: test plan test schedule risk register and entry and exit criteria . Risk register is a list of risks together with risk likelihood risk impact and information about risk mitigation . Test schedule risk register and entry and exit criteria are often a part of test plan. Test monitoring and control work products include: test progress reports documentation of control directives and risk information . Test analysis work products include: test conditions and defect reports regarding defects test basis . Test design work products include: test cases test charters coverage items test data requirements and test environment requirements. Test implementation work products include: test procedures automated test scripts test suites test data test execution schedule and test environment elements. Examples of test environment elements include: stubs drivers simulators and service virtualizations. Test execution work products include: test logs and defect reports . Test completion work products include: test completion report action items for improvement of subsequent projects iterations documented lessons learned and change requests . 1.4.4. Traceability between Test Basis and Testware order to implement effective test monitoring and control it is important to establish and maintain traceability throughout test process between test basis elements testware associated with these elements test results and detected defects. Accurate traceability supports coverage evaluation so it is very useful if measurable coverage criteria are defined test basis. coverage criteria can function as key performance indicators to drive activities that show to what extent test objectives have been achieved . For example: Traceability of test cases to requirements can verify that requirements are covered by test cases. Traceability of test results to risks can be used to evaluate level of residual risk a test object. addition to evaluating coverage good traceability makes it possible to determine impact of changes facilitates test audits and helps meet IT governance criteria. Good traceability also makes test progress and completion reports more easily understandable by including status of test basis elements. This can also assist communicating technical aspects of testing to stakeholders an understandable manner. Traceability provides information to assess product quality process capability and project progress against business goals. v4.0 Page of 74 2023-04-21 1.4.5. Roles Testing this syllabus two principal roles testing are covered: a test management role and a testing role. activities and tasks assigned to these two roles depend on factors such as project and product context skills of people roles and organization. test management role takes overall responsibility for test process test team and leadership of test activities. test management role is mainly focused on activities of test planning test monitoring and control and test completion. way which test management role is carried out varies depending on context. For example Agile software development some of test management tasks may be handled by Agile team. Tasks that span multiple teams entire organization may be performed by test managers outside of development team. testing role takes overall responsibility for engineering aspect of testing. testing role is mainly focused on activities of test analysis test design test implementation and test execution. Different people may take on these roles at different times. For example test management role can be performed by a team leader by a test manager by a development manager etc. It is also possible for one person to take on roles of testing and test management at same time. 1.5. Essential Skills and Good Practices Testing Skill is ability to do something well that comes from one’s knowledge practice and aptitude. Good testers should possess some essential skills to do their job well. Good testers should be effective team players and should be able to perform testing on different levels of test independence. 1.5.1. Generic Skills Required for Testing While being generic following skills are particularly relevant for testers: Testing knowledge Thoroughness carefulness curiosity attention to details being methodical Good communication skills active listening being a team player Analytical thinking critical thinking creativity Technical knowledge Domain knowledge Testers are often bearers of bad news. It is a common human trait to blame bearer of bad news. This makes communication skills crucial for testers. Communicating test results may be perceived as criticism of product and of its author. Confirmation bias can make it difficult to accept information that disagrees with currently held beliefs. Some people may perceive testing as a destructive activity even though it contributes greatly to project success and product quality. To try to improve this view information about defects and failures should be communicated a constructive way. v4.0 Page of 74 2023-04-21 1.5.2. Whole Team Approach One of important skills for a tester is ability to work effectively a team context and to contribute positively to team goals. whole team approach – a practice coming from Extreme Programming – builds upon this skill. whole-team approach any team member with necessary knowledge and skills can perform any task and everyone is responsible for quality. team members share same workspace as co-location facilitates communication and interaction. whole team approach improves team dynamics enhances communication and collaboration within team and creates synergy by allowing various skill sets within team to be leveraged for benefit of project. Testers work closely with other team members to ensure that desired quality levels are achieved. This includes collaborating with business representatives to help them create suitable acceptance tests and working with developers to agree on test strategy and decide on test automation approaches. Testers can thus transfer testing knowledge to other team members and influence development of product. Depending on context whole team approach may not always be appropriate. For instance some situations such as safety-critical a high level of test independence may be needed. 1.5.3. Independence of Testing A certain degree of independence makes tester more effective at finding defects due to differences between author’s and tester’s cognitive biases . Independence is not however a replacement for familiarity e.g. developers can efficiently find many defects their own code. Work products can be tested by their author by author's peers from same team by testers from outside author's team but within organization by testers from outside organization . For most projects it is usually best to carry out testing with multiple levels of independence . main benefit of independence of testing is that independent testers are likely to recognize different kinds of failures and defects compared to developers because of their different backgrounds technical perspectives and biases. Moreover an independent tester can verify challenge disprove assumptions made by stakeholders during specification and implementation of system. However there are also some drawbacks. Independent testers may be isolated from development team which may lead to a lack of collaboration communication problems an adversarial relationship with development team. Developers may lose a sense of responsibility for quality. Independent testers may be seen as a bottleneck be blamed for delays release. v4.0 Page of 74 2023-04-21 2. Testing Throughout Software Development Lifecycle – 130 minutes Keywords acceptance testing black-box testing component integration testing component testing confirmation testing functional testing integration testing maintenance testing non-functional testing regression testing shift-left system integration testing system testing test level test object test type white-box testing Learning Objectives for Chapter 2: 2.1 Testing Context of a Software Development Lifecycle FL-2.1.1 Explain impact of chosen software development lifecycle on testing FL-2.1.2 Recall good testing practices that apply to all software development lifecycles FL-2.1.3 Recall examples of test-first approaches to development FL-2.1.4 Summarize how DevOps might have an impact on testing FL-2.1.5 Explain shift-left approach FL-2.1.6 Explain how retrospectives can be used as a mechanism for process improvement 2.2 Test Levels and Test Types FL-2.2.1 Distinguish different test levels FL-2.2.2 Distinguish different test types FL-2.2.3 Distinguish confirmation testing from regression testing 2.3 Maintenance Testing FL-2.3.1 Summarize maintenance testing and its triggers v4.0 Page of 74 2023-04-21 2.1. Testing Context of a Software Development Lifecycle A software development lifecycle model is an abstract high-level representation of software development process. A SDLC model defines how different development phases and types of activities performed within this process relate to each other both logically and chronologically. Examples of SDLC models include: sequential development models iterative development models and incremental development models . Some activities within software development processes can also be described by more detailed software development methods and Agile practices. Examples include: acceptance test-driven development behavior-driven development domain-driven design extreme programming feature-driven development Kanban Lean IT Scrum and test-driven development . 2.1.1. Impact of Software Development Lifecycle on Testing Testing must be adapted to SDLC to succeed. choice of SDLC impacts on the: Scope and timing of test activities Level of detail of test documentation Choice of test techniques and test approach Extent of test automation Role and responsibilities of a tester sequential development models initial phases testers typically participate requirement reviews test analysis and test design. executable code is usually created later phases so typically dynamic testing cannot be performed early SDLC. some iterative and incremental development models it is assumed that each iteration delivers a working prototype product increment. This implies that each iteration both static and dynamic testing may be performed at all test levels. Frequent delivery of increments requires fast feedback and extensive regression testing. Agile software development assumes that change may occur throughout project. Therefore lightweight work product documentation and extensive test automation to make regression testing easier are favored agile projects. Also most of manual testing tends to be done using experience-based test techniques that do not require extensive prior test analysis and design. 2.1.2. Software Development Lifecycle and Good Testing Practices Good testing practices independent of chosen SDLC model include following: For every software development activity there is a corresponding test activity so that all development activities are subject to quality control Different test levels have specific and different test objectives which allows for testing to be appropriately comprehensive while avoiding redundancy Test analysis and design for a given test level begins during corresponding development phase of SDLC so that testing can adhere to principle of early testing v4.0 Page of 74 2023-04-21 Testers are involved reviewing work products as soon as drafts of this documentation are available so that this earlier testing and defect detection can support shift-left strategy 2.1.3. Testing as a Driver for Software Development TDD ATDD and BDD are similar development approaches where tests are defined as a means of directing development. Each of these approaches implements principle of early testing and follows a shift-left approach since tests are defined before code is written. They support an iterative development model. These approaches are characterized as follows: Test-Driven Development : Directs coding through test cases Tests are written first then code is written to satisfy tests and then tests and code are refactored Acceptance Test-Driven Development : Derives tests from acceptance criteria as part of system design process Tests are written before part of application is developed to satisfy tests Behavior-Driven Development : Expresses desired behavior of an application with test cases written a simple form of natural language which is easy to understand by stakeholders – usually using Given/When/Then format. Test cases are then automatically translated into executable tests For all above approaches tests may persist as automated tests to ensure code quality future adaptions / refactoring. 2.1.4. DevOps and Testing DevOps is an organizational approach aiming to create synergy by getting development and operations to work together to achieve a set of common goals. DevOps requires a cultural shift within an organization to bridge gaps between development and operations while treating their functions with equal value. DevOps promotes team autonomy fast feedback integrated toolchains and technical practices like continuous integration and continuous delivery . This enables teams to build test and release high-quality code faster through a DevOps delivery pipeline . From testing perspective some of benefits of DevOps are: Fast feedback on code quality and whether changes adversely affect existing code CI promotes a shift-left approach testing by encouraging developers to submit high quality code accompanied by component tests and static analysis Promotes automated processes like CI/CD that facilitate establishing stable test environments Increases view on non-functional quality characteristics v4.0 Page of 74 2023-04-21 Automation through a delivery pipeline reduces need for repetitive manual testing risk regression is minimized due to scale and range of automated regression tests DevOps is not without its risks and challenges which include: DevOps delivery pipeline must be defined and established CI / CD tools must be introduced and maintained Test automation requires additional resources and may be difficult to establish and maintain Although DevOps comes with a high level of automated testing manual testing – especially from user's perspective – will still be needed. 2.1.5. Shift-Left Approach principle of early testing is sometimes referred to as shift-left because it is an approach where testing is performed earlier SDLC. Shift-left normally suggests that testing should be done earlier but it does not mean that testing later SDLC should be neglected. There are some good practices that illustrate how to achieve a “shift-left” testing which include: Reviewing specification from perspective of testing. These review activities on specifications often find potential defects such as ambiguities incompleteness and inconsistencies Writing test cases before code is written and have code run a test harness during code implementation Using CI and even better CD as it comes with fast feedback and automated component tests to accompany source code when it is submitted to code repository Completing static analysis of source code prior to dynamic testing as part of an automated process Performing non-functional testing starting at component test level where possible. This is a form of shift-left as these non-functional test types tend to be performed later SDLC when a complete system and a representative test environment are available A shift-left approach might result extra training effort and/or costs earlier process but is expected to save efforts and/or costs later process. For shift-left approach it is important that stakeholders are convinced and bought into this concept. 2.1.6. Retrospectives and Process Improvement Retrospectives are often held at end of a project an iteration at a release milestone can be held when needed. timing and organization of retrospectives depend on particular SDLC model being followed. these meetings participants discuss: What was successful and should be retained? v4.0 Page of 74 2023-04-21 What was not successful and could be improved? How to incorporate improvements and retain successes future? results should be recorded and are normally part of test completion report . Retrospectives are critical for successful implementation of continuous improvement and it is important that any recommended improvements are followed up. Typical benefits for testing include: Increased test effectiveness / efficiency Increased quality of testware Team bonding and learning Improved quality of test basis Better cooperation between development and testing 2.2. Test Levels and Test Types Test levels are groups of test activities that are organized and managed together. Each test level is an instance of test process performed relation to software at a given stage of development from individual components to complete systems where applicable systems of systems. Test levels are related to other activities within SDLC. sequential SDLC models test levels are often defined such that exit criteria of one level are part of entry criteria for next level. some iterative models this may not apply. Development activities may span through multiple test levels. Test levels may overlap time. Test types are groups of test activities related to specific quality characteristics and most of those test activities can be performed at every test level. 2.2.1. Test Levels this syllabus following five test levels are described: Component testing focuses on testing components isolation. It often requires specific support such as test harnesses unit test frameworks. Component testing is normally performed by developers their development environments. Component integration testing focuses on testing interfaces and interactions between components. Component integration testing is heavily dependent on integration strategy approaches like bottom-up top-down big-bang. System testing focuses on overall behavior and capabilities of an entire system product often including functional testing of end-to-end tasks and non-functional testing of quality characteristics. For some non-functional quality characteristics it is preferable to test them on a complete system a representative test environment . Using simulations of sub- v4.0 Page of 74 2023-04-21 systems is also possible. System testing may be performed by an independent test team and is related to specifications for system. System integration testing focuses on testing interfaces of system under test and other systems and external services . System integration testing requires suitable test environments preferably similar to operational environment. Acceptance testing focuses on validation and on demonstrating readiness for deployment which means that system fulfills user’s business needs. Ideally acceptance testing should be performed by intended users. main forms of acceptance testing are: user acceptance testing operational acceptance testing contractual and regulatory acceptance testing alpha testing and beta testing. Test levels are distinguished by following non-exhaustive list of attributes to avoid overlapping of test activities: Test object Test objectives Test basis Defects and failures Approach and responsibilities 2.2.2. Test Types A lot of test types exist and can be applied projects. this syllabus following four test types are addressed: Functional testing evaluates functions that a component system should perform. functions are “what” test object should do. main objective of functional testing is checking functional completeness functional correctness and functional appropriateness. Non-functional testing evaluates attributes other than functional characteristics of a component system. Non-functional testing is testing of “how well system behaves”. main objective of non- functional testing is checking non-functional software quality characteristics. ISO/IEC 25010 standard provides following classification of non-functional software quality characteristics: Performance efficiency Compatibility Usability Reliability Security Maintainability Portability It is sometimes appropriate for non-functional testing to start early life cycle . Many non-functional tests are derived from functional tests as v4.0 Page of 74 2023-04-21 they use same functional tests but check that while performing function a non-functional constraint is satisfied . late discovery of non-functional defects can pose a serious threat to success of a project. Non-functional testing sometimes needs a very specific test environment such as a usability lab for usability testing. Black-box testing is specification-based and derives tests from documentation external to test object. main objective of black-box testing is checking system's behavior against its specifications. White-box testing is structure-based and derives tests from system's implementation internal structure . main objective of white-box testing is to cover underlying structure by tests to acceptable level. All four above mentioned test types can be applied to all test levels although focus will be different at each level. Different test techniques can be used to derive test conditions and test cases for all mentioned test types. 2.2.3. Confirmation Testing and Regression Testing Changes are typically made to a component system to either enhance it by adding a new feature to fix it by removing a defect. Testing should then also include confirmation testing and regression testing. Confirmation testing confirms that an original defect has been successfully fixed. Depending on risk one can test fixed version of software several ways including: executing all test cases that previously have failed due to defect also by adding new tests to cover any changes that were needed to fix defect However when time money is short when fixing defects confirmation testing might be restricted to simply exercising steps that should reproduce failure caused by defect and checking that failure does not occur. Regression testing confirms that no adverse consequences have been caused by a change including a fix that has already been confirmation tested. These adverse consequences could affect same component where change was made other components same system even other connected systems. Regression testing may not be restricted to test object itself but can also be related to environment. It is advisable first to perform an impact analysis to optimize extent of regression testing. Impact analysis shows which parts of software could be affected. Regression test suites are run many times and generally number of regression test cases will increase with each iteration release so regression testing is a strong candidate for automation. Automation of these tests should start early project. Where CI is used such as DevOps it is good practice to also include automated regression tests. Depending on situation this may include regression tests on different levels. Confirmation testing and/or regression testing for test object are needed on all test levels if defects are fixed and/or changes are made on these test levels. v4.0 Page of 74 2023-04-21 2.3. Maintenance Testing There are different categories of maintenance it can be corrective adaptive to changes environment improve performance maintainability so maintenance can involve planned releases/deployments and unplanned releases/deployments . Impact analysis may be done before a change is made to help decide if change should be made based on potential consequences other areas of system. Testing changes to a system production includes both evaluating success of implementation of change and checking for possible regressions parts of system that remain unchanged . scope of maintenance testing typically depends on: degree of risk of change size of existing system size of change triggers for maintenance and maintenance testing can be classified as follows: Modifications such as planned enhancements corrective changes hot fixes. Upgrades migrations of operational environment such as from one platform to another which can require tests associated with new environment as well as of changed software tests of data conversion when data from another application is migrated into system being maintained. Retirement such as when an application reaches end of its life. When a system is retired this can require testing of data archiving if long data-retention periods are required. Testing of restore and retrieval procedures after archiving may also be needed event that certain data is required during archiving period. v4.0 Page of 74 2023-04-21 3. Static Testing – 80 minutes Keywords anomaly dynamic testing formal review informal review inspection review static analysis static testing technical review walkthrough Learning Objectives for Chapter 3: 3.1 Static Testing Basics FL-3.1.1 Recognize types of products that can be examined by different static test techniques FL-3.1.2 Explain value of static testing FL-3.1.3 Compare and contrast static and dynamic testing 3.2 Feedback and Review Process FL-3.2.1 Identify benefits of early and frequent stakeholder feedback FL-3.2.2 Summarize activities of review process FL-3.2.3 Recall which responsibilities are assigned to principal roles when performing reviews FL-3.2.4 Compare and contrast different review types FL-3.2.5 Recall factors that contribute to a successful review v4.0 Page of 74 2023-04-21 3.1. Static Testing Basics contrast to dynamic testing static testing software under test does not need to be executed. Code process specification system architecture specification other work products are evaluated through manual examination with help of a tool . Test objectives include improving quality detecting defects and assessing characteristics like readability completeness correctness testability and consistency. Static testing can be applied for both verification and validation. Testers business representatives and developers work together during example mappings collaborative user story writing and backlog refinement sessions to ensure that user stories and related work products meet defined criteria e.g. Definition of Ready . Review techniques can be applied to ensure user stories are complete and understandable and include testable acceptance criteria. By asking right questions testers explore challenge and help improve proposed user stories. Static analysis can identify problems prior to dynamic testing while often requiring less effort since no test cases are required and tools are typically used. Static analysis is often incorporated into CI frameworks . While largely used to detect specific code defects static analysis is also used to evaluate maintainability and security. Spelling checkers and readability tools are other examples of static analysis tools. 3.1.1. Work Products Examinable by Static Testing Almost any work product can be examined using static testing. Examples include requirement specification documents source code test plans test cases product backlog items test charters project documentation contracts and models. Any work product that can be read and understood can be subject of a review. However for static analysis work products need a structure against which they can be checked . Work products that are not appropriate for static testing include those that are difficult to interpret by human beings and that should not be analyzed by tools . 3.1.2. Value of Static Testing Static testing can detect defects earliest phases of SDLC fulfilling principle of early testing . It can also identify defects which cannot be detected by dynamic testing . Static testing provides ability to evaluate quality of and to build confidence work products. By verifying documented requirements stakeholders can also make sure that these requirements describe their actual needs. Since static testing can be performed early SDLC a shared understanding can be created among involved stakeholders. Communication will also be improved between involved stakeholders. For this reason it is recommended to involve a wide variety of stakeholders static testing. Even though reviews can be costly to implement overall project costs are usually much lower than when no reviews are performed because less time and effort needs to be spent on fixing defects later project. v4.0 Page of 74 2023-04-21 Code defects can be detected using static analysis more efficiently than dynamic testing usually resulting both fewer code defects and a lower overall development effort. 3.1.3. Differences between Static Testing and Dynamic Testing Static testing and dynamic testing practices complement each other. They have similar objectives such as supporting detection of defects work products but there are also some differences such as: Static and dynamic testing can both lead to detection of defects however there are some defect types that can only be found by either static dynamic testing. Static testing finds defects directly while dynamic testing causes failures from which associated defects are determined through subsequent analysis Static testing may more easily detect defects that lay on paths through code that are rarely executed hard to reach using dynamic testing Static testing can be applied to non-executable work products while dynamic testing can only be applied to executable work products Static testing can be used to measure quality characteristics that are not dependent on executing code while dynamic testing can be used to measure quality characteristics that are dependent on executing code Typical defects that are easier and/or cheaper to find through static testing include: Defects requirements Design defects Certain types of coding defects Deviations from standards Incorrect interface specifications Specific types of security vulnerabilities Gaps inaccuracies test basis coverage 3.2. Feedback and Review Process 3.2.1. Benefits of Early and Frequent Stakeholder Feedback Early and frequent feedback allows for early communication of potential quality problems. If there is little stakeholder involvement during SDLC product being developed might not meet stakeholder’s original current vision. A failure to deliver what stakeholder wants can result costly rework missed deadlines blame games and might even lead to complete project failure. v4.0 Page of 74 2023-04-21 Frequent stakeholder feedback throughout SDLC can prevent misunderstandings about requirements and ensure that changes to requirements are understood and implemented earlier. This helps development team to improve their understanding of what they are building. It allows them to focus on those features that deliver most value to stakeholders and that have most positive impact on identified risks. 3.2.2. Review Process Activities ISO/IEC 20246 standard defines a generic review process that provides a structured but flexible framework from which a specific review process may be tailored to a particular situation. If required review is more formal then more of tasks described for different activities will be needed. size of many work products makes them too large to be covered by a single review. review process may be invoked a couple of times to complete review for entire work product. activities review process are: Planning. During planning phase scope of review which comprises purpose work product to be reviewed quality characteristics to be evaluated areas to focus on exit criteria supporting information such as standards effort and timeframes for review shall be defined. Review initiation. During review initiation goal is to make sure that everyone and everything involved is prepared to start review. This includes making sure that every participant has access to work product under review understands their role and responsibilities and receives everything needed to perform review. Individual review. Every reviewer performs an individual review to assess quality of work product under review and to identify anomalies recommendations and questions by applying one more review techniques . ISO/IEC 20246 standard provides more depth on different review techniques. reviewers log all their identified anomalies recommendations and questions. Communication and analysis. Since anomalies identified during a review are not necessarily defects all these anomalies need to be analyzed and discussed. For every anomaly decision should be made on its status ownership and required actions. This is typically done a review meeting during which participants also decide what quality level of reviewed work product is and what follow-up actions are required. A follow-up review may be required to complete actions. Fixing and reporting. For every defect a defect report should be created so that corrective actions can be followed-up. Once exit criteria are reached work product can be accepted. review results are reported. 3.2.3. Roles and Responsibilities Reviews Reviews involve various stakeholders who may take on several roles. principal roles and their responsibilities are: Manager – decides what is to be reviewed and provides resources such as staff and time for review Author – creates and fixes work product under review v4.0 Page of 74 2023-04-21 Moderator – ensures effective running of review meetings including mediation time management and a safe review environment which everyone can speak freely Scribe – collates anomalies from reviewers and records review information such as decisions and new anomalies found during review meeting Reviewer – performs reviews. A reviewer may be someone working on project a subject matter expert any other stakeholder Review leader – takes overall responsibility for review such as deciding who will be involved and organizing when and where review will take place Other more detailed roles are possible as described ISO/IEC 20246 standard. 3.2.4. Review Types There exist many review types ranging from informal reviews to formal reviews. required level of formality depends on factors such as SDLC being followed maturity of development process criticality and complexity of work product being reviewed legal regulatory requirements and need for an audit trail. same work product can be reviewed with different review types e.g. first an informal one and later a more formal one. Selecting right review type is key to achieving required review objectives . selection is not only based on objectives but also on factors such as project needs available resources work product type and risks business domain and company culture. Some commonly used review types are: Informal review. Informal reviews do not follow a defined process and do not require a formal documented output. main objective is detecting anomalies. Walkthrough. A walkthrough which is led by author can serve many objectives such as evaluating quality and building confidence work product educating reviewers gaining consensus generating new ideas motivating and enabling authors to improve and detecting anomalies. Reviewers might perform an individual review before walkthrough but this is not required. Technical Review. A technical review is performed by technically qualified reviewers and led by a moderator. objectives of a technical review are to gain consensus and make decisions regarding a technical problem but also to detect anomalies evaluate quality and build confidence work product generate new ideas and to motivate and enable authors to improve. Inspection. As inspections are most formal type of review they follow complete generic process . main objective is to find maximum number of anomalies. Other objectives are to evaluate quality build confidence work product and to motivate and enable authors to improve. Metrics are collected and used to improve SDLC including inspection process. inspections author cannot act as review leader scribe. 3.2.5. Success Factors for Reviews There are several factors that determine success of reviews which include: v4.0 Page of 74 2023-04-21 Defining clear objectives and measurable exit criteria. Evaluation of participants should never be an objective Choosing appropriate review type to achieve given objectives and to suit type of work product review participants project needs and context Conducting reviews on small chunks so that reviewers do not lose concentration during an individual review and/or review meeting Providing feedback from reviews to stakeholders and authors so they can improve product and their activities Providing adequate time to participants to prepare for review Support from management for review process Making reviews part of organization’s culture to promote learning and process improvement Providing adequate training for all participants so they know how to fulfil their role Facilitating meetings v4.0 Page of 74 2023-04-21 4. Test Analysis and Design – 390 minutes Keywords acceptance criteria acceptance test-driven development black-box test technique boundary value analysis branch coverage checklist-based testing collaboration-based test approach coverage coverage item decision table testing equivalence partitioning error guessing experience-based test technique exploratory testing state transition testing statement coverage test technique white-box test technique 4.1 Test Techniques Overview FL-4.1.1 Distinguish black-box white-box and experience-based test techniques 4.2 Black-box Test Techniques FL-4.2.1 Use equivalence partitioning to derive test cases FL-4.2.2 Use boundary value analysis to derive test cases FL-4.2.3 Use decision table testing to derive test cases FL-4.2.4 Use state transition testing to derive test cases 4.3 White-box Test Techniques FL-4.3.1 Explain statement testing FL-4.3.2 Explain branch testing FL-4.3.3 Explain value of white-box testing 4.4 Experience-based Test Techniques FL-4.4.1 Explain error guessing FL-4.4.2 Explain exploratory testing FL-4.4.3 Explain checklist-based testing 4.5. Collaboration-based Test Approaches FL-4.5.1 Explain how to write user stories collaboration with developers and business representatives FL-4.5.2 Classify different options for writing acceptance criteria FL-4.5.3 Use acceptance test-driven development to derive test cases v4.0 Page of 74 2023-04-21 4.1. Test Techniques Overview Test techniques support tester test analysis and test design . Test techniques help to develop a relatively small but sufficient set of test cases a systematic way. Test techniques also help tester to define test conditions identify coverage items and identify test data during test analysis and design. Further information on test techniques and their corresponding measures can be found ISO/IEC/IEEE 29119-4 standard and . this syllabus test techniques are classified as black-box white-box and experience-based. Black-box test techniques are based on an analysis of specified behavior of test object without reference to its internal structure. Therefore test cases are independent of how software is implemented. Consequently if implementation changes but required behavior stays same then test cases are still useful. White-box test techniques are based on an analysis of test object’s internal structure and processing. As test cases are dependent on how software is designed they can only be created after design implementation of test object. Experience-based test techniques effectively use knowledge and experience of testers for design and implementation of test cases. effectiveness of these techniques depends heavily on tester’s skills. Experience-based test techniques can detect defects that may be missed using black- box and white-box test techniques. Hence experience-based test techniques are complementary to black-box and white-box test techniques. 4.2. Black-Box Test Techniques Commonly used black-box test techniques discussed following sections are: Equivalence Partitioning Boundary Value Analysis Decision Table Testing State Transition Testing 4.2.1. Equivalence Partitioning Equivalence Partitioning divides data into partitions based on expectation that all elements of a given partition are to be processed same way by test object. theory behind this technique is that if a test case that tests one value from an equivalence partition detects a defect this defect should also be detected by test cases that test any other value from same partition. Therefore one test for each partition is sufficient. Equivalence partitions can be identified for any data element related to test object including inputs outputs configuration items internal values time-related values and interface parameters. partitions may be continuous discrete ordered unordered finite infinite. partitions must not overlap and must be non-empty sets. For simple test objects EP can be easy but practice understanding how test object will treat different values is often complicated. Therefore partitioning should be done with care. v4.0 Page of 74 2023-04-21 A partition containing valid values is called a valid partition. A partition containing invalid values is called an invalid partition. definitions of valid and invalid values may vary among teams and organizations. For example valid values may be interpreted as those that should be processed by test object as those for which specification defines their processing. Invalid values may be interpreted as those that should be ignored rejected by test object as those for which no processing is defined test object specification. EP coverage items are equivalence partitions. To achieve 100% coverage with this technique test cases must exercise all identified partitions by covering each partition at least once. Coverage is measured as number of partitions exercised by at least one test case divided by total number of identified partitions and is expressed as a percentage. Many test objects include multiple sets of partitions which means that a test case will cover partitions from different sets of partitions. simplest coverage criterion case of multiple sets of partitions is called Each Choice coverage . Each Choice coverage requires test cases to exercise each partition from each set of partitions at least once. Each Choice coverage does not take into account combinations of partitions. 4.2.2. Boundary Value Analysis Boundary Value Analysis is a technique based on exercising boundaries of equivalence partitions. Therefore BVA can only be used for ordered partitions. minimum and maximum values of a partition are its boundary values. case of BVA if two elements belong to same partition all elements between them must also belong to that partition. BVA focuses on boundary values of partitions because developers are more likely to make errors with these boundary values. Typical defects found by BVA are located where implemented boundaries are misplaced to positions above below their intended positions are omitted altogether. This syllabus covers two versions of BVA: 2-value and 3-value BVA. They differ terms of coverage items per boundary that need to be exercised to achieve 100% coverage. 2-value BVA for each boundary value there are two coverage items: this boundary value and its closest neighbor belonging to adjacent partition. To achieve 100% coverage with 2-value BVA test cases must exercise all coverage items i.e. all identified boundary values. Coverage is measured as number of boundary values that were exercised divided by total number of identified boundary values and is expressed as a percentage. 3-value BVA for each boundary value there are three coverage items: this boundary value and both its neighbors. Therefore 3-value BVA some of coverage items may not be boundary values. To achieve 100% coverage with 3-value BVA test cases must exercise all coverage items i.e. identified boundary values and their neighbors. Coverage is measured as number of boundary values and their neighbors exercised divided by total number of identified boundary values and their neighbors and is expressed as a percentage. 3-value BVA is more rigorous than 2-value BVA as it may detect defects overlooked by 2-value BVA. For example if decision “if …” is incorrectly implemented as “if …” no test data derived from 2-value BVA can detect defect. However x = 9 derived from 3-value BVA is likely to detect it. v4.0 Page of 74 2023-04-21 4.2.3. Decision Table Testing Decision tables are used for testing implementation of system requirements that specify how different combinations of conditions result different outcomes. Decision tables are an effective way of recording complex logic such as business rules. When creating decision tables conditions and resulting actions of system are defined. These form rows of table. Each column corresponds to a decision rule that defines a unique combination of conditions along with associated actions. limited-entry decision tables all values of conditions and actions are shown as Boolean values . Alternatively extended-entry decision tables some all conditions and actions may also take on multiple values . notation for conditions is as follows: “T” means that condition is satisfied. “F” means that condition is not satisfied. “–” means that value of condition is irrelevant for action outcome. “N/A” means that condition is infeasible for a given rule. For actions: “X” means that action should occur. Blank means that action should not occur. Other notations may also be used. A full decision table has enough columns to cover every combination of conditions. table can be simplified by deleting columns containing infeasible combinations of conditions. table can also be minimized by merging columns which some conditions do not affect outcome into a single column. Decision table minimization algorithms are out of scope of this syllabus. decision table testing coverage items are columns containing feasible combinations of conditions. To achieve 100% coverage with this technique test cases must exercise all these columns. Coverage is measured as number of exercised columns divided by total number of feasible columns and is expressed as a percentage. strength of decision table testing is that it provides a systematic approach to identify all combinations of conditions some of which might otherwise be overlooked. It also helps to find any gaps contradictions requirements. If there are many conditions exercising all decision rules may be time consuming since number of rules grows exponentially with number of conditions. such a case to reduce number of rules that need to be exercised a minimized decision table a risk- based approach may be used. 4.2.4. State Transition Testing A state transition diagram models behavior of a system by showing its possible states and valid state transitions. A transition is initiated by an event which may be additionally qualified by a guard condition. transitions are assumed to be instantaneous and may sometimes result software taking action. common transition labeling syntax is as follows: “event [guard condition] / action”. Guard conditions and actions can be omitted if they do not exist are irrelevant for tester. A state table is a model equivalent to a state transition diagram. Its rows represent states and its columns represent events . Table entries represent transitions and contain target state as well as resulting actions if defined. contrast to state transition diagram state table explicitly shows invalid transitions which are represented by empty cells. A test case based on a state transition diagram state table is usually represented as a sequence of events which results a sequence of state changes . One test case may and usually will cover several transitions between states. There exist many coverage criteria for state transition testing. This syllabus discusses three of them. v4.0 Page of 74 2023-04-21 all states coverage coverage items are states. To achieve 100% all states coverage test cases must ensure that all states are visited. Coverage is measured as number of visited states divided by total number of states and is expressed as a percentage. valid transitions coverage coverage items are single valid transitions. To achieve 100% valid transitions coverage test cases must exercise all valid transitions. Coverage is measured as number of exercised valid transitions divided by total number of valid transitions and is expressed as a percentage. all transitions coverage coverage items are all transitions shown a state table. To achieve 100% all transitions coverage test cases must exercise all valid transitions and attempt to execute invalid transitions. Testing only one invalid transition a single test case helps to avoid fault masking i.e. a situation which one defect prevents detection of another. Coverage is measured as number of valid and invalid transitions exercised attempted to be covered by executed test cases divided by total number of valid and invalid transitions and is expressed as a percentage. All states coverage is weaker than valid transitions coverage because it can typically be achieved without exercising all transitions. Valid transitions coverage is most widely used coverage criterion. Achieving full valid transitions coverage guarantees full all states coverage. Achieving full all transitions coverage guarantees both full all states coverage and full valid transitions coverage and should be a minimum requirement for mission and safety-critical software. 4.3. White-Box Test Techniques Because of their popularity and simplicity this section focuses on two code-related white-box test techniques: Statement testing Branch testing There are more rigorous techniques that are used some safety-critical mission-critical high-integrity environments to achieve more thorough code coverage. There are also white-box test techniques used higher test levels using coverage not related to code . These techniques are not discussed this syllabus. 4.3.1. Statement Testing and Statement Coverage statement testing coverage items are executable statements. aim is to design test cases that exercise statements code until an acceptable level of coverage is achieved. Coverage is measured as number of statements exercised by test cases divided by total number of executable statements code and is expressed as a percentage. When 100% statement coverage is achieved it ensures that all executable statements code have been exercised at least once. particular this means that each statement with a defect will be executed which may cause a failure demonstrating presence of defect. However exercising a statement with a test case will not detect defects all cases. For example it may not detect defects that are data dependent . Also 100% statement coverage does not ensure that all decision logic has been tested as for instance it may not exercise all branches code. v4.0 Page of 74 2023-04-21 4.3.2. Branch Testing and Branch Coverage A branch is a transfer of control between two nodes control flow graph which shows possible sequences which source code statements are executed test object. Each transfer of control can be either unconditional conditional . branch testing coverage items are branches and aim is to design test cases to exercise branches code until an acceptable level of coverage is achieved. Coverage is measured as number of branches exercised by test cases divided by total number of branches and is expressed as a percentage. When 100% branch coverage is achieved all branches code unconditional and conditional are exercised by test cases. Conditional branches typically correspond to a true false outcome from an “if...then” decision an outcome from a switch/case statement a decision to exit continue a loop. However exercising a branch with a test case will not detect defects all cases. For example it may not detect defects requiring execution of a specific path a code. Branch coverage subsumes statement coverage. This means that any set of test cases achieving 100% branch coverage also achieves 100% statement coverage . 4.3.3. Value of White-box Testing A fundamental strength that all white-box techniques share is that entire software implementation is taken into account during testing which facilitates defect detection even when software specification is vague outdated incomplete. A corresponding weakness is that if software does not implement one more requirements white box testing may not detect resulting defects of omission . White-box techniques can be used static testing . They are well suited to reviewing code that is not yet ready for execution as well as pseudocode and other high- level top-down logic which can be modeled with a control flow graph. Performing only black-box testing does not provide a measure of actual code coverage. White-box coverage measures provide an objective measurement of coverage and provide necessary information to allow additional tests to be generated to increase this coverage and subsequently increase confidence code. 4.4. Experience-based Test Techniques Commonly used experience-based test techniques discussed following sections are: Error guessing Exploratory testing Checklist-based testing 4.4.1. Error Guessing Error guessing is a technique used to anticipate occurrence of errors defects and failures based on tester’s knowledge including: How application has worked past v4.0 Page of 74 2023-04-21 types of errors developers tend to make and types of defects that result from these errors types of failures that have occurred other similar applications general errors defects and failures may be related to: input output logic computation interfaces data . Fault attacks are a methodical approach to implementation of error guessing. This technique requires tester to create acquire a list of possible errors defects and failures and to design tests that will identify defects associated with errors expose defects cause failures. These lists can be built based on experience defect and failure data from common knowledge about why software fails. See for more information on error guessing and fault attacks. 4.4.2. Exploratory Testing exploratory testing tests are simultaneously designed executed and evaluated while tester learns about test object. testing is used to learn more about test object to explore it more deeply with focused tests and to create tests for untested areas. Exploratory testing is sometimes conducted using session-based testing to structure testing. a session-based approach exploratory testing is conducted within a defined time-box. tester uses a test charter containing test objectives to guide testing. test session is usually followed by a debriefing that involves a discussion between tester and stakeholders interested test results of test session. this approach test objectives may be treated as high-level test conditions. Coverage items are identified and exercised during test session. tester may use test session sheets to document steps followed and discoveries made. Exploratory testing is useful when there are few inadequate specifications there is significant time pressure on testing. Exploratory testing is also useful to complement other more formal test techniques. Exploratory testing will be more effective if tester is experienced has domain knowledge and has a high degree of essential skills like analytical skills curiosity and creativeness . Exploratory testing can incorporate use of other test techniques . More information about exploratory testing can be found . 4.4.3. Checklist-Based Testing checklist-based testing a tester designs implements and executes tests to cover test conditions from a checklist. Checklists can be built based on experience knowledge about what is important for user an understanding of why and how software fails. Checklists should not contain items that can be checked automatically items better suited as entry/exit criteria items that are too general . Checklist items are often phrased form of a question. It should be possible to check each item separately and directly. These items may refer to requirements graphical interface properties quality characteristics other forms of test conditions. Checklists can be created to support various test types including functional and non-functional testing ). v4.0 Page of 74 2023-04-21 Some checklist entries may gradually become less effective over time because developers will learn to avoid making same errors. New entries may also need to be added to reflect newly found high severity defects. Therefore checklists should be regularly updated based on defect analysis. However care should be taken to avoid letting checklist become too long . absence of detailed test cases checklist-based testing can provide guidelines and some degree of consistency for testing. If checklists are high-level some variability actual testing is likely to occur resulting potentially greater coverage but less repeatability. 4.5. Collaboration-based Test Approaches Each of above-mentioned techniques has a particular objective with respect to defect detection. Collaboration-based approaches on other hand focus also on defect avoidance by collaboration and communication. 4.5.1. Collaborative User Story Writing A user story represents a feature that will be valuable to either a user purchaser of a system software. User stories have three critical aspects called together “3 C’s”: Card – medium describing a user story Conversation – explains how software will be used Confirmation – acceptance criteria most common format for a user story is “As a [role] I want [goal to be accomplished] so that I can [resulting business value for role]” followed by acceptance criteria. Collaborative authorship of user story can use techniques such as brainstorming and mind mapping. collaboration allows team to obtain a shared vision of what should be delivered by taking into account three perspectives: business development and testing. Good user stories should be: Independent Negotiable Valuable Estimable Small and Testable . If a stakeholder does not know how to test a user story this may indicate that user story is not clear enough that it does not reflect something valuable to them that stakeholder just needs help testing . 4.5.2. Acceptance Criteria Acceptance criteria for a user story are conditions that an implementation of user story must meet to be accepted by stakeholders. From this perspective acceptance criteria may be viewed as test conditions that should be exercised by tests. Acceptance criteria are usually a result of Conversation . Acceptance criteria are used to: Define scope of user story Reach consensus among stakeholders Describe both positive and negative scenarios Serve as a basis for user story acceptance testing v4.0 Page of 74 2023-04-21 Allow accurate planning and estimation There are several ways to write acceptance criteria for a user story. two most common formats are: Scenario-oriented Rule-oriented Most acceptance criteria can be documented one of these two formats. However team may use another custom format as long as acceptance criteria are well-defined and unambiguous. 4.5.3. Acceptance Test-driven Development ATDD is a test-first approach . Test cases are created prior to implementing user story. test cases are created by team members with different perspectives e.g. customers developers and testers . Test cases may be executed manually automated. first step is a specification workshop where user story and its acceptance criteria are analyzed discussed and written by team members. Incompleteness ambiguities defects user story are resolved during this process. next step is to create test cases. This can be done by team as a whole by tester individually. test cases are based on acceptance criteria and can be seen as examples of how software works. This will help team implement user story correctly. Since examples and tests are same these terms are often used interchangeably. During test design test techniques described sections 4.2 4.3 and 4.4 may be applied. Typically first test cases are positive confirming correct behavior without exceptions error conditions and comprising sequence of activities executed if everything goes as expected. After positive test cases are done team should perform negative testing. Finally team should cover non-functional quality characteristics as well . Test cases should be expressed a way that is understandable for stakeholders. Typically test cases contain sentences natural language involving necessary preconditions inputs and postconditions. test cases must cover all characteristics of user story and should not go beyond story. However acceptance criteria may detail some of issues described user story. addition no two test cases should describe same characteristics of user story. When captured a format supported by a test automation framework developers can automate test cases by writing supporting code as they implement feature described by a user story. acceptance tests then become executable requirements. v4.0 Page of 74 2023-04-21 5. Managing Test Activities – 335 minutes Keywords defect management defect report entry criteria exit criteria product risk project risk risk risk analysis risk assessment risk control risk identification risk level risk management risk mitigation risk monitoring risk-based testing test approach test completion report test control test monitoring test plan test planning test progress report test pyramid testing quadrants Learning Objectives for Chapter 5: 5.1 Test Planning FL-5.1.1 Exemplify purpose and content of a test plan FL-5.1.2 Recognize how a tester adds value to iteration and release planning FL-5.1.3 Compare and contrast entry criteria and exit criteria FL-5.1.4 Use estimation techniques to calculate required test effort FL-5.1.5 Apply test case prioritization FL-5.1.6 Recall concepts of test pyramid FL-5.1.7 Summarize testing quadrants and their relationships with test levels and test types 5.2 Risk Management FL-5.2.1 Identify risk level by using risk likelihood and risk impact FL-5.2.2 Distinguish between project risks and product risks FL-5.2.3 Explain how product risk analysis may influence thoroughness and scope of testing FL-5.2.4 Explain what measures can be taken response to analyzed product risks 5.3 Test Monitoring Test Control and Test Completion FL-5.3.1 Recall metrics used for testing FL-5.3.2 Summarize purposes content and audiences for test reports FL-5.3.3 Exemplify how to communicate status of testing 5.4 Configuration Management FL-5.4.1 Summarize how configuration management supports testing 5.5 Defect Management FL-5.5.1 Prepare a defect report v4.0 Page of 74 2023-04-21 5.1. Test Planning 5.1.1. Purpose and Content of a Test Plan A test plan describes objectives resources and processes for a test project. A test plan: Documents means and schedule for achieving test objectives Helps to ensure that performed test activities will meet established criteria Serves as a means of communication with team members and other stakeholders Demonstrates that testing will adhere to existing test policy and test strategy Test planning guides testers’ thinking and forces testers to confront future challenges related to risks schedules people tools costs effort etc. process of preparing a test plan is a useful way to think through efforts needed to achieve test project objectives. typical content of a test plan includes: Context of testing Assumptions and constraints of test project Stakeholders Communication Risk register Test approach Budget and schedule More details about test plan and its content can be found ISO/IEC/IEEE 29119-3 standard. 5.1.2. Tester's Contribution to Iteration and Release Planning iterative SDLCs typically two kinds of planning occur: release planning and iteration planning. Release planning looks ahead to release of a product defines and re-defines product backlog and may involve refining larger user stories into a set of smaller user stories. It also serves as basis for test approach and test plan across all iterations. Testers involved release planning participate writing testable user stories and acceptance criteria participate project and quality risk analyses estimate test effort associated with user stories determine test approach and plan testing for release. Iteration planning looks ahead to end of a single iteration and is concerned with iteration backlog. Testers involved iteration planning participate detailed risk analysis of user stories determine testability of user stories break down user stories into tasks estimate test effort for all testing tasks and identify and refine functional and non-functional aspects of test object. v4.0 Page of 74 2023-04-21 5.1.3. Entry Criteria and Exit Criteria Entry criteria define preconditions for undertaking a given activity. If entry criteria are not met it is likely that activity will prove to be more difficult time-consuming costly and riskier. Exit criteria define what must be achieved order to declare an activity completed. Entry criteria and exit criteria should be defined for each test level and will differ based on test objectives. Typical entry criteria include: availability of resources availability of testware and initial quality level of a test object . Typical exit criteria include: measures of thoroughness and completion criteria . Running out of time budget can also be viewed as valid exit criteria. Even without other exit criteria being satisfied it can be acceptable to end testing under such circumstances if stakeholders have reviewed and accepted risk to go live without further testing. Agile software development exit criteria are often called Definition of Done defining team’s objective metrics for a releasable item. Entry criteria that a user story must fulfill to start development and/or testing activities are called Definition of Ready. 5.1.4. Estimation Techniques Test effort estimation involves predicting amount of test-related work needed to meet objectives of a test project. It is important to make it clear to stakeholders that estimate is based on a number of assumptions and is always subject to estimation error. Estimation for small tasks is usually more accurate than for large ones. Therefore when estimating a large task it can be decomposed into a set of smaller tasks which then turn can be estimated. this syllabus following four estimation techniques are described. Estimation based on ratios. this metrics-based technique figures are collected from previous projects within organization which makes it possible to derive “standard” ratios for similar projects. ratios of an organization’s own projects are generally best source to use estimation process. These standard ratios can then be used to estimate test effort for new project. For example if previous project development-to-test effort ratio was 3:2 and current project development effort is expected to be 600 person-days test effort can be estimated to be 400 person-days. Extrapolation. this metrics-based technique measurements are made as early as possible current project to gather data. Having enough observations effort required for remaining work can be approximated by extrapolating this data . This method is very suitable iterative SDLCs. For example team may extrapolate test effort forthcoming iteration as averaged effort from last three iterations. Wideband Delphi. this iterative expert-based technique experts make experience-based estimations. Each expert isolation estimates effort. results are collected and if there are deviations that are out of range of agreed upon boundaries experts discuss their current estimates. Each expert is then asked to make a new estimation based on that feedback again isolation. This process is repeated until a consensus is reached. Planning Poker is a variant of Wideband Delphi commonly used Agile v4.0 Page of 74 2023-04-21 software development. Planning Poker estimates are usually made using cards with numbers that represent effort size. Three-point estimation. this expert-based technique three estimations are made by experts: most optimistic estimation most likely estimation and most pessimistic estimation . final estimate is their weighted arithmetic mean. most popular version of this technique estimate is calculated as E = / 6. advantage of this technique is that it allows experts to calculate measurement error: SD = / 6. For example if estimates are: a=6 m=9 and b=18 then final estimation is 10±2 person-hours because E = / 6 = 10 and SD = / 6 = 2. See for these and many other test estimation techniques. 5.1.5. Test Case Prioritization Once test cases and test procedures are specified and assembled into test suites these test suites can be arranged a test execution schedule that defines order which they are to be run. When prioritizing test cases different factors can be taken into account. most commonly used test case prioritization strategies are as follows: Risk-based prioritization where order of test execution is based on results of risk analysis . Test cases covering most important risks are executed first. Coverage-based prioritization where order of test execution is based on coverage . Test cases achieving highest coverage are executed first. another variant called additional coverage prioritization test case achieving highest coverage is executed first; each subsequent test case is one that achieves highest additional coverage. Requirements-based prioritization where order of test execution is based on priorities of requirements traced back to corresponding test cases. Requirement priorities are defined by stakeholders. Test cases related to most important requirements are executed first. Ideally test cases would be ordered to run based on their priority levels using for example one of above-mentioned prioritization strategies. However this practice may not work if test cases features being tested have dependencies. If a test case with a higher priority is dependent on a test case with a lower priority lower priority test case must be executed first. order of test execution must also take into account availability of resources. For example required test tools test environments people that may only be available for a specific time window. 5.1.6. Test Pyramid test pyramid is a model showing that different tests may have different granularity. test pyramid model supports team test automation and test effort allocation by showing that different goals are supported by different levels of test automation. pyramid layers represent groups of tests. higher layer lower test granularity test isolation and test execution time. Tests bottom layer are small isolated fast and check a small piece of functionality so usually a lot of them are needed to achieve a reasonable coverage. top layer represents complex high-level end-to-end tests. These high-level tests are generally slower than tests from lower layers and they typically check a large piece of functionality so usually just a few of them are needed to achieve a reasonable coverage. number and naming of layers may differ. For example original test pyramid model defines three layers: “unit tests” “service tests” and “UI tests”. Another popular model defines unit v4.0 Page of 74 2023-04-21 tests integration tests and end-to-end tests. Other test levels can also be used. 5.1.7. Testing Quadrants testing quadrants defined by Brian Marick group test levels with appropriate test types activities test techniques and work products Agile software development. model supports test management visualizing these to ensure that all appropriate test types and test levels are included SDLC and understanding that some test types are more relevant to certain test levels than others. This model also provides a way to differentiate and describe types of tests to all stakeholders including developers testers and business representatives. this model tests can be business facing technology facing. Tests can also support team critique product . combination of these two viewpoints determines four quadrants: Quadrant Q1 . This quadrant contains component and component integration tests. These tests should be automated and included CI process. Quadrant Q2 . This quadrant contains functional tests examples user story tests user experience prototypes API testing and simulations. These tests check acceptance criteria and can be manual automated. Quadrant Q3 . This quadrant contains exploratory testing usability testing user acceptance testing. These tests are user-oriented and often manual. Quadrant Q4 . This quadrant contains smoke tests and non-functional tests . These tests are often automated. 5.2. Risk Management Organizations face many internal and external factors that make it uncertain whether and when they will achieve their objectives . Risk management allows organizations to increase likelihood of achieving objectives improve quality of their products and increase stakeholders’ confidence and trust. main risk management activities are: Risk analysis Risk control test approach which test activities are selected prioritized and managed based on risk analysis and risk control is called risk-based testing. 5.2.1. Risk Definition and Risk Attributes Risk is a potential event hazard threat situation whose occurrence causes an adverse effect. A risk can be characterized by two factors: Risk likelihood – probability of risk occurrence Risk impact – consequences of this occurrence v4.0 Page of 74 2023-04-21 These two factors express risk level which is a measure for risk. higher risk level more important is its treatment. 5.2.2. Project Risks and Product Risks software testing one is generally concerned with two types of risks: project risks and product risks. Project risks are related to management and control of project. Project risks include: Organizational issues People issues Technical issues Supplier issues Project risks when they occur may have an impact on project schedule budget scope which affects project's ability to achieve its objectives. Product risks are related to product quality characteristics . Examples of product risks include: missing wrong functionality incorrect calculations runtime errors poor architecture inefficient algorithms inadequate response time poor user experience security vulnerabilities. Product risks when they occur may result various negative consequences including: User dissatisfaction Loss of revenue trust reputation Damage to third parties High maintenance costs overload of helpdesk Criminal penalties extreme cases physical damage injuries even death 5.2.3. Product Risk Analysis From a testing perspective goal of product risk analysis is to provide an awareness of product risk order to focus testing effort a way that minimizes residual level of product risk. Ideally product risk analysis begins early SDLC. Product risk analysis consists of risk identification and risk assessment. Risk identification is about generating a comprehensive list of risks. Stakeholders can identify risks by using various techniques and tools e.g. brainstorming workshops interviews cause-effect diagrams. Risk assessment involves: categorization of identified risks determining their risk likelihood risk impact and level prioritizing and proposing ways to handle them. Categorization helps assigning mitigation actions because usually risks falling into same category can be mitigated using a similar approach. Risk assessment can use a quantitative qualitative approach a mix of them. quantitative approach risk level is calculated as multiplication of risk likelihood and risk impact. qualitative approach risk level can be determined using a risk matrix. Product risk analysis may influence thoroughness and scope of testing. Its results are used to: v4.0 Page of 74 2023-04-21 Determine scope of testing to be carried out Determine particular test levels and propose test types to be performed Determine test techniques to be employed and coverage to be achieved Estimate test effort required for each task Prioritize testing an attempt to find critical defects as early as possible Determine whether any activities addition to testing could be employed to reduce risk 5.2.4. Product Risk Control Product risk control comprises all measures that are taken response to identified and assessed product risks. Product risk control consists of risk mitigation and risk monitoring. Risk mitigation involves implementing actions proposed risk assessment to reduce risk level. aim of risk monitoring is to ensure that mitigation actions are effective to obtain further information to improve risk assessment and to identify emerging risks. With respect to product risk control once a risk has been analyzed several response options to risk are possible e.g. risk mitigation by testing risk acceptance risk transfer contingency plan . Actions that can be taken to mitigate product risks by testing are as follows: Select testers with right level of experience and skills suitable for a given risk type Apply an appropriate level of independence of testing Conduct reviews and perform static analysis Apply appropriate test techniques and coverage levels Apply appropriate test types addressing affected quality characteristics Perform dynamic testing including regression testing 5.3. Test Monitoring Test Control and Test Completion Test monitoring is concerned with gathering information about testing. This information is used to assess test progress and to measure whether test exit criteria test tasks associated with exit criteria are satisfied such as meeting targets for coverage of product risks requirements acceptance criteria. Test control uses information from test monitoring to provide a form of control directives guidance and necessary corrective actions to achieve most effective and efficient testing. Examples of control directives include: Reprioritizing tests when an identified risk becomes an issue Re-evaluating whether a test item meets entry criteria exit criteria due to rework Adjusting test schedule to address a delay delivery of test environment Adding new resources when and where needed v4.0 Page of 74 2023-04-21 Test completion collects data from completed test activities to consolidate experience testware and any other relevant information. Test completion activities occur at project milestones such as when a test level is completed an agile iteration is finished a test project is completed a software system is released a maintenance release is completed. 5.3.1. Metrics used Testing Test metrics are gathered to show progress against planned schedule and budget current quality of test object and effectiveness of test activities with respect to objectives an iteration goal. Test monitoring gathers a variety of metrics to support test control and test completion. Common test metrics include: Project progress metrics Test progress metrics Product quality metrics Defect metrics Risk metrics Coverage metrics Cost metrics 5.3.2. Purpose Content and Audience for Test Reports Test reporting summarizes and communicates test information during and after testing. Test progress reports support ongoing control of testing and must provide enough information to make modifications to test schedule resources test plan when such changes are needed due to deviation from plan changed circumstances. Test completion reports summarize a specific stage of testing and can give information for subsequent testing. During test monitoring and control test team generates test progress reports for stakeholders to keep them informed. Test progress reports are usually generated on a regular basis and include: Test period Test progress including any notable deviations Impediments for testing and their workarounds Test metrics New and changed risks within testing period Testing planned for next period v4.0 Page of 74 2023-04-21 A test completion report is prepared during test completion when a project test level test type is complete and when ideally its exit criteria have been met. This report uses test progress reports and other data. Typical test completion reports include: Test summary Testing and product quality evaluation based on original test plan Deviations from test plan . Testing impediments and workarounds Test metrics based on test progress reports Unmitigated risks defects not fixed Lessons learned that are relevant to testing Different audiences require different information reports and influence degree of formality and frequency of reporting. Reporting on test progress to others same team is often frequent and informal while reporting on testing for a completed project follows a set template and occurs only once. ISO/IEC/IEEE 29119-3 standard includes templates and examples for test progress reports and test completion reports. 5.3.3. Communicating Status of Testing best means of communicating test status varies depending on test management concerns organizational test strategies regulatory standards case of self-organizing teams on team itself. options include: Verbal communication with team members and other stakeholders Dashboards Electronic communication channels Online documentation Formal test reports One more of these options can be used. More formal communication may be more appropriate for distributed teams where direct face-to-face communication is not always possible due to geographical distance time differences. Typically different stakeholders are interested different types of information so communication should be tailored accordingly. 5.4. Configuration Management testing configuration management provides a discipline for identifying controlling and tracking work products such as test plans test strategies test conditions test cases test scripts test results test logs and test reports as configuration items. v4.0 Page of 74 2023-04-21 For a complex configuration item CM records items it consists of their relationships and versions. If configuration item is approved for testing it becomes a baseline and can only be changed through a formal change control process. Configuration management keeps a record of changed configuration items when a new baseline is created. It is possible to revert to a previous baseline to reproduce previous test results. To properly support testing CM ensures following: All configuration items including test items are uniquely identified version controlled tracked for changes and related to other configuration items so that traceability can be maintained throughout test process All identified documentation and software items are referenced unambiguously test documentation Continuous integration continuous delivery continuous deployment and associated testing are typically implemented as part of an automated DevOps pipeline which automated CM is normally included. 5.5. Defect Management Since one of major test objectives is to find defects an established defect management process is essential. Although we refer to "defects" here reported anomalies may turn out to be real defects something else - this is resolved during process of dealing with defect reports. Anomalies may be reported during any phase of SDLC and form depends on SDLC. At a minimum defect management process includes a workflow for handling individual anomalies from their discovery to their closure and rules for their classification. workflow typically comprises activities to log reported anomalies analyze and classify them decide on a suitable response such as to fix keep it as it is and finally to close defect report. process must be followed by all involved stakeholders. It is advisable to handle defects from static testing a similar way. Typical defect reports have following objectives: Provide those responsible for handling and resolving reported defects with sufficient information to resolve issue Provide a means of tracking quality of work product Provide ideas for improvement of development and test process A defect report logged during dynamic testing typically includes: Unique identifier Title with a short summary of anomaly being reported Date when anomaly was observed issuing organization and author including their role Identification of test object and test environment Context of defect v4.0 Page 55 of 74 2023-04-21 Description of failure to enable reproduction and resolution including steps that detected anomaly and any relevant test logs database dumps screenshots recordings Expected results and actual results Severity of defect on interests of stakeholders requirements Priority to fix Status of defect References Some of this data may be automatically included when using defect management tools . Document templates for a defect report and example defect reports can be found ISO/IEC/IEEE 29119-3 standard which refers to defect reports as incident reports. v4.0 